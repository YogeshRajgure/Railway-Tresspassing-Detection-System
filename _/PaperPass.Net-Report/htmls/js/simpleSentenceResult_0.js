var data = [{"content":"Railway trespassing detection and alert system using Deep learning，CNN，YOLO","segRedList":["Detection","detection","learning","Learning","using","Deep","and"],"synonymsContent":"","result":[{"score":0.50000006,"similaritySentence":"DL(Embedded Ship Detection and Recognition using Deep Learning)，in","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Railway":"0"},{"trespassing":"0"},{"detection":"1"},{"and":"1"},{"alert":"0"},{"system":"0"},{"using":"1"},{"Deep":"1"},{"learning":"1"},{"，":"0"},{"CNN":"0"},{"，":"0"},{"YOLO":"0"}],"similaritySegGreyList":[{"DL":"0"},{"(":"0"},{"Embedded":"0"},{"Ship":"0"},{"Detection":"1"},{"and":"1"},{"Recognition":"0"},{"using":"1"},{"Deep":"1"},{"Learning":"1"},{")":"0"},{"，":"0"},{"in":"0"}],"subSimilaritySentenceSection":"deep learning approach called ESDR- DL(Embedded Ship Detection and Recognition using Deep Learning)，in order to conduct ship recognition on","duplicateSourceMD5":"1879EDDB5B1F1F06BEE4FDC972E469FC"}]},{"content":"Karthik Arumugam1，Harsh Ingle2，Yogesh Rajgure3","segRedList":[],"synonymsContent":"","result":[]},{"content":"1Student，D.Y.","segRedList":[],"synonymsContent":"","result":[]},{"content":"Patil Institute of Engineering\u0026Technology，Maharashtra，India","segRedList":[],"synonymsContent":"","result":[]},{"content":"2Student，D.Y.","segRedList":[],"synonymsContent":"","result":[]},{"content":"Patil Institute of Engineering\u0026Technology，Maharashtra，India","segRedList":[],"synonymsContent":"","result":[]},{"content":"3Student，D.Y.","segRedList":[],"synonymsContent":"","result":[]},{"content":"Patil Institute of Engineering\u0026Technology，Maharashtra，India","segRedList":[],"synonymsContent":"","result":[]},{"content":"-----------------------------","segRedList":[],"synonymsContent":"","result":[]},{"content":"-----------------------------","segRedList":[],"synonymsContent":"","result":[]},{"content":"-----------***---------------","segRedList":[],"synonymsContent":"","result":[]},{"content":"-----------------------------","segRedList":[],"synonymsContent":"","result":[]},{"content":"-------------------------","segRedList":[],"synonymsContent":"","result":[]},{"content":"Abstract-Railroad trespassing and deaths related to it are becoming inevitable due to negligence by people.","segRedList":[],"synonymsContent":"","result":[]},{"content":"Such acts pose great safety and security concerns.","segRedList":[],"synonymsContent":"","result":[]},{"content":"However，regular monitoring on such sites is not feasible since it requires a lot human labour and also the high resource required and costs corresponding to it are very high.","segRedList":[],"synonymsContent":"","result":[]},{"content":"This issue raises an alarm for human safety and thereby laying the foundation of an automated trespassing detection and","segRedList":[],"synonymsContent":"","result":[]},{"content":"an alert system to leverage the state of art security system in today\u0027s world.","segRedList":[],"synonymsContent":"","result":[]},{"content":"","segRedList":[],"synonymsContent":"","result":[]},{"content":"For overcoming this challenge and deploying a security system，we propose a newfangled framework for Live trespassing detection and alert system on railroads.","segRedList":[],"synonymsContent":"","result":[]},{"content":"The brain ideation of our system uses a CNN based Deep Learning architecture capable of processing a video and detecting any trespassing activity.","segRedList":[],"synonymsContent":"","result":[]},{"content":"","segRedList":[],"synonymsContent":"","result":[]},{"content":"These deep learning-based systems are both good and bad.","segRedList":["learning","based","deep","and"],"synonymsContent":"","result":[{"score":0.5,"similaritySentence":"parts:a deep learning-based segmentation model and a QA","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"These":"0"},{"deep":"1"},{"learning":"1"},{"-":"1"},{"based":"1"},{"systems":"0"},{"are":"0"},{"both":"0"},{"good":"0"},{"and":"1"},{"bad":"0"},{".":"0"}],"similaritySegGreyList":[{"parts":"0"},{":":"0"},{"a":"0"},{"deep":"1"},{"learning":"1"},{"-":"1"},{"based":"1"},{"segmentation":"0"},{"model":"0"},{"and":"1"},{"a":"0"},{"QA":"0"}],"subSimilaritySentenceSection":"It had the following two main parts:a deep learning-based segmentation model and a QA network.The network of the","duplicateSourceMD5":"6BC4AF717535BA98344316FBEE0ADA94"}]},{"content":"Such intelligent system on hand proves to be extremely efficient in avoid such dangerous activity but on other hand","segRedList":[],"synonymsContent":"","result":[]},{"content":"are time consuming along with being computationally very expensive while dealing with loads of video data.","segRedList":[],"synonymsContent":"","result":[]},{"content":"To overcome this hurdle，our proposed solution involves a multi stage approach that uses Deep learning architecture，composed of an inexpensive trespassing detection，","segRedList":[],"synonymsContent":"","result":[]},{"content":"followed by a robust and accurate trespassing classification using the deep neural networks.","segRedList":["Classification","classification","Networks","networks","robust","Neural","neural","Robust","Using","using","deep","Deep","the"],"synonymsContent":"","result":[{"score":0.5119048,"similaritySentence":"apply to the journal pertain.Robust Acoustic Event Classification Using Deep Neural Networks","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"followed":"0"},{"by":"0"},{"a":"0"},{"robust":"1"},{"and":"0"},{"accurate":"0"},{"trespassing":"0"},{"classification":"1"},{"using":"1"},{"the":"1"},{"deep":"1"},{"neural":"1"},{"networks":"1"},{".":"1"}],"similaritySegGreyList":[{"apply":"0"},{"to":"0"},{"the":"1"},{"journal":"0"},{"pertain":"0"},{".":"0"},{"Robust":"1"},{"Acoustic":"0"},{"Event":"0"},{"Classification":"1"},{"Using":"1"},{"Deep":"1"},{"Neural":"1"},{"Networks":"1"}],"subSimilaritySentenceSection":"，and all legal disclaimers that apply to the journal pertain.Robust Acoustic Event Classification Using Deep Neural Networks Roneel V Sharan*and Tom","duplicateSourceMD5":"07FEF4636B24B7CDBF5017F4C17B1C69"},{"score":0.5,"similaritySentence":"pertain.Semantic Region of Interest and Species Classification in the Deep Neural Network","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"followed":"0"},{"by":"0"},{"a":"0"},{"robust":"0"},{"and":"1"},{"accurate":"0"},{"trespassing":"0"},{"classification":"1"},{"using":"0"},{"the":"1"},{"deep":"1"},{"neural":"1"},{"networks":"1"},{".":"1"}],"similaritySegGreyList":[{"pertain":"0"},{".":"0"},{"Semantic":"0"},{"Region":"0"},{"of":"0"},{"Interest":"0"},{"and":"1"},{"Species":"0"},{"Classification":"1"},{"in":"0"},{"the":"1"},{"Deep":"1"},{"Neural":"1"},{"Network":"1"}],"subSimilaritySentenceSection":"disclaimers that apply to the journal pertain.Semantic Region of Interest and Species Classification in the Deep Neural Network Feature Domain Ahmed Ahmeda，Hayder","duplicateSourceMD5":"F295E079D757031D1D494E7CB6B4B570"}]},{"content":"The resulting solution overcomes the issue discussed above and is flexible in terms of trading the accuracy with the time required for the computations.","segRedList":[],"synonymsContent":"","result":[]},{"content":"","segRedList":[],"synonymsContent":"","result":[]},{"content":"Key Words:","segRedList":[],"synonymsContent":"","result":[]},{"content":"Railway Trespassing detection，Railroad safety，Deep learning，Video surveillance，Deep Convolutional Neural Networks，Background subtraction，Computer vision，You Only Look Once.","segRedList":[],"synonymsContent":"","result":[]},{"content":"1.","segRedList":[],"synonymsContent":"","result":[]},{"content":"INTRODUCTION","segRedList":[],"synonymsContent":"","result":[]},{"content":"An interesting and yet one of the disturbing facts of this century has been related to railway security.","segRedList":[],"synonymsContent":"","result":[]},{"content":"In the last decade and till mid-2015，over 2700 deaths and more than 10000 injuries related to railway trespassing have been reported.","segRedList":[],"synonymsContent":"","result":[]},{"content":"This has further worsened with the ministry of railways reporting nearly 29，000-30，000 deaths in past 3 years.","segRedList":[],"synonymsContent":"","result":[]},{"content":"According to reports over 50%of railway crossings are exposed to potentially dangerous activities.","segRedList":[],"synonymsContent":"","result":[]},{"content":"Such sites pose a great threat to loss of life.","segRedList":["threat","great","pose","to","a"],"synonymsContent":"","result":[{"score":0.5670995,"similaritySentence":"does not seem to pose a great threat to coastal wooden","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Such":"0"},{"sites":"0"},{"pose":"1"},{"a":"1"},{"great":"1"},{"threat":"1"},{"to":"1"},{"loss":"0"},{"of":"0"},{"life":"0"},{".":"0"}],"similaritySegGreyList":[{"does":"0"},{"not":"0"},{"seem":"0"},{"to":"1"},{"pose":"1"},{"a":"1"},{"great":"1"},{"threat":"1"},{"to":"1"},{"coastal":"0"},{"wooden":"0"}],"subSimilaritySentenceSection":"which similarly to Nototeredo norvagica， does not seem to pose a great threat to coastal wooden structures.This species has been","duplicateSourceMD5":"528E2F46705986BD2B29F4C58359E0CE"},{"score":0.55838877,"similaritySentence":"such objects pose a real threat to life on the Earth","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Such":"1"},{"sites":"0"},{"pose":"1"},{"a":"1"},{"great":"0"},{"threat":"1"},{"to":"1"},{"loss":"0"},{"of":"0"},{"life":"1"},{".":"1"}],"similaritySegGreyList":[{"such":"1"},{"objects":"0"},{"pose":"1"},{"a":"1"},{"real":"0"},{"threat":"1"},{"to":"1"},{"life":"1"},{"on":"0"},{"the":"0"},{"Earth":"0"}],"subSimilaritySentenceSection":")，it is clear that such objects pose a real threat to life on the Earth .The immediacy of this threat","duplicateSourceMD5":"82A2FEFF0F13CC66F38A0289FA531739"},{"score":0.51574564,"similaritySentence":"landslides pose a great threat to shipping safety，human lives","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Such":"0"},{"sites":"0"},{"pose":"1"},{"a":"1"},{"great":"1"},{"threat":"1"},{"to":"1"},{"loss":"0"},{"of":"0"},{"life":"1"},{".":"1"}],"similaritySegGreyList":[{"landslides":"0"},{"pose":"1"},{"a":"1"},{"great":"1"},{"threat":"1"},{"to":"1"},{"shipping":"0"},{"safety":"0"},{"，":"0"},{"human":"0"},{"lives":"1"}],"subSimilaritySentenceSection":"Gorges Reservoir，China Abstract Reservoir landslides pose a great threat to shipping safety，human lives and properties，and the operation","duplicateSourceMD5":"C6B5BA9503723A20F732132F90FCD600"}]},{"content":"In most cases，collision with a train happens to be a reason of death of the trespasser.","segRedList":[],"synonymsContent":"","result":[]},{"content":"","segRedList":[],"synonymsContent":"","result":[]},{"content":"In such scenarios human supervision become challenging provided the scale at which the deaths are happening.","segRedList":[],"synonymsContent":"","result":[]},{"content":"Due to this，deploying an automated artificial intelligence tool becomes vital.","segRedList":["intelligence","artificial","automated","an","to","a"],"synonymsContent":"","result":[{"score":0.5923891,"similaritySentence":"cycles.Thus，a fully automated artificial intelligence program has to be","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Due":"0"},{"to":"1"},{"this":"0"},{"，":"0"},{"deploying":"0"},{"an":"1"},{"automated":"1"},{"artificial":"1"},{"intelligence":"1"},{"tool":"0"},{"becomes":"0"},{"vital":"0"},{".":"0"}],"similaritySegGreyList":[{"cycles":"0"},{".":"0"},{"Thus":"0"},{"，":"0"},{"a":"1"},{"fully":"0"},{"automated":"1"},{"artificial":"1"},{"intelligence":"1"},{"program":"0"},{"has":"0"},{"to":"1"},{"be":"0"}],"subSimilaritySentenceSection":"also requires predicting the global business cycles.Thus，a fully automated artificial intelligence program has to be able to model the expectations for","duplicateSourceMD5":"94CE124899D94D45883F77E8A0C73808"},{"score":0.513431,"similaritySentence":"pace.Thus，a new mode of using artificial intelligence to identify","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Due":"0"},{"to":"1"},{"this":"0"},{"，":"0"},{"deploying":"0"},{"an":"1"},{"automated":"0"},{"artificial":"1"},{"intelligence":"1"},{"tool":"0"},{"becomes":"0"},{"vital":"0"},{".":"0"}],"similaritySegGreyList":[{"pace":"0"},{".":"0"},{"Thus":"0"},{"，":"0"},{"a":"1"},{"new":"0"},{"mode":"0"},{"of":"0"},{"using":"0"},{"artificial":"1"},{"intelligence":"1"},{"to":"1"},{"identify":"0"}],"subSimilaritySentenceSection":"attacks are increasing at an accelerating pace.Thus，a new mode of using artificial intelligence to identify existing vulnerabilities and patching them in","duplicateSourceMD5":"BF8F74F6DF66F820F676323892A2ECF5"},{"score":0.513431,"similaritySentence":"to be able to link to an artificial intelligence source，e.","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Due":"0"},{"to":"1"},{"this":"0"},{"，":"0"},{"deploying":"0"},{"an":"1"},{"automated":"0"},{"artificial":"1"},{"intelligence":"1"},{"tool":"0"},{"becomes":"0"},{"vital":"0"},{".":"0"}],"similaritySegGreyList":[{"to":"1"},{"be":"0"},{"able":"0"},{"to":"1"},{"link":"0"},{"to":"1"},{"an":"1"},{"artificial":"1"},{"intelligence":"1"},{"source":"0"},{"，":"0"},{"e":"0"},{".":"0"}],"subSimilaritySentenceSection":"databases.There is a need to be able to link to an artificial intelligence source，e. g.，neural networks that","duplicateSourceMD5":"110B26EE1C88F3EC3A5E4A6D00374032"},{"score":0.513431,"similaritySentence":"to deploy A.I.or artificial intelligence.2Estimates of the","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"Due":"0"},{"to":"1"},{"this":"0"},{"，":"0"},{"deploying":"1"},{"an":"1"},{"automated":"0"},{"artificial":"1"},{"intelligence":"1"},{"tool":"0"},{"becomes":"0"},{"vital":"0"},{".":"0"}],"similaritySegGreyList":[{"to":"1"},{"deploy":"1"},{"A":"1"},{".":"0"},{"I":"0"},{".":"0"},{"or":"0"},{"artificial":"1"},{"intelligence":"1"},{".":"0"},{"2Estimates":"0"},{"of":"0"},{"the":"0"}],"subSimilaritySentenceSection":"term Big Data includes its capacity to deploy A.I.or artificial intelligence.2Estimates of the storage capacity of the NSA data","duplicateSourceMD5":"B3D274932F790AE7CF7425304C746D87"},{"score":0.506216,"similaritySentence":"are deployed，these weapons will use artificial intelligence to select and attack","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Due":"0"},{"to":"1"},{"this":"0"},{"，":"0"},{"deploying":"1"},{"an":"0"},{"automated":"0"},{"artificial":"1"},{"intelligence":"1"},{"tool":"0"},{"becomes":"0"},{"vital":"0"},{".":"0"}],"similaritySegGreyList":[{"are":"0"},{"deployed":"1"},{"，":"0"},{"these":"0"},{"weapons":"0"},{"will":"0"},{"use":"0"},{"artificial":"1"},{"intelligence":"1"},{"to":"1"},{"select":"0"},{"and":"0"},{"attack":"0"}],"subSimilaritySentenceSection":"is fast approaching.Once they are deployed，these weapons will use artificial intelligence to select and attack targets without further human intervention.","duplicateSourceMD5":"18C3B192E298D332E5DA654AB8AC01F5"},{"score":0.5002035,"similaritySentence":".To this end，the use of Artificial Intelligence and deep learning","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Due":"0"},{"to":"1"},{"this":"1"},{"，":"0"},{"deploying":"0"},{"an":"0"},{"automated":"0"},{"artificial":"1"},{"intelligence":"1"},{"tool":"0"},{"becomes":"0"},{"vital":"0"},{".":"0"}],"similaritySegGreyList":[{".":"1"},{"To":"1"},{"this":"1"},{"end":"0"},{"，":"0"},{"the":"0"},{"use":"0"},{"of":"0"},{"Artificial":"1"},{"Intelligence":"1"},{"and":"0"},{"deep":"0"},{"learning":"0"}],"subSimilaritySentenceSection":"very useful for any road authority .To this end，the use of Artificial Intelligence and deep learning technologies have gained traction in this","duplicateSourceMD5":"6E2756BFD3B6C5F8630AD5774A6C2688"}]},{"content":"Trespassing detection serves as the foundation in any automated AI-driven trespassing detection and alert solution.","segRedList":[],"synonymsContent":"","result":[]},{"content":"An automated trespassing detection system would not only provide reliable detection in a timely manner but will also allow us to do advanced analytics for trespassing in future.","segRedList":[],"synonymsContent":"","result":[]},{"content":"1.1 Motivation","segRedList":[],"synonymsContent":"","result":[]},{"content":"Being responsible and having awareness about the rules to be followed at the railway station are the things we lack.","segRedList":[],"synonymsContent":"","result":[]},{"content":"In order to save a few minutes or seconds people are willing to put their life on the line.","segRedList":[],"synonymsContent":"","result":[]},{"content":"Following necessary precaution is extremely important but at the same time it is hard to expect everyone to abide by the rules.","segRedList":[],"synonymsContent":"","result":[]},{"content":"To overcome the above challenge attempts have been made by researchers and governments to build a reliable solution to detect trespassing on rail tracks.","segRedList":[],"synonymsContent":"","result":[]},{"content":"Such attempts have been proven to decrease fatality rate and thereby forming the foundation of our project to build a robust trespassing detection and alert system.","segRedList":[],"synonymsContent":"","result":[]},{"content":"1.2 Problem Statement","segRedList":[],"synonymsContent":"","result":[]},{"content":"The railways being a huge network connecting different parts of the country also causes exposure of the tracks and aids in human trespassing causing a lot of deaths.","segRedList":[],"synonymsContent":"","result":[]},{"content":"A large number of crowds makes it difficult to identify and separate those from trespassing and those following the rules.","segRedList":[],"synonymsContent":"","result":[]},{"content":"","segRedList":[],"synonymsContent":"","result":[]},{"content":"In 2019，Muzammil Bashir，Elike A.","segRedList":[],"synonymsContent":"","result":[]},{"content":"Rundensteiner and Ramoza Ahsan came with their research paper titled\"A deep learning approach to trespassing detection using video surveillance data\"[1].","segRedList":["surveillance","trespassing","detection","learning","approach","using","video","data","deep","and","to","1","A"],"synonymsContent":"","result":[{"score":0.619403,"similaritySentence":"Request PDF On Dec 1， 2019， Muzammil Bashir and others published A deep learning approach to trespassing detection using video surveillance data Find，","articleType":"net","classification":"net","originalSegGreyList":[{"Rundensteiner":"0"},{"and":"1"},{"Ramoza":"0"},{"Ahsan":"0"},{"came":"0"},{"with":"0"},{"their":"0"},{"research":"0"},{"paper":"0"},{"titled":"0"},{"\"":"0"},{"A":"1"},{"deep":"1"},{"learning":"1"},{"approach":"1"},{"to":"1"},{"trespassing":"1"},{"detection":"1"},{"using":"1"},{"video":"1"},{"surveillance":"1"},{"data":"1"},{"\"":"1"},{"[":"1"},{"1":"1"},{"]":"1"},{".":"1"}],"similaritySegGreyList":[{"Request":"0"},{"PDF":"0"},{"On":"0"},{"Dec":"0"},{"1":"1"},{"，":"0"},{"2019":"0"},{"，":"0"},{"Muzammil":"0"},{"Bashir":"0"},{"and":"1"},{"others":"0"},{"published":"0"},{"A":"1"},{"deep":"1"},{"learning":"1"},{"approach":"1"},{"to":"1"},{"trespassing":"1"},{"detection":"1"},{"using":"1"},{"video":"1"},{"surveillance":"1"},{"data":"1"},{"Find":"0"},{"，":"0"}],"subSimilaritySentenceSection":"Request PDF On Dec 1， 2019， Muzammil Bashir and others published A deep learning approach to trespassing detection using video surveillance data Find，","duplicateSourceMD5":"D51964199048317FAB487BDA28A8CB5E"}]},{"content":"In this paper they have researched and demonstrated trespassing monitoring using video surveillance footage.","segRedList":[],"synonymsContent":"","result":[]},{"content":"The architecture they designed gives an intuition to solve the problem.","segRedList":["designed","problem","solving","design","solve","The","the","to"],"synonymsContent":"","result":[{"score":0.5211988,"similaritySentence":"the FSO team to design the CiS artefacts for solving the problem","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"The":"1"},{"architecture":"0"},{"they":"0"},{"designed":"1"},{"gives":"0"},{"an":"0"},{"intuition":"0"},{"to":"1"},{"solve":"1"},{"the":"1"},{"problem":"1"},{".":"1"}],"similaritySegGreyList":[{"the":"1"},{"FSO":"0"},{"team":"0"},{"to":"1"},{"design":"1"},{"the":"1"},{"CiS":"0"},{"artefacts":"0"},{"for":"0"},{"solving":"1"},{"the":"1"},{"problem":"1"}],"subSimilaritySentenceSection":"worked(ADR intervention)with the FSO team to design the CiS artefacts for solving the problem at hand.The core of","duplicateSourceMD5":"66A034B938C123A3531D56DAC06E10B7"},{"score":0.5211988,"similaritySentence":"the brain.The ANN can be designed to solve the problem","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"The":"1"},{"architecture":"0"},{"they":"0"},{"designed":"1"},{"gives":"0"},{"an":"0"},{"intuition":"0"},{"to":"1"},{"solve":"1"},{"the":"1"},{"problem":"1"},{".":"1"}],"similaritySegGreyList":[{"the":"1"},{"brain":"0"},{".":"0"},{"The":"1"},{"ANN":"0"},{"can":"0"},{"be":"0"},{"designed":"1"},{"to":"1"},{"solve":"1"},{"the":"1"},{"problem":"1"}],"subSimilaritySentenceSection":"the\u0027recall\u0027function of the brain.The ANN can be designed to solve the problem by regression as well as by","duplicateSourceMD5":"D1E56189BCB9BF5CEADDA108CBB7A3DC"}]},{"content":"Keeping in mind the approaches made previously we further propose a system that is capable to detecting trespassing activity on railway tracks using video","segRedList":[],"synonymsContent":"","result":[]},{"content":"surveillance data and sending an alert in form of alarm or notification along with the location where the trespassing has happened.","segRedList":[],"synonymsContent":"","result":[]},{"content":"This solution will help in drastically reducing the deaths caused by human trespassing.","segRedList":[],"synonymsContent":"","result":[]},{"content":"2.","segRedList":[],"synonymsContent":"","result":[]},{"content":"LITERATURE SURVEY","segRedList":[],"synonymsContent":"","result":[]},{"content":"Sr.","segRedList":[],"synonymsContent":"","result":[]},{"content":"No.","segRedList":[],"synonymsContent":"","result":[]},{"content":"Title","segRedList":[],"synonymsContent":"","result":[]},{"content":"Authors","segRedList":[],"synonymsContent":"","result":[]},{"content":"Year","segRedList":[],"synonymsContent":"","result":[]},{"content":"Description/Approach","segRedList":[],"synonymsContent":"","result":[]},{"content":"1","segRedList":[],"synonymsContent":"","result":[]},{"content":"A deep learning approach to trespassing detection","segRedList":[],"synonymsContent":"","result":[]},{"content":"using video surveillance data","segRedList":[],"synonymsContent":"","result":[]},{"content":"Muzammil Bashir，Elke A.","segRedList":[],"synonymsContent":"","result":[]},{"content":"Rundensteiner，Ramoza Ahsan","segRedList":[],"synonymsContent":"","result":[]},{"content":"2019","segRedList":[],"synonymsContent":"","result":[]},{"content":"In this paper the researchers have developed their automated framework called ARTS-Automated railroad trespassing detection system.","segRedList":[],"synonymsContent":"","result":[]},{"content":"","segRedList":[],"synonymsContent":"","result":[]},{"content":"This system uses a 2 stage approach based on CNN to solve the trespassing detection.","segRedList":[],"synonymsContent":"","result":[]},{"content":"2","segRedList":[],"synonymsContent":"","result":[]},{"content":"A deep learning approach towards railway safety risk assessment","segRedList":[],"synonymsContent":"","result":[]},{"content":"Hamad Alawad，Sakdirat Kaewunruen，Min An","segRedList":[],"synonymsContent":"","result":[]},{"content":"2020","segRedList":[],"synonymsContent":"","result":[]},{"content":"It proposes an effective Realtime risk management solution based on CNN to improve safety throughout the entire railway industry by preventing fatal accidents.","segRedList":[],"synonymsContent":"","result":[]},{"content":"3","segRedList":[],"synonymsContent":"","result":[]},{"content":"YOLO Based Real-Time Human Detection for Smart Video Surveillance at the Edge","segRedList":["Surveillance","Detection","Based","Smart","Human","Video","Real","YOLO","Time","for","at"],"synonymsContent":"","result":[{"score":0.71653795,"similaritySentence":"others published YOLO Based Real-Time Human Detection for Smart Video Surveillance at","articleType":"net","classification":"net","originalSegGreyList":[{"YOLO":"1"},{"Based":"1"},{"Real":"1"},{"-":"1"},{"Time":"1"},{"Human":"1"},{"Detection":"1"},{"for":"1"},{"Smart":"1"},{"Video":"1"},{"Surveillance":"1"},{"at":"1"},{"the":"0"},{"Edge":"0"}],"similaritySegGreyList":[{"others":"0"},{"published":"0"},{"YOLO":"1"},{"Based":"1"},{"Real":"1"},{"-":"1"},{"Time":"1"},{"Human":"1"},{"Detection":"1"},{"for":"1"},{"Smart":"1"},{"Video":"1"},{"Surveillance":"1"},{"at":"1"}],"subSimilaritySentenceSection":"2021，Huy Hoang Nguyen and others published YOLO Based Real-Time Human Detection for Smart Video Surveillance at","duplicateSourceMD5":"9F884CB9CA9E82621BF9BE2A0045B012"},{"score":0.5578703,"similaritySentence":"propose a real-time blob detection algorithm for software video surveillance.Their","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"YOLO":"0"},{"Based":"0"},{"Real":"1"},{"-":"1"},{"Time":"1"},{"Human":"0"},{"Detection":"1"},{"for":"1"},{"Smart":"0"},{"Video":"1"},{"Surveillance":"1"},{"at":"0"},{"the":"0"},{"Edge":"0"}],"similaritySegGreyList":[{"propose":"0"},{"a":"0"},{"real":"1"},{"-":"1"},{"time":"1"},{"blob":"0"},{"detection":"1"},{"algorithm":"0"},{"for":"1"},{"software":"0"},{"video":"1"},{"surveillance":"1"},{".":"0"},{"Their":"0"}],"subSimilaritySentenceSection":"The authors in[32] propose a real-time blob detection algorithm for software video surveillance.Their method relies on applying a correction","duplicateSourceMD5":"E2F0C87830FCC0309C43EA5072E54009"},{"score":0.5014881,"similaritySentence":"resolution real-time object detection algorithm for drone and surveillance video capture in","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"YOLO":"0"},{"Based":"0"},{"Real":"1"},{"-":"1"},{"Time":"1"},{"Human":"0"},{"Detection":"1"},{"for":"1"},{"Smart":"0"},{"Video":"1"},{"Surveillance":"1"},{"at":"0"},{"the":"0"},{"Edge":"0"}],"similaritySegGreyList":[{"resolution":"0"},{"real":"1"},{"-":"1"},{"time":"1"},{"object":"0"},{"detection":"1"},{"algorithm":"0"},{"for":"1"},{"drone":"0"},{"and":"0"},{"surveillance":"1"},{"video":"1"},{"capture":"0"},{"in":"0"}],"subSimilaritySentenceSection":"MEC-ELM as a low resolution real-time object detection algorithm for drone and surveillance video capture in the agriculture industry.Choosing between","duplicateSourceMD5":"2560D23E3340FC264E206D0C3F227FBE"}]},{"content":"Huy Hoang Nguyen，","segRedList":[],"synonymsContent":"","result":[]},{"content":"Thi Nhung Ta，Ngoc Cuong Nguyen，Van Truong Bui，","segRedList":["Truong","Nguyen","Cuong","Nhung","Ngoc","Bui","Van","Thi","Ta"],"synonymsContent":"","result":[{"score":1.0,"similaritySentence":"Thi Nhung Ta，Ngoc Cuong Nguyen，Van Truong Bui，","articleType":"net","classification":"net","originalSegGreyList":[{"Thi":"1"},{"Nhung":"1"},{"Ta":"1"},{"，":"1"},{"Ngoc":"1"},{"Cuong":"1"},{"Nguyen":"1"},{"，":"1"},{"Van":"1"},{"Truong":"1"},{"Bui":"1"},{"，":"1"}],"similaritySegGreyList":[{"Thi":"1"},{"Nhung":"1"},{"Ta":"1"},{"，":"1"},{"Ngoc":"1"},{"Cuong":"1"},{"Nguyen":"1"},{"，":"1"},{"Van":"1"},{"Truong":"1"},{"Bui":"1"},{"，":"1"}],"subSimilaritySentenceSection":"Huy Hoang Nguyen，Thi Nhung Ta，Ngoc Cuong Nguyen，Van Truong Bui， Hung Manh Pham，Duc.","duplicateSourceMD5":"B71EA0058FE152C3868D36E228640C9D"},{"score":0.53968257,"similaritySentence":"，Huu Noi Nguyen，Ngoc Tran Nguyen，Cao Truong Tran","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"Thi":"0"},{"Nhung":"0"},{"Ta":"0"},{"，":"0"},{"Ngoc":"1"},{"Cuong":"0"},{"Nguyen":"1"},{"，":"0"},{"Van":"0"},{"Truong":"1"},{"Bui":"0"},{"，":"0"}],"similaritySegGreyList":[{"，":"0"},{"Huu":"0"},{"Noi":"0"},{"Nguyen":"1"},{"，":"1"},{"Ngoc":"1"},{"Tran":"0"},{"Nguyen":"1"},{"，":"0"},{"Cao":"0"},{"Truong":"1"},{"Tran":"0"}],"subSimilaritySentenceSection":"Classification Systems 347 Bao Ngoc Vi ，Huu Noi Nguyen，Ngoc Tran Nguyen，Cao Truong Tran Alpha-DBL:A Reasonable","duplicateSourceMD5":"C38B82791EB4A8A4ACEF7B52F2173091"},{"score":0.5347222,"similaritySentence":"Thi Nhung Le，Viet Hung Nguyen，Ngoc Sy Dinh，","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Thi":"1"},{"Nhung":"1"},{"Ta":"0"},{"，":"0"},{"Ngoc":"1"},{"Cuong":"0"},{"Nguyen":"1"},{"，":"0"},{"Van":"0"},{"Truong":"0"},{"Bui":"0"},{"，":"0"}],"similaritySegGreyList":[{"Thi":"1"},{"Nhung":"1"},{"Le":"0"},{"，":"0"},{"Viet":"0"},{"Hung":"0"},{"Nguyen":"1"},{"，":"1"},{"Ngoc":"1"},{"Sy":"0"},{"Dinh":"0"},{"，":"0"}],"subSimilaritySentenceSection":"Nguyen，Tran Ngoc Buu， Thi Nhung Le，Viet Hung Nguyen，Ngoc Sy Dinh， Warwick John Britton，Guy Barrington","duplicateSourceMD5":"E1F80552775291EB0C1CD87E75D816E1"},{"score":0.5315041,"similaritySentence":"Thi Nhung，Nguyen Thi Bich Van，Nguyen Van Cuong，","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Thi":"1"},{"Nhung":"1"},{"Ta":"0"},{"，":"0"},{"Ngoc":"0"},{"Cuong":"1"},{"Nguyen":"1"},{"，":"1"},{"Van":"1"},{"Truong":"0"},{"Bui":"0"},{"，":"0"}],"similaritySegGreyList":[{"Thi":"1"},{"Nhung":"1"},{"，":"1"},{"Nguyen":"1"},{"Thi":"1"},{"Bich":"0"},{"Van":"1"},{"，":"1"},{"Nguyen":"1"},{"Van":"1"},{"Cuong":"1"},{"，":"1"}],"subSimilaritySentenceSection":"markets and supermarkets in Vietnam Nguyen Thi Nhung，Nguyen Thi Bich Van，Nguyen Van Cuong， Truong Thi Quy Duong，Tran","duplicateSourceMD5":"BE896F2E1764813241FA53E53C6C6DB5"},{"score":0.5165989,"similaritySentence":"Trong Hoan，Vu Tan Phuong，Nguyen Van Truong and D","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Thi":"0"},{"Nhung":"0"},{"Ta":"0"},{"，":"0"},{"Ngoc":"0"},{"Cuong":"0"},{"Nguyen":"1"},{"，":"1"},{"Van":"1"},{"Truong":"1"},{"Bui":"0"},{"，":"0"}],"similaritySegGreyList":[{"Trong":"0"},{"Hoan":"0"},{"，":"0"},{"Vu":"0"},{"Tan":"0"},{"Phuong":"0"},{"，":"0"},{"Nguyen":"1"},{"Van":"1"},{"Truong":"1"},{"and":"0"},{"D":"0"}],"subSimilaritySentenceSection":":31-41.Do Trong Hoan，Vu Tan Phuong，Nguyen Van Truong and D .Catacutan(2018)‘","duplicateSourceMD5":"8FF381C97659A93E015B9D3B3598A1A9"},{"score":0.5,"similaritySentence":"Minh，Nguyen，Van Nam，Nguyen，Van Tri，","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Thi":"0"},{"Nhung":"0"},{"Ta":"0"},{"，":"0"},{"Ngoc":"0"},{"Cuong":"0"},{"Nguyen":"1"},{"，":"1"},{"Van":"1"},{"Truong":"0"},{"Bui":"0"},{"，":"0"}],"similaritySegGreyList":[{"Minh":"0"},{"，":"0"},{"Nguyen":"1"},{"，":"1"},{"Van":"1"},{"Nam":"0"},{"，":"0"},{"Nguyen":"1"},{"，":"1"},{"Van":"1"},{"Tri":"0"},{"，":"0"}],"subSimilaritySentenceSection":"Miyamoto，T.，Thi Minh，Nguyen，Van Nam，Nguyen，Van Tri， Tran，2008.Collision zone","duplicateSourceMD5":"EE9C55097A42AD1117E1F34A859188C9"},{"score":0.5,"similaritySentence":"Thi Hong Hanh，Hai Dang Nguyen，Tran Hong Quang，","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Thi":"1"},{"Nhung":"0"},{"Ta":"0"},{"，":"0"},{"Ngoc":"0"},{"Cuong":"0"},{"Nguyen":"1"},{"，":"0"},{"Van":"0"},{"Truong":"0"},{"Bui":"0"},{"，":"0"}],"similaritySegGreyList":[{"Thi":"1"},{"Hong":"0"},{"Hanh":"0"},{"，":"0"},{"Hai":"0"},{"Dang":"0"},{"Nguyen":"1"},{"，":"0"},{"Tran":"0"},{"Hong":"0"},{"Quang":"0"},{"，":"0"}],"subSimilaritySentenceSection":":Ninh Thi Ngoc，Tran Thi Hong Hanh，Hai Dang Nguyen，Tran Hong Quang， Nguyen Xuan Cuong，Nguyen Hoai","duplicateSourceMD5":"FDFAD728DD562141EC2985D7E9697ECF"}]},{"content":"Hung Manh Pham，","segRedList":[],"synonymsContent":"","result":[]},{"content":"Duc Minh Nguyen","segRedList":[],"synonymsContent":"","result":[]},{"content":"2021","segRedList":[],"synonymsContent":"","result":[]},{"content":"This paper proposes an approach based on YOLOv2 for human detection.","segRedList":["detection","proposes","paper","This","for","an","a"],"synonymsContent":"","result":[{"score":0.57341266,"similaritySentence":"This paper proposes a new deep neural network for object detection.","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"This":"1"},{"paper":"1"},{"proposes":"1"},{"an":"1"},{"approach":"0"},{"based":"0"},{"on":"0"},{"YOLOv2":"0"},{"for":"1"},{"human":"0"},{"detection":"1"},{".":"1"}],"similaritySegGreyList":[{"This":"1"},{"paper":"1"},{"proposes":"1"},{"a":"1"},{"new":"0"},{"deep":"0"},{"neural":"0"},{"network":"0"},{"for":"1"},{"object":"0"},{"detection":"1"},{".":"1"}],"subSimilaritySentenceSection":"Object detection Attention Single shot detection This paper proposes a new deep neural network for object detection. The proposed network，termed ASSD","duplicateSourceMD5":"60717DEA72E0BD144E437D77ED33E1D0"},{"score":0.5426245,"similaritySentence":"IOU tracker for object tracking based on YOLOv2 for object detection.","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"This":"0"},{"paper":"0"},{"proposes":"0"},{"an":"0"},{"approach":"0"},{"based":"1"},{"on":"1"},{"YOLOv2":"1"},{"for":"1"},{"human":"0"},{"detection":"1"},{".":"1"}],"similaritySegGreyList":[{"IOU":"0"},{"tracker":"0"},{"for":"1"},{"object":"0"},{"tracking":"0"},{"based":"1"},{"on":"1"},{"YOLOv2":"1"},{"for":"1"},{"object":"0"},{"detection":"1"},{".":"1"}],"subSimilaritySentenceSection":"framework using a Kalman filter and IOU tracker for object tracking based on YOLOv2 for object detection. In the proposed algorithm，trajectory","duplicateSourceMD5":"59E8575AF48C5286BCD3DFA193D0A6A6"},{"score":0.52272725,"similaritySentence":"This paper proposes a YOLOv3-based model for the recognition of","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"This":"1"},{"paper":"1"},{"proposes":"1"},{"an":"1"},{"approach":"0"},{"based":"1"},{"on":"0"},{"YOLOv2":"0"},{"for":"1"},{"human":"0"},{"detection":"0"},{".":"0"}],"similaritySegGreyList":[{"This":"1"},{"paper":"1"},{"proposes":"1"},{"a":"1"},{"YOLOv3":"0"},{"-":"0"},{"based":"1"},{"model":"0"},{"for":"1"},{"the":"0"},{"recognition":"0"},{"of":"0"}],"subSimilaritySentenceSection":"Faster R-CNN 5 Conclusion This paper proposes a YOLOv3-based model for the recognition of WT blade damage from surveillance drone","duplicateSourceMD5":"0970A8C70A90A7F8DA96F522D4713885"},{"score":0.52146465,"similaritySentence":"of UAVs，this paper proposes a UAV-based visual detection","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"This":"1"},{"paper":"1"},{"proposes":"1"},{"an":"1"},{"approach":"0"},{"based":"1"},{"on":"0"},{"YOLOv2":"0"},{"for":"0"},{"human":"0"},{"detection":"1"},{".":"1"}],"similaritySegGreyList":[{"of":"0"},{"UAVs":"0"},{"，":"0"},{"this":"1"},{"paper":"1"},{"proposes":"1"},{"a":"1"},{"UAV":"0"},{"-":"0"},{"based":"1"},{"visual":"0"},{"detection":"1"}],"subSimilaritySentenceSection":"more convenient.With the merits of UAVs，this paper proposes a UAV-based visual detection technology for green mangoes on trees","duplicateSourceMD5":"DF2A04FC95A3D5A71245BFE65BD8A008"},{"score":0.5205026,"similaritySentence":"This paper proposes a novel approach of bounding box based pothole localization","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"This":"1"},{"paper":"1"},{"proposes":"1"},{"an":"1"},{"approach":"1"},{"based":"1"},{"on":"0"},{"YOLOv2":"0"},{"for":"0"},{"human":"0"},{"detection":"0"},{".":"0"}],"similaritySegGreyList":[{"This":"1"},{"paper":"1"},{"proposes":"1"},{"a":"1"},{"novel":"0"},{"approach":"1"},{"of":"0"},{"bounding":"0"},{"box":"0"},{"based":"1"},{"pothole":"0"},{"localization":"0"}],"subSimilaritySentenceSection":"and maintenance of the roads. This paper proposes a novel approach of bounding box based pothole localization from thermal images using deep neural","duplicateSourceMD5":"801B41E1699DD64124622BD55575CCF3"},{"score":0.5068267,"similaritySentence":"This paper proposes a real-time detection method for urban pedestrian","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"This":"1"},{"paper":"1"},{"proposes":"1"},{"an":"1"},{"approach":"0"},{"based":"0"},{"on":"0"},{"YOLOv2":"0"},{"for":"1"},{"human":"0"},{"detection":"1"},{".":"1"}],"similaritySegGreyList":[{"This":"1"},{"paper":"1"},{"proposes":"1"},{"a":"1"},{"real":"0"},{"-":"0"},{"time":"0"},{"detection":"1"},{"method":"0"},{"for":"1"},{"urban":"0"},{"pedestrian":"0"}],"subSimilaritySentenceSection":"high robustness.5.Conclusion This paper proposes a real-time detection method for urban pedestrian flow monitoring based on improved YOLOv3","duplicateSourceMD5":"8D9D1CCE4D2FCE6ED0407C5371E1B56C"},{"score":0.50505054,"similaritySentence":"this paper proposes a new detection method based on YOLOv3 for non","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"This":"1"},{"paper":"1"},{"proposes":"1"},{"an":"1"},{"approach":"0"},{"based":"1"},{"on":"1"},{"YOLOv2":"0"},{"for":"1"},{"human":"0"},{"detection":"1"},{".":"1"}],"similaritySegGreyList":[{"this":"1"},{"paper":"1"},{"proposes":"1"},{"a":"1"},{"new":"0"},{"detection":"1"},{"method":"0"},{"based":"1"},{"on":"1"},{"YOLOv3":"0"},{"for":"1"},{"non":"0"}],"subSimilaritySentenceSection":".To improve aboard safety， this paper proposes a new detection method based on YOLOv3 for non -helmet-use behaviour of","duplicateSourceMD5":"F687FD8CC8FF0871962AC18D3167D310"}]},{"content":"This approach utilizes combined benefits of YOLO residual blocks and multiple spatial pyramid pooling blocks.","segRedList":[],"synonymsContent":"","result":[]},{"content":"The proposed model shows high accuracy across various datasets.","segRedList":["datasets","proposed","various","models","model","the","The"],"synonymsContent":"","result":[{"score":0.59679484,"similaritySentence":"of the proposed models.We have utilized various datasets","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"The":"1"},{"proposed":"1"},{"model":"1"},{"shows":"0"},{"high":"0"},{"accuracy":"0"},{"across":"0"},{"various":"1"},{"datasets":"1"},{".":"1"}],"similaritySegGreyList":[{"of":"0"},{"the":"1"},{"proposed":"1"},{"models":"1"},{".":"0"},{"We":"0"},{"have":"0"},{"utilized":"0"},{"various":"1"},{"datasets":"1"}],"subSimilaritySentenceSection":"employed to improve the classification capability of the proposed models.We have utilized various datasets with varying degrees of imbalance and","duplicateSourceMD5":"E1E5858958D9F9776DDE8ACB99D6B868"},{"score":0.5647436,"similaritySentence":"IG ’ tries to maintain high accuracy across various machine","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"The":"0"},{"proposed":"0"},{"model":"0"},{"shows":"0"},{"high":"1"},{"accuracy":"1"},{"across":"1"},{"various":"1"},{"datasets":"0"},{".":"0"}],"similaritySegGreyList":[{"IG":"0"},{"’":"0"},{"tries":"0"},{"to":"0"},{"maintain":"0"},{"high":"1"},{"accuracy":"1"},{"across":"1"},{"various":"1"},{"machine":"0"}],"subSimilaritySentenceSection":".The dataset ‘ DS03_ IG ’ tries to maintain high accuracy across various machine learning models.In Fig.","duplicateSourceMD5":"7153D857B624D4B1BD9832D213D3F901"},{"score":0.5647436,"similaritySentence":"the proposed model accuracy.As a result，it","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"The":"1"},{"proposed":"1"},{"model":"1"},{"shows":"0"},{"high":"0"},{"accuracy":"1"},{"across":"0"},{"various":"0"},{"datasets":"0"},{".":"0"}],"similaritySegGreyList":[{"the":"1"},{"proposed":"1"},{"model":"1"},{"accuracy":"1"},{".":"0"},{"As":"0"},{"a":"0"},{"result":"0"},{"，":"0"},{"it":"0"}],"subSimilaritySentenceSection":"and Fig.13 shows that the proposed model accuracy.As a result，it is inferred that the Parallel DCNN","duplicateSourceMD5":"DE57BAD1DEE299DAA375EAF358B85346"},{"score":0.5326924,"similaritySentence":"the proposed model shows better accuracy.Table 4 shows","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"The":"1"},{"proposed":"1"},{"model":"1"},{"shows":"1"},{"high":"0"},{"accuracy":"1"},{"across":"0"},{"various":"0"},{"datasets":"0"},{".":"0"}],"similaritySegGreyList":[{"the":"1"},{"proposed":"1"},{"model":"1"},{"shows":"1"},{"better":"0"},{"accuracy":"1"},{".":"0"},{"Table":"0"},{"4":"0"},{"shows":"1"}],"subSimilaritySentenceSection":".According to this definition， the proposed model shows better accuracy.Table 4 shows how much the proposed model is","duplicateSourceMD5":"78E9D5365C8108908EE8C240767EE593"},{"score":0.509836,"similaritySentence":"the proposed model improved the accuracy ofthe classification models from","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"The":"1"},{"proposed":"1"},{"model":"1"},{"shows":"0"},{"high":"0"},{"accuracy":"1"},{"across":"0"},{"various":"0"},{"datasets":"0"},{".":"0"}],"similaritySegGreyList":[{"the":"1"},{"proposed":"1"},{"model":"1"},{"improved":"0"},{"the":"1"},{"accuracy":"1"},{"ofthe":"0"},{"classification":"0"},{"models":"1"},{"from":"0"}],"subSimilaritySentenceSection":"on various public datasets showed that the proposed model improved the accuracy ofthe classification models from 1%to 30%depending","duplicateSourceMD5":"EE735F0D5D42F5EA56571FCD66D80B18"},{"score":0.50064105,"similaritySentence":"2.3 Proposed model evaluation across the dataset To","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"The":"1"},{"proposed":"1"},{"model":"1"},{"shows":"0"},{"high":"0"},{"accuracy":"0"},{"across":"1"},{"various":"0"},{"datasets":"1"},{".":"1"}],"similaritySegGreyList":[{"2":"0"},{".":"0"},{"3":"0"},{"Proposed":"1"},{"model":"1"},{"evaluation":"0"},{"across":"1"},{"the":"1"},{"dataset":"1"},{"To":"0"}],"subSimilaritySentenceSection":"decent performance score.4. 2.3 Proposed model evaluation across the dataset To figure out the effect of inter","duplicateSourceMD5":"BE8D9B7E0738909498BF9495DA8B8B17"},{"score":0.50064105,"similaritySentence":"The proposed model provides an accuracy of 85.36","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"The":"1"},{"proposed":"1"},{"model":"1"},{"shows":"0"},{"high":"0"},{"accuracy":"1"},{"across":"0"},{"various":"0"},{"datasets":"0"},{".":"0"}],"similaritySegGreyList":[{"The":"1"},{"proposed":"1"},{"model":"1"},{"provides":"0"},{"an":"0"},{"accuracy":"1"},{"of":"0"},{"85":"0"},{".":"0"},{"36":"0"}],"subSimilaritySentenceSection":"clips taken from GTZAN dataset. The proposed model provides an accuracy of 85.36 %on 10-class genre","duplicateSourceMD5":"F98FB14B66AAF1600C28A915789E81CF"},{"score":0.5,"similaritySentence":"of the proposed model on all the benchmark datasets.","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"The":"1"},{"proposed":"1"},{"model":"1"},{"shows":"0"},{"high":"0"},{"accuracy":"0"},{"across":"0"},{"various":"0"},{"datasets":"1"},{".":"1"}],"similaritySegGreyList":[{"of":"0"},{"the":"1"},{"proposed":"1"},{"model":"1"},{"on":"0"},{"all":"0"},{"the":"1"},{"benchmark":"0"},{"datasets":"1"},{".":"1"}],"subSimilaritySentenceSection":"the ROC curve and AUC values of the proposed model on all the benchmark datasets. From the figure it is observed","duplicateSourceMD5":"AD5E673196F1FB44431A9E49625D6793"},{"score":0.5,"similaritySentence":"does not have a consistently high accuracy across datasets.","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"The":"0"},{"proposed":"0"},{"model":"0"},{"shows":"0"},{"high":"1"},{"accuracy":"1"},{"across":"1"},{"various":"0"},{"datasets":"1"},{".":"1"}],"similaritySegGreyList":[{"does":"0"},{"not":"0"},{"have":"0"},{"a":"0"},{"consistently":"0"},{"high":"1"},{"accuracy":"1"},{"across":"1"},{"datasets":"1"},{".":"1"}],"subSimilaritySentenceSection":"expert users)，while FA does not have a consistently high accuracy across datasets. Designing interpretable and consistently accurate approaches","duplicateSourceMD5":"321185DCB65D461565DAA6B107EF0587"}]},{"content":"4","segRedList":[],"synonymsContent":"","result":[]},{"content":"Analysis of Deep Learning Architectures for Object","segRedList":[],"synonymsContent":"","result":[]},{"content":"Detection-A Critical Review","segRedList":[],"synonymsContent":"","result":[]},{"content":"Mohit Pandiya，Sayonee Dassani，Dr.","segRedList":[],"synonymsContent":"","result":[]},{"content":"Mangalraj P","segRedList":[],"synonymsContent":"","result":[]},{"content":"2020","segRedList":[],"synonymsContent":"","result":[]},{"content":"This paper proposes a solution having 4 different architectures for comparative analysis and explains the optimality and compatibility.","segRedList":[],"synonymsContent":"","result":[]},{"content":"5","segRedList":[],"synonymsContent":"","result":[]},{"content":"Video Abnormal Event Detection Based on CNN and LSTM","segRedList":["Detection","Abnormal","Based","Event","Video","LSTM","CNN","and","on"],"synonymsContent":"","result":[{"score":1.0,"similaritySentence":"Video Abnormal Event Detection Based on CNN and LSTM","articleType":"net","classification":"net","originalSegGreyList":[{"Video":"1"},{"Abnormal":"1"},{"Event":"1"},{"Detection":"1"},{"Based":"1"},{"on":"1"},{"CNN":"1"},{"and":"1"},{"LSTM":"1"}],"similaritySegGreyList":[{"Video":"1"},{"Abnormal":"1"},{"Event":"1"},{"Detection":"1"},{"Based":"1"},{"on":"1"},{"CNN":"1"},{"and":"1"},{"LSTM":"1"}],"subSimilaritySentenceSection":"，Guangli Wu and others published Video Abnormal Event Detection Based on CNN and LSTM Find，read and cite all","duplicateSourceMD5":"447BB94F14E33D05AE2D2DD9673D43E4"},{"score":0.67398787,"similaritySentence":"H.Abnormal event detection based on analysis of","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Video":"0"},{"Abnormal":"1"},{"Event":"1"},{"Detection":"1"},{"Based":"1"},{"on":"1"},{"CNN":"0"},{"and":"0"},{"LSTM":"0"}],"similaritySegGreyList":[{"H":"0"},{".":"0"},{"Abnormal":"1"},{"event":"1"},{"detection":"1"},{"based":"1"},{"on":"1"},{"analysis":"0"},{"of":"0"}],"subSimilaritySentenceSection":"H，Lyu Q，Snoussi H.Abnormal event detection based on analysis of movement information of video sequence.","duplicateSourceMD5":"079C697D6CFBAFDCFA63468B36368192"},{"score":0.6541868,"similaritySentence":"data set Abnormal event detection based on analysis of","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Video":"0"},{"Abnormal":"1"},{"Event":"1"},{"Detection":"1"},{"Based":"1"},{"on":"1"},{"CNN":"0"},{"and":"0"},{"LSTM":"0"}],"similaritySegGreyList":[{"data":"0"},{"set":"0"},{"Abnormal":"1"},{"event":"1"},{"detection":"1"},{"based":"1"},{"on":"1"},{"analysis":"0"},{"of":"0"}],"subSimilaritySentenceSection":"Jones and Viola face detector Own data set Abnormal event detection based on analysis of movement information of video sequence[","duplicateSourceMD5":"079C697D6CFBAFDCFA63468B36368192"},{"score":0.6352283,"similaritySentence":"been done on abnormal flow detection based on LSTM","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Video":"0"},{"Abnormal":"1"},{"Event":"0"},{"Detection":"1"},{"Based":"1"},{"on":"1"},{"CNN":"0"},{"and":"0"},{"LSTM":"1"}],"similaritySegGreyList":[{"been":"0"},{"done":"0"},{"on":"1"},{"abnormal":"1"},{"flow":"0"},{"detection":"1"},{"based":"1"},{"on":"1"},{"LSTM":"1"}],"subSimilaritySentenceSection":"21].Existing researches have been done on abnormal flow detection based on LSTM [22]，and they","duplicateSourceMD5":"70B6BE744063329E42EA53E8B9EF9BA3"},{"score":0.6154684,"similaritySentence":"]proposed abnormal event detection based on motion attention","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Video":"0"},{"Abnormal":"1"},{"Event":"1"},{"Detection":"1"},{"Based":"1"},{"on":"1"},{"CNN":"0"},{"and":"0"},{"LSTM":"0"}],"similaritySegGreyList":[{"]":"0"},{"proposed":"0"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"based":"1"},{"on":"1"},{"motion":"0"},{"attention":"0"}],"subSimilaritySentenceSection":"et al.，[21 ]proposed abnormal event detection based on motion attention using sparse coding by comparing current","duplicateSourceMD5":"1F3550D5D08E80C9C62975DC2F66C461"},{"score":0.6111111,"similaritySentence":"of the abnormal event detection model based on tourism","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Video":"0"},{"Abnormal":"1"},{"Event":"1"},{"Detection":"1"},{"Based":"1"},{"on":"1"},{"CNN":"0"},{"and":"0"},{"LSTM":"0"}],"similaritySegGreyList":[{"of":"0"},{"the":"0"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"model":"0"},{"based":"1"},{"on":"1"},{"tourism":"0"}],"subSimilaritySentenceSection":"learning.The structure frame diagram of the abnormal event detection model based on tourism video is shown in Figure 1","duplicateSourceMD5":"587A9C20D6AC21B822775F78535AB1FB"},{"score":0.5770888,"similaritySentence":"video captioning have been based on CNN and LSTM","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Video":"1"},{"Abnormal":"0"},{"Event":"0"},{"Detection":"0"},{"Based":"1"},{"on":"1"},{"CNN":"1"},{"and":"1"},{"LSTM":"1"}],"similaritySegGreyList":[{"video":"1"},{"captioning":"0"},{"have":"0"},{"been":"0"},{"based":"1"},{"on":"1"},{"CNN":"1"},{"and":"1"},{"LSTM":"1"}],"subSimilaritySentenceSection":"Most of the recent studies on video captioning have been based on CNN and LSTM models because of their good captioning","duplicateSourceMD5":"B037B7FEEE8AD99097D4ADE11879FC60"},{"score":0.5733618,"similaritySentence":"proposed an abnormal event detection method based on Generative","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Video":"0"},{"Abnormal":"1"},{"Event":"1"},{"Detection":"1"},{"Based":"1"},{"on":"1"},{"CNN":"0"},{"and":"0"},{"LSTM":"0"}],"similaritySegGreyList":[{"proposed":"0"},{"an":"0"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"method":"0"},{"based":"1"},{"on":"1"},{"Generative":"0"}],"subSimilaritySentenceSection":".Ravanbakhsh et al.， proposed an abnormal event detection method based on Generative Adversarial Network(Ravan-bakhsh","duplicateSourceMD5":"7EE98964C187ED2FBE2C65D1C8E2C508"},{"score":0.5585702,"similaritySentence":"and for abnormal event detection.The pattern we","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"Video":"0"},{"Abnormal":"1"},{"Event":"1"},{"Detection":"1"},{"Based":"0"},{"on":"0"},{"CNN":"0"},{"and":"1"},{"LSTM":"0"}],"similaritySegGreyList":[{"and":"1"},{"for":"0"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{".":"0"},{"The":"0"},{"pattern":"0"},{"we":"0"}],"subSimilaritySentenceSection":"this pattern for traffic state identification and for abnormal event detection.The pattern we created is shown in Fig.","duplicateSourceMD5":"28CC3FA9325DD3DECDFE6D7F90598CF6"},{"score":0.5501089,"similaritySentence":"based abnormal event detection task.• Empirical evaluations","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Video":"0"},{"Abnormal":"1"},{"Event":"1"},{"Detection":"1"},{"Based":"1"},{"on":"0"},{"CNN":"0"},{"and":"0"},{"LSTM":"0"}],"similaritySegGreyList":[{"based":"1"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"task":"0"},{".":"0"},{"•":"0"},{"Empirical":"0"},{"evaluations":"0"}],"subSimilaritySentenceSection":"employs GRUs for appearance and motion based abnormal event detection task.• Empirical evaluations show that our method SiTGRU performs","duplicateSourceMD5":"5C424BCD4A4BABDEE09A0C1788B106B9"},{"score":0.53919035,"similaritySentence":"and abnormal event detection[2].With","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"Video":"0"},{"Abnormal":"1"},{"Event":"1"},{"Detection":"1"},{"Based":"0"},{"on":"0"},{"CNN":"0"},{"and":"1"},{"LSTM":"0"}],"similaritySegGreyList":[{"and":"1"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"[":"0"},{"2":"0"},{"]":"0"},{".":"0"},{"With":"0"}],"subSimilaritySentenceSection":"applications like surveillance[1] and abnormal event detection[2].With the remarkable success of deep learning","duplicateSourceMD5":"D0B31CF35A229605089288C7A1216FD8"},{"score":0.53832906,"similaritySentence":"video salient object detection based on CSTR block.","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Video":"1"},{"Abnormal":"0"},{"Event":"0"},{"Detection":"1"},{"Based":"1"},{"on":"1"},{"CNN":"0"},{"and":"0"},{"LSTM":"0"}],"similaritySegGreyList":[{"video":"1"},{"salient":"0"},{"object":"0"},{"detection":"1"},{"based":"1"},{"on":"1"},{"CSTR":"0"},{"block":"0"},{".":"0"}],"subSimilaritySentenceSection":"construct a novel CNN architecture for video salient object detection based on CSTR block. Different from previous works that simply","duplicateSourceMD5":"C367C380B4D4ED1F9F022613E2CC2C0C"},{"score":0.5374677,"similaritySentence":"for abnormal event detection in videos.The method","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Video":"1"},{"Abnormal":"1"},{"Event":"1"},{"Detection":"1"},{"Based":"0"},{"on":"0"},{"CNN":"0"},{"and":"0"},{"LSTM":"0"}],"similaritySegGreyList":[{"for":"0"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"in":"0"},{"videos":"1"},{".":"0"},{"The":"0"},{"method":"0"}],"subSimilaritySentenceSection":"-t-solver and LSTM for abnormal event detection in videos.The method involves three-step processing，","duplicateSourceMD5":"D9C1E89708CDD0E33D657E5AE3939FCF"},{"score":0.537037,"similaritySentence":"for abnormal events detection in video sequences.Their","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Video":"1"},{"Abnormal":"1"},{"Event":"1"},{"Detection":"1"},{"Based":"0"},{"on":"0"},{"CNN":"0"},{"and":"0"},{"LSTM":"0"}],"similaritySegGreyList":[{"for":"0"},{"abnormal":"1"},{"events":"1"},{"detection":"1"},{"in":"0"},{"video":"1"},{"sequences":"0"},{".":"0"},{"Their":"0"}],"subSimilaritySentenceSection":"introduced a new DBN based approach for abnormal events detection in video sequences.Their system estimated the background，then","duplicateSourceMD5":"F5ED211BD5A73852DBAB500A69133241"},{"score":0.5310633,"similaritySentence":"videos Abnormal event detection in transportation surveillance videos is","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Video":"1"},{"Abnormal":"1"},{"Event":"1"},{"Detection":"1"},{"Based":"0"},{"on":"0"},{"CNN":"0"},{"and":"0"},{"LSTM":"0"}],"similaritySegGreyList":[{"videos":"1"},{"Abnormal":"1"},{"event":"1"},{"detection":"1"},{"in":"0"},{"transportation":"0"},{"surveillance":"0"},{"videos":"1"},{"is":"0"}],"subSimilaritySentenceSection":".Abnormal event detection in surveillance videos Abnormal event detection in transportation surveillance videos is an application in which deep learning","duplicateSourceMD5":"CE91CC035828198B039DA3347B24A25E"},{"score":0.52737516,"similaritySentence":"work Abnormal event detection in video data has many","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Video":"1"},{"Abnormal":"1"},{"Event":"1"},{"Detection":"1"},{"Based":"0"},{"on":"0"},{"CNN":"0"},{"and":"0"},{"LSTM":"0"}],"similaritySegGreyList":[{"work":"0"},{"Abnormal":"1"},{"event":"1"},{"detection":"1"},{"in":"0"},{"video":"1"},{"data":"0"},{"has":"0"},{"many":"0"}],"subSimilaritySentenceSection":"datasets.7 Conclusion and future work Abnormal event detection in video data has many real-time applications.Hence","duplicateSourceMD5":"52D6607956B2D1F670FC92B0E9EB8AC2"}]},{"content":"Guangli Wu*，Zhenzhou Guo，Leiting Li，Chengxiang Wang","segRedList":[],"synonymsContent":"","result":[]},{"content":"2020","segRedList":[],"synonymsContent":"","result":[]},{"content":"In this paper，CNN combined with LSTM of","segRedList":[],"synonymsContent":"","result":[]},{"content":"convolutional neural network was used to build the model of","segRedList":["convolutional","network","neural","build","model","used","was","the","to","is"],"synonymsContent":"","result":[{"score":0.8047619,"similaritySentence":"convolutional neural network is used to build the extraction model","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"was":"1"},{"used":"1"},{"to":"1"},{"build":"1"},{"the":"1"},{"model":"1"},{"of":"0"}],"similaritySegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"is":"1"},{"used":"1"},{"to":"1"},{"build":"1"},{"the":"1"},{"extraction":"0"},{"model":"1"}],"subSimilaritySentenceSection":"7.b.The trained convolutional neural network is used to build the extraction model and get the map of the","duplicateSourceMD5":"5D9AE40D272B2472551B53CEA936C5A7"},{"score":0.7937107,"similaritySentence":"convolutional neural network is used to construct the model for","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"was":"1"},{"used":"1"},{"to":"1"},{"build":"0"},{"the":"1"},{"model":"1"},{"of":"0"}],"similaritySegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"is":"1"},{"used":"1"},{"to":"1"},{"construct":"0"},{"the":"1"},{"model":"1"},{"for":"0"}],"subSimilaritySentenceSection":"low-dimensional features.Temporal convolutional neural network is used to construct the model for predicting low-dimensional features.","duplicateSourceMD5":"98E2F677B32832B30FEE72D2A92F8EB1"},{"score":0.76292515,"similaritySentence":"convolutional neural network is then built as the brain of","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"was":"1"},{"used":"0"},{"to":"0"},{"build":"1"},{"the":"1"},{"model":"0"},{"of":"1"}],"similaritySegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"is":"1"},{"then":"0"},{"built":"1"},{"as":"0"},{"the":"1"},{"brain":"0"},{"of":"1"}],"subSimilaritySentenceSection":"the control decision.A deep convolutional neural network is then built as the brain of the intelligent agent，which takes","duplicateSourceMD5":"6CE031BB2391A535C18FD570365F19CC"},{"score":0.74230766,"similaritySentence":"convolutional neural network is used to extract the features of","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"was":"1"},{"used":"1"},{"to":"1"},{"build":"0"},{"the":"1"},{"model":"0"},{"of":"1"}],"similaritySegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"is":"1"},{"used":"1"},{"to":"1"},{"extract":"0"},{"the":"1"},{"features":"0"},{"of":"1"}],"subSimilaritySentenceSection":"in speech feature extraction，the convolutional neural network is used to extract the features of underwater ship ’ s sound signal","duplicateSourceMD5":"1FEF043E14B4C2457174401D5E7AF295"},{"score":0.7416666,"similaritySentence":"Convolutional neural network is built by using the module.","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"was":"1"},{"used":"1"},{"to":"0"},{"build":"1"},{"the":"1"},{"model":"0"},{"of":"0"}],"similaritySegGreyList":[{"Convolutional":"1"},{"neural":"1"},{"network":"1"},{"is":"1"},{"built":"1"},{"by":"0"},{"using":"1"},{"the":"1"},{"module":"0"},{".":"0"}],"subSimilaritySentenceSection":"and depth of the network. Convolutional neural network is built by using the module. The experimental datasets are Caltech256 and","duplicateSourceMD5":"EF61E1C804197B524939ADB549BC05A2"},{"score":0.7416666,"similaritySentence":"convolutional neural network is used to predict the user\u0027","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"was":"1"},{"used":"1"},{"to":"1"},{"build":"0"},{"the":"1"},{"model":"0"},{"of":"0"}],"similaritySegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"is":"1"},{"used":"1"},{"to":"1"},{"predict":"0"},{"the":"1"},{"user":"0"},{"\u0027":"0"}],"subSimilaritySentenceSection":"grayscale image，so that the convolutional neural network is used to predict the user\u0027 s role based on the user","duplicateSourceMD5":"70DA57F462BE9034FF806CAE9F34EC50"},{"score":0.7416666,"similaritySentence":"convolutional neural network is used to predict the user\u0027","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"was":"1"},{"used":"1"},{"to":"1"},{"build":"0"},{"the":"1"},{"model":"0"},{"of":"0"}],"similaritySegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"is":"1"},{"used":"1"},{"to":"1"},{"predict":"0"},{"the":"1"},{"user":"0"},{"\u0027":"0"}],"subSimilaritySentenceSection":"grayscale image，so that the convolutional neural network is used to predict the user\u0027 s role based on the user","duplicateSourceMD5":"5BA7BCC0FBEF1AE870E87D084B15D996"},{"score":0.72987413,"similaritySentence":"convolutional neural network was used to classify the images into","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"was":"1"},{"used":"1"},{"to":"1"},{"build":"0"},{"the":"1"},{"model":"0"},{"of":"0"}],"similaritySegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"was":"1"},{"used":"1"},{"to":"1"},{"classify":"0"},{"the":"1"},{"images":"0"},{"into":"0"}],"subSimilaritySentenceSection":"in Figure 1).A convolutional neural network was used to classify the images into normal or abnormal images.The","duplicateSourceMD5":"37D18F5B30F6E06121DD243282685AA8"},{"score":0.7285715,"similaritySentence":"convolutional neural network is used to ensemble the predictions of","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"was":"1"},{"used":"1"},{"to":"1"},{"build":"0"},{"the":"1"},{"model":"0"},{"of":"1"}],"similaritySegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"is":"1"},{"used":"1"},{"to":"1"},{"ensemble":"0"},{"the":"1"},{"predictions":"0"},{"of":"1"}],"subSimilaritySentenceSection":"each time.Finally，a convolutional neural network is used to ensemble the predictions of the first stage classifiers and extract","duplicateSourceMD5":"B74CBFB63EF003EBF2C96CC04D2D0305"},{"score":0.72017545,"similaritySentence":"convolutional neural network is used to learn the characteristics of","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"was":"1"},{"used":"1"},{"to":"1"},{"build":"0"},{"the":"1"},{"model":"0"},{"of":"1"}],"similaritySegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"is":"1"},{"used":"1"},{"to":"1"},{"learn":"0"},{"the":"1"},{"characteristics":"0"},{"of":"1"}],"subSimilaritySentenceSection":"by-layer training model of convolutional neural network is used to learn the characteristics of traffic data internal information，traffic","duplicateSourceMD5":"99805D91D6A7BE5C0AF65EECB539719C"},{"score":0.70694447,"similaritySentence":"convolutional neural networks was performed with the use of a","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"was":"1"},{"used":"1"},{"to":"0"},{"build":"0"},{"the":"1"},{"model":"0"},{"of":"1"}],"similaritySegGreyList":[{"convolutional":"1"},{"neural":"1"},{"networks":"1"},{"was":"1"},{"performed":"0"},{"with":"0"},{"the":"1"},{"use":"1"},{"of":"1"},{"a":"0"}],"subSimilaritySentenceSection":"outcomes of selected implementations of the convolutional neural networks was performed with the use of a limited set of visual training data","duplicateSourceMD5":"0078E93020F5B5E790B00209EE74E022"},{"score":0.7040983,"similaritySentence":"convolutional neural network are used to build convolutional deep belief","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"was":"1"},{"used":"1"},{"to":"1"},{"build":"1"},{"the":"0"},{"model":"0"},{"of":"0"}],"similaritySegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"are":"1"},{"used":"1"},{"to":"1"},{"build":"1"},{"convolutional":"1"},{"deep":"0"},{"belief":"0"}],"subSimilaritySentenceSection":"example，deep belief network and convolutional neural network are used to build convolutional deep belief network，which has higher capabilities","duplicateSourceMD5":"60BC56A00D8D427F1322E9AF4906AD28"},{"score":0.6895833,"similaritySentence":"convolutional neural network is used as a model to explore","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"was":"1"},{"used":"1"},{"to":"1"},{"build":"0"},{"the":"0"},{"model":"1"},{"of":"0"}],"similaritySegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"is":"1"},{"used":"1"},{"as":"0"},{"a":"0"},{"model":"1"},{"to":"1"},{"explore":"0"}],"subSimilaritySentenceSection":"In this experiment，a single convolutional neural network is used as a model to explore the effect of iteration times and","duplicateSourceMD5":"9D3A8C1BEA18056623C727D7C2E0B628"},{"score":0.6876543,"similaritySentence":"convolutional neural network can be used to select the associating","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"was":"1"},{"used":"1"},{"to":"1"},{"build":"0"},{"the":"1"},{"model":"0"},{"of":"0"}],"similaritySegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"can":"0"},{"be":"1"},{"used":"1"},{"to":"1"},{"select":"0"},{"the":"1"},{"associating":"0"}],"subSimilaritySentenceSection":"is realized.And the trained convolutional neural network can be used to select the associating base types of CNC machine tools","duplicateSourceMD5":"3E6B67F1F99BD072C06E2BB70D3E6891"},{"score":0.6827044,"similaritySentence":"used convolutional neural network algorithm to build a model to","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"was":"0"},{"used":"1"},{"to":"1"},{"build":"1"},{"the":"0"},{"model":"1"},{"of":"0"}],"similaritySegGreyList":[{"used":"1"},{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"algorithm":"0"},{"to":"1"},{"build":"1"},{"a":"0"},{"model":"1"},{"to":"1"}],"subSimilaritySentenceSection":"et al.[15] used convolutional neural network algorithm to build a model to predict the possibility of repeated purchase","duplicateSourceMD5":"C9F14D04062D713E3A930C5DB92F13B6"},{"score":0.6775641,"similaritySentence":"WRN convolutional neural network was used to construct tomato，","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"was":"1"},{"used":"1"},{"to":"1"},{"build":"0"},{"the":"0"},{"model":"0"},{"of":"0"}],"similaritySegGreyList":[{"WRN":"0"},{"convolutional":"1"},{"neural":"1"},{"network":"1"},{"was":"1"},{"used":"1"},{"to":"1"},{"construct":"0"},{"tomato":"0"},{"，":"0"}],"subSimilaritySentenceSection":"，potato and corn，the WRN convolutional neural network was used to construct tomato， potato and corn models，and","duplicateSourceMD5":"F1FD76B3EB1AEF691E3D898A5ADE9398"}]},{"content":"abnormal event detection in video.","segRedList":[],"synonymsContent":"","result":[]},{"content":"UCSD and UMN dataset to carry","segRedList":[],"synonymsContent":"","result":[]},{"content":"out the experiment of abnormal event detection and analysis","segRedList":["experiment","detection","abnormal","event","and","The","the"],"synonymsContent":"","result":[{"score":0.6195857,"similaritySentence":"The second experiment compared abnormal event detection and dispatch","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"out":"0"},{"the":"1"},{"experiment":"1"},{"of":"0"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"and":"1"},{"analysis":"0"}],"similaritySegGreyList":[{"The":"1"},{"second":"0"},{"experiment":"1"},{"compared":"0"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"and":"1"},{"dispatch":"0"}],"subSimilaritySentenceSection":"in real-time accessing. The second experiment compared abnormal event detection and dispatch performance of active detect-push","duplicateSourceMD5":"6239CCA8CBF0211026E22D00D6BDFFEA"},{"score":0.6176471,"similaritySentence":"of the tracker tree for abnormal event detection and","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"out":"0"},{"the":"1"},{"experiment":"0"},{"of":"1"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"and":"1"},{"analysis":"0"}],"similaritySegGreyList":[{"of":"1"},{"the":"1"},{"tracker":"0"},{"tree":"0"},{"for":"0"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"and":"1"}],"subSimilaritySentenceSection":"following，we analyze the performance of the tracker tree for abnormal event detection and compare it to state-of","duplicateSourceMD5":"CC6000A51D6ADF19B6EB63D89C0B9260"},{"score":0.6013072,"similaritySentence":".The normal/abnormal event detection accuracy of","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"out":"0"},{"the":"1"},{"experiment":"0"},{"of":"1"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"and":"0"},{"analysis":"0"}],"similaritySegGreyList":[{".":"1"},{"The":"1"},{"normal":"0"},{"/":"0"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"accuracy":"0"},{"of":"1"}],"subSimilaritySentenceSection":"MCMC，but lower detection accuracy .The normal/abnormal event detection accuracy of the iHMM and iGMM methods are","duplicateSourceMD5":"5A08A7FBC8E92F72450BF6F8DCB5C766"},{"score":0.59912854,"similaritySentence":"in the field of abnormal event detection.Unlike","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"out":"0"},{"the":"1"},{"experiment":"0"},{"of":"1"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"and":"0"},{"analysis":"0"}],"similaritySegGreyList":[{"in":"0"},{"the":"1"},{"field":"0"},{"of":"1"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{".":"0"},{"Unlike":"0"}],"subSimilaritySentenceSection":"learning has recently achieved outstanding results in the field of abnormal event detection.Unlike the features of manual design，","duplicateSourceMD5":"B5FBCF5256014D35FBA850794D40D283"},{"score":0.58278865,"similaritySentence":"of methods about the abnormality detection and analysis:","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"out":"0"},{"the":"1"},{"experiment":"0"},{"of":"1"},{"abnormal":"0"},{"event":"0"},{"detection":"1"},{"and":"1"},{"analysis":"1"}],"similaritySegGreyList":[{"of":"1"},{"methods":"0"},{"about":"0"},{"the":"1"},{"abnormality":"0"},{"detection":"1"},{"and":"1"},{"analysis":"1"},{":":"1"}],"subSimilaritySentenceSection":")Generally there are two kinds of methods about the abnormality detection and analysis: the method based on models and","duplicateSourceMD5":"7AF78A2E4F7C354CEC631DE6136214F1"},{"score":0.58278865,"similaritySentence":"two kinds of methods on abnormality detection and analysis","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"out":"0"},{"the":"0"},{"experiment":"0"},{"of":"1"},{"abnormal":"0"},{"event":"0"},{"detection":"1"},{"and":"1"},{"analysis":"1"}],"similaritySegGreyList":[{"two":"0"},{"kinds":"0"},{"of":"1"},{"methods":"0"},{"on":"0"},{"abnormality":"0"},{"detection":"1"},{"and":"1"},{"analysis":"1"}],"subSimilaritySentenceSection":"field.Generally，there are two kinds of methods on abnormality detection and analysis :the method based on models","duplicateSourceMD5":"6242EC7B75D3CF86D5D9B03E44B1F1D8"},{"score":0.58278865,"similaritySentence":"suited for global abnormal event detection and Bag-","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"out":"0"},{"the":"0"},{"experiment":"0"},{"of":"0"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"and":"1"},{"analysis":"0"}],"similaritySegGreyList":[{"suited":"0"},{"for":"0"},{"global":"0"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"and":"1"},{"Bag":"0"},{"-":"0"}],"subSimilaritySentenceSection":"MP-MIDL is the most suited for global abnormal event detection and Bag- MIDL performs best for local abnormal","duplicateSourceMD5":"5F56008DCCD1055514EA4ECD5E7B66E4"},{"score":0.5686274,"similaritySentence":"out the abnormal event detection of the pre-","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"out":"1"},{"the":"1"},{"experiment":"0"},{"of":"1"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"and":"0"},{"analysis":"0"}],"similaritySegGreyList":[{"out":"1"},{"the":"1"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"of":"1"},{"the":"1"},{"pre":"0"},{"-":"0"}],"subSimilaritySentenceSection":"more substantial computational resources for carrying out the abnormal event detection of the pre- processed frames using the edge computing","duplicateSourceMD5":"749A2707CD839BAF6F1BD47A0A2C3BB7"},{"score":0.5664488,"similaritySentence":"the arts for the abnormal event detection problem in","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"out":"0"},{"the":"1"},{"experiment":"0"},{"of":"0"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"and":"0"},{"analysis":"0"}],"similaritySegGreyList":[{"the":"1"},{"arts":"0"},{"for":"0"},{"the":"1"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"problem":"0"},{"in":"0"}],"subSimilaritySentenceSection":"the proposed algorithm outperforms state of the arts for the abnormal event detection problem in crowded scenes.Keywords Video surveillance","duplicateSourceMD5":"6462B2364CC7B43FE02D33EE4100705C"},{"score":0.5664488,"similaritySentence":"of-the-art abnormal event detection algorithms","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"out":"0"},{"the":"1"},{"experiment":"0"},{"of":"1"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"and":"0"},{"analysis":"0"}],"similaritySegGreyList":[{"of":"1"},{"-":"1"},{"the":"1"},{"-":"0"},{"art":"0"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"algorithms":"0"}],"subSimilaritySentenceSection":"learning method with some state- of-the-art abnormal event detection algorithms ，i.e.，","duplicateSourceMD5":"2287AD9422373606D53BA4D813E656E2"},{"score":0.55733615,"similaritySentence":"qualitative results of abnormal event detection in the BEHAVE","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"out":"0"},{"the":"1"},{"experiment":"0"},{"of":"1"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"and":"0"},{"analysis":"0"}],"similaritySegGreyList":[{"qualitative":"0"},{"results":"0"},{"of":"1"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"in":"0"},{"the":"1"},{"BEHAVE":"0"}],"subSimilaritySentenceSection":"BEHAVE dataset Figure 16 shows the qualitative results of abnormal event detection in the BEHAVE dataset:Top row is the","duplicateSourceMD5":"A1655FCF2E46BDAACEC5856CFC116CAB"},{"score":0.5501089,"similaritySentence":"graphs is used for abnormal event detection.Jiang","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"out":"0"},{"the":"0"},{"experiment":"0"},{"of":"0"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"and":"0"},{"analysis":"0"}],"similaritySegGreyList":[{"graphs":"0"},{"is":"0"},{"used":"0"},{"for":"0"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{".":"0"},{"Jiang":"0"}],"subSimilaritySentenceSection":"，and the spectral analysis of graphs is used for abnormal event detection.Jiang et al.20 defined the","duplicateSourceMD5":"16057F76C73AB788007DFCA07D8A2B95"},{"score":0.5501089,"similaritySentence":"propose our framework of abnormal event detection in Sec","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"out":"0"},{"the":"0"},{"experiment":"0"},{"of":"1"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"and":"0"},{"analysis":"0"}],"similaritySegGreyList":[{"propose":"0"},{"our":"0"},{"framework":"0"},{"of":"1"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"in":"0"},{"Sec":"0"}],"subSimilaritySentenceSection":"solutions in Sec.2 and propose our framework of abnormal event detection in Sec .3，which combines sparse","duplicateSourceMD5":"BD806366390CE1601A21B121505A941D"},{"score":0.5337691,"similaritySentence":"it is clear that the abnormal event detection rate","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"out":"0"},{"the":"1"},{"experiment":"0"},{"of":"0"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"and":"0"},{"analysis":"0"}],"similaritySegGreyList":[{"it":"0"},{"is":"0"},{"clear":"0"},{"that":"0"},{"the":"1"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"rate":"0"}],"subSimilaritySentenceSection":"11(a)]， it is clear that the abnormal event detection rate of the proposed approach is always","duplicateSourceMD5":"549D8D96D25E95EE4F25C25876685DB8"},{"score":0.5337691,"similaritySentence":"the accuracy of abnormal events detection.The remainder","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"out":"0"},{"the":"1"},{"experiment":"0"},{"of":"1"},{"abnormal":"1"},{"event":"1"},{"detection":"1"},{"and":"0"},{"analysis":"0"}],"similaritySegGreyList":[{"the":"1"},{"accuracy":"0"},{"of":"1"},{"abnormal":"1"},{"events":"1"},{"detection":"1"},{".":"1"},{"The":"1"},{"remainder":"0"}],"subSimilaritySentenceSection":"19，32]to evaluate the accuracy of abnormal events detection.The remainder of this paper has six sections","duplicateSourceMD5":"AE6B450157577E3187556F72FE7ECF3D"},{"score":0.5337691,"similaritySentence":"the timeliness of abnormal detection will be.This","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"out":"0"},{"the":"1"},{"experiment":"0"},{"of":"1"},{"abnormal":"1"},{"event":"0"},{"detection":"1"},{"and":"0"},{"analysis":"0"}],"similaritySegGreyList":[{"the":"1"},{"timeliness":"0"},{"of":"1"},{"abnormal":"1"},{"detection":"1"},{"will":"0"},{"be":"0"},{".":"0"},{"This":"0"}],"subSimilaritySentenceSection":"performance will be and the better the timeliness of abnormal detection will be.This results in a higher false negative","duplicateSourceMD5":"5055BD7D7CD291A5C9D6E2DFDA7A5C7B"}]},{"content":"in video，and achieved certain good results.","segRedList":[],"synonymsContent":"","result":[]},{"content":"3.","segRedList":[],"synonymsContent":"","result":[]},{"content":"PROPOSED WORK","segRedList":[],"synonymsContent":"","result":[]},{"content":"Our proposed system solves the problem of trespassing with a multistage automated approach.","segRedList":[],"synonymsContent":"","result":[]},{"content":"The first stage consists of capturing the live video on railway tracks/station and then will try to remove the frames in which no significant activity will be detected.","segRedList":[],"synonymsContent":"","result":[]},{"content":"This will allow to reduce the computation time required to process the data and this will be aided by state of art OpenCV techniques in Python.","segRedList":[],"synonymsContent":"","result":[]},{"content":"Each frame of the video will be dissected and processing will done where human motion is captured.","segRedList":[],"synonymsContent":"","result":[]},{"content":"The next stage of the solution involves the use of YOLO-deep learning model based on CNN which uses the TensorFlow and","segRedList":[],"synonymsContent":"","result":[]},{"content":"Keras libraries at the backend which will be responsible for effective classification to decide whether or not trespassing is happening.","segRedList":[],"synonymsContent":"","result":[]},{"content":"","segRedList":[],"synonymsContent":"","result":[]},{"content":"All of the above process will be happening in all of the cameras placed at the station simultaneously.","segRedList":[],"synonymsContent":"","result":[]},{"content":"So initially after capturing any kind of motion in the surveillance，the processing will happen in the form of filtering out the frames that","segRedList":[],"synonymsContent":"","result":[]},{"content":"have no motion activity and the places where there is any activity happening will be marked as Region of interest(ROI).","segRedList":[],"synonymsContent":"","result":[]},{"content":"Upon filtering these frames/data will passed to the YOLO model which will then detect the exact coordinates of where the human is in the ROI.","segRedList":[],"synonymsContent":"","result":[]},{"content":"Once the model has the location，it will then try to calculate if that location falls in safe limits or not and will.","segRedList":[],"synonymsContent":"","result":[]},{"content":"The ones within the safe limits and identified as the station worker will be skipped and further withing the ROI the bounding boxes will be used","segRedList":[],"synonymsContent":"","result":[]},{"content":"to detect and classify the commoners who are breaching the rules as trespassers and thereby registering a trespassing event and raise an immediate alarm.","segRedList":[],"synonymsContent":"","result":[]},{"content":"All of this information will be stored in the database.","segRedList":["information","database","stored","this","will","the","All","be","in"],"synonymsContent":"","result":[{"score":0.9111111,"similaritySentence":".All this information will be stored in the database，","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"All":"1"},{"of":"0"},{"this":"1"},{"information":"1"},{"will":"1"},{"be":"1"},{"stored":"1"},{"in":"1"},{"the":"1"},{"database":"1"},{".":"1"}],"similaritySegGreyList":[{".":"1"},{"All":"1"},{"this":"1"},{"information":"1"},{"will":"1"},{"be":"1"},{"stored":"1"},{"in":"1"},{"the":"1"},{"database":"1"},{"，":"1"}],"subSimilaritySentenceSection":"，business related attributes，etc .All this information will be stored in the database， while the file will remain on","duplicateSourceMD5":"B442D5A0780F7A05A16776AC799CD64A"},{"score":0.72147006,"similaritySentence":"patient.This information will be stored in the Drugs view","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"All":"0"},{"of":"0"},{"this":"1"},{"information":"1"},{"will":"1"},{"be":"1"},{"stored":"1"},{"in":"1"},{"the":"1"},{"database":"0"},{".":"0"}],"similaritySegGreyList":[{"patient":"0"},{".":"0"},{"This":"1"},{"information":"1"},{"will":"1"},{"be":"1"},{"stored":"1"},{"in":"1"},{"the":"1"},{"Drugs":"0"},{"view":"0"}],"subSimilaritySentenceSection":"find the correct treatment for a patient.This information will be stored in the Drugs view of CSHG.In this view","duplicateSourceMD5":"DF1852AA9FF52598EBA571F19BDB38AB"},{"score":0.7190382,"similaritySentence":"the person.This information is stored in the database.","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"All":"0"},{"of":"0"},{"this":"1"},{"information":"1"},{"will":"0"},{"be":"1"},{"stored":"1"},{"in":"1"},{"the":"1"},{"database":"1"},{".":"1"}],"similaritySegGreyList":[{"the":"1"},{"person":"0"},{".":"0"},{"This":"1"},{"information":"1"},{"is":"1"},{"stored":"1"},{"in":"1"},{"the":"1"},{"database":"1"},{".":"1"}],"subSimilaritySentenceSection":"，and other pertinent information about the person.This information is stored in the database. Only a reference to the picture","duplicateSourceMD5":"20EFAB21046D54A412E0A5B567AA830C"},{"score":0.70889753,"similaritySentence":"leak out the information that is stored in their database.","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"All":"0"},{"of":"0"},{"this":"0"},{"information":"1"},{"will":"0"},{"be":"1"},{"stored":"1"},{"in":"1"},{"the":"1"},{"database":"1"},{".":"1"}],"similaritySegGreyList":[{"leak":"0"},{"out":"0"},{"the":"1"},{"information":"1"},{"that":"0"},{"is":"1"},{"stored":"1"},{"in":"1"},{"their":"0"},{"database":"1"},{".":"1"}],"subSimilaritySentenceSection":".The companies cannot afford to leak out the information that is stored in their database. If there is encryption of data","duplicateSourceMD5":"F64FEC07A0FBEECF02D2BE0AA19FFC6D"},{"score":0.67676765,"similaritySentence":"set of reference information to be stored in the database.","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"All":"0"},{"of":"1"},{"this":"0"},{"information":"1"},{"will":"0"},{"be":"1"},{"stored":"1"},{"in":"1"},{"the":"1"},{"database":"1"},{".":"1"}],"similaritySegGreyList":[{"set":"0"},{"of":"1"},{"reference":"0"},{"information":"1"},{"to":"0"},{"be":"1"},{"stored":"1"},{"in":"1"},{"the":"1"},{"database":"1"},{".":"1"}],"subSimilaritySentenceSection":"the operator or analyst creates a set of reference information to be stored in the database. This information includes canonical sets of","duplicateSourceMD5":"63804E9C9A8A76F09F98F38B2AA3C720"},{"score":0.67676765,"similaritySentence":"If all this information were stored in a single database，","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"All":"1"},{"of":"0"},{"this":"1"},{"information":"1"},{"will":"0"},{"be":"1"},{"stored":"1"},{"in":"1"},{"the":"0"},{"database":"1"},{".":"1"}],"similaritySegGreyList":[{"If":"0"},{"all":"1"},{"this":"1"},{"information":"1"},{"were":"1"},{"stored":"1"},{"in":"1"},{"a":"0"},{"single":"0"},{"database":"1"},{"，":"1"}],"subSimilaritySentenceSection":"into different databases was necessary. If all this information were stored in a single database， a significant increase in memory and","duplicateSourceMD5":"A3872EDE5A0A2ED6F34AC4AA68BF7096"},{"score":0.67676765,"similaritySentence":"and type of information that is stored in the database will","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"All":"0"},{"of":"1"},{"this":"0"},{"information":"1"},{"will":"1"},{"be":"1"},{"stored":"1"},{"in":"1"},{"the":"1"},{"database":"1"},{".":"1"}],"similaritySegGreyList":[{"and":"0"},{"type":"0"},{"of":"1"},{"information":"1"},{"that":"0"},{"is":"1"},{"stored":"1"},{"in":"1"},{"the":"1"},{"database":"1"},{"will":"1"}],"subSimilaritySentenceSection":"Comprehensiveness:The level of detail and type of information that is stored in the database will vary from project to project，","duplicateSourceMD5":"9C07E563B385AEEFB03DFB1B8BA01750"},{"score":0.6582492,"similaritySentence":"all the information will be stored in data blocks of a","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"All":"1"},{"of":"1"},{"this":"0"},{"information":"1"},{"will":"1"},{"be":"1"},{"stored":"1"},{"in":"1"},{"the":"1"},{"database":"0"},{".":"0"}],"similaritySegGreyList":[{"all":"1"},{"the":"1"},{"information":"1"},{"will":"1"},{"be":"1"},{"stored":"1"},{"in":"1"},{"data":"0"},{"blocks":"0"},{"of":"1"},{"a":"0"}],"subSimilaritySentenceSection":"database system in the same region all the information will be stored in data blocks of a single row in T-mode","duplicateSourceMD5":"D773BE1D947B9C8DAA132C4D422C2F42"},{"score":0.6545454,"similaritySentence":"describes what information can be stored in the database.The","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"All":"0"},{"of":"0"},{"this":"0"},{"information":"1"},{"will":"0"},{"be":"1"},{"stored":"1"},{"in":"1"},{"the":"1"},{"database":"1"},{".":"1"}],"similaritySegGreyList":[{"describes":"0"},{"what":"0"},{"information":"1"},{"can":"0"},{"be":"1"},{"stored":"1"},{"in":"1"},{"the":"1"},{"database":"1"},{".":"1"},{"The":"1"}],"subSimilaritySentenceSection":"-structured data model，which describes what information can be stored in the database.The objective of the investigation reported here","duplicateSourceMD5":"582E780BD6AAB96FE352E27D3ADB130B"},{"score":0.646184,"similaritySentence":"all the possible information that might be stored in the database","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"All":"1"},{"of":"0"},{"this":"0"},{"information":"1"},{"will":"0"},{"be":"1"},{"stored":"1"},{"in":"1"},{"the":"1"},{"database":"1"},{".":"1"}],"similaritySegGreyList":[{"all":"1"},{"the":"1"},{"possible":"0"},{"information":"1"},{"that":"0"},{"might":"0"},{"be":"1"},{"stored":"1"},{"in":"1"},{"the":"1"},{"database":"1"}],"subSimilaritySentenceSection":"number of colleagues—jots down all the possible information that might be stored in the database system.Does the college，","duplicateSourceMD5":"4D86FF98C7250FC8E32FEBEE74A9501A"},{"score":0.64309764,"similaritySentence":"all of the data that you will store in your database","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"All":"1"},{"of":"1"},{"this":"0"},{"information":"0"},{"will":"1"},{"be":"0"},{"stored":"1"},{"in":"1"},{"the":"1"},{"database":"1"},{".":"1"}],"similaritySegGreyList":[{"all":"1"},{"of":"1"},{"the":"1"},{"data":"0"},{"that":"0"},{"you":"0"},{"will":"1"},{"store":"1"},{"in":"1"},{"your":"0"},{"database":"1"}],"subSimilaritySentenceSection":".Tables define the structure for all of the data that you will store in your database .With the exception of filestream","duplicateSourceMD5":"973F4BE07177C0CE55DB33A4024DB5DA"},{"score":0.64309764,"similaritySentence":"all of the data that you will store in your database","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"All":"1"},{"of":"1"},{"this":"0"},{"information":"0"},{"will":"1"},{"be":"0"},{"stored":"1"},{"in":"1"},{"the":"1"},{"database":"1"},{".":"1"}],"similaritySegGreyList":[{"all":"1"},{"of":"1"},{"the":"1"},{"data":"0"},{"that":"0"},{"you":"0"},{"will":"1"},{"store":"1"},{"in":"1"},{"your":"0"},{"database":"1"}],"subSimilaritySentenceSection":".Tables define the structure for all of the data that you will store in your database .With the exception of filestream","duplicateSourceMD5":"E68429ED8D1456493AC1B4A7AB145DC7"},{"score":0.63973063,"similaritySentence":"results of the query will be stored in the database.","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"All":"0"},{"of":"1"},{"this":"0"},{"information":"0"},{"will":"1"},{"be":"1"},{"stored":"1"},{"in":"1"},{"the":"1"},{"database":"1"},{".":"1"}],"similaritySegGreyList":[{"results":"0"},{"of":"1"},{"the":"1"},{"query":"0"},{"will":"1"},{"be":"1"},{"stored":"1"},{"in":"1"},{"the":"1"},{"database":"1"},{".":"1"}],"subSimilaritySentenceSection":"step 3，then only the results of the query will be stored in the database. Otherwise，the entire RDF file","duplicateSourceMD5":"72D0C96A14574CB76114CA8EA7971784"},{"score":0.63490677,"similaritySentence":"information.Information that is stored in the database can be","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"All":"0"},{"of":"0"},{"this":"0"},{"information":"1"},{"will":"0"},{"be":"1"},{"stored":"1"},{"in":"1"},{"the":"1"},{"database":"1"},{".":"1"}],"similaritySegGreyList":[{"information":"1"},{".":"1"},{"Information":"1"},{"that":"0"},{"is":"1"},{"stored":"1"},{"in":"1"},{"the":"1"},{"database":"1"},{"can":"0"},{"be":"1"}],"subSimilaritySentenceSection":"of the database is to store information.Information that is stored in the database can be represented by a variety of data","duplicateSourceMD5":"10F173AEED2351C7CDC95D39BC1C7D72"},{"score":0.62457913,"similaritySentence":"the information which were going to be stored in the database","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"All":"0"},{"of":"0"},{"this":"0"},{"information":"1"},{"will":"0"},{"be":"1"},{"stored":"1"},{"in":"1"},{"the":"1"},{"database":"1"},{".":"1"}],"similaritySegGreyList":[{"the":"1"},{"information":"1"},{"which":"0"},{"were":"1"},{"going":"0"},{"to":"0"},{"be":"1"},{"stored":"1"},{"in":"1"},{"the":"1"},{"database":"1"}],"subSimilaritySentenceSection":"to consider was the types of the information which were going to be stored in the database .For this reason the database","duplicateSourceMD5":"23D57A5BFC9F4C0BCAD122D13BF1355D"},{"score":0.6212121,"similaritySentence":"CT-scan etc.will be stored in the database","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"All":"0"},{"of":"0"},{"this":"0"},{"information":"0"},{"will":"1"},{"be":"1"},{"stored":"1"},{"in":"1"},{"the":"1"},{"database":"1"},{".":"1"}],"similaritySegGreyList":[{"CT":"0"},{"-":"0"},{"scan":"0"},{"etc":"0"},{".":"0"},{"will":"1"},{"be":"1"},{"stored":"1"},{"in":"1"},{"the":"1"},{"database":"1"}],"subSimilaritySentenceSection":"reports，images like MRI， CT-scan etc.will be stored in the database of the hospital local server.","duplicateSourceMD5":"D3400CA3CA83F735AA6230AB7BD032E1"}]},{"content":"And for ones where the alarm is raised the location details/coordinates will be sent to the Railway administration to take further necessary actions.","segRedList":[],"synonymsContent":"","result":[]},{"content":"Figure 1:","segRedList":[],"synonymsContent":"","result":[]},{"content":"System Architecture","segRedList":[],"synonymsContent":"","result":[]},{"content":"3.1 YOLO(You Only Look Once)","segRedList":[],"synonymsContent":"","result":[]},{"content":"YOLO You Only Look Once，is a framework that deals with object detection in a different way.","segRedList":["different","detection","object","deals","Once","Only","Look","with","way","You","in","a"],"synonymsContent":"","result":[{"score":0.73391813,"similaritySentence":"You Only Look Once)on the other hand，deals with object detection in a different way.","articleType":"net","classification":"net","originalSegGreyList":[{"YOLO":"0"},{"You":"1"},{"Only":"1"},{"Look":"1"},{"Once":"1"},{"，":"0"},{"is":"0"},{"a":"1"},{"framework":"0"},{"that":"0"},{"deals":"1"},{"with":"1"},{"object":"1"},{"detection":"1"},{"in":"1"},{"a":"1"},{"different":"1"},{"way":"1"},{".":"1"}],"similaritySegGreyList":[{"You":"1"},{"Only":"1"},{"Look":"1"},{"Once":"1"},{")":"0"},{"on":"0"},{"the":"0"},{"other":"0"},{"hand":"0"},{"，":"0"},{"deals":"1"},{"with":"1"},{"object":"1"},{"detection":"1"},{"in":"1"},{"a":"1"},{"different":"1"},{"way":"1"},{".":"1"}],"subSimilaritySentenceSection":"object.The YOLO framework( You Only Look Once)on the other hand，deals with object detection in a different way. It takes the entire image in","duplicateSourceMD5":"A47D1A1C113E204138222B36D372E27F"},{"score":0.61637425,"similaritySentence":".The yolo framework，on the other hand，deals with object detection in a different way.","articleType":"net","classification":"net","originalSegGreyList":[{"YOLO":"1"},{"You":"0"},{"Only":"0"},{"Look":"0"},{"Once":"0"},{"，":"0"},{"is":"0"},{"a":"1"},{"framework":"1"},{"that":"0"},{"deals":"1"},{"with":"1"},{"object":"1"},{"detection":"1"},{"in":"1"},{"a":"1"},{"different":"1"},{"way":"1"},{".":"1"}],"similaritySegGreyList":[{".":"0"},{"The":"0"},{"yolo":"1"},{"framework":"1"},{"，":"0"},{"on":"0"},{"the":"0"},{"other":"0"},{"hand":"0"},{"，":"0"},{"deals":"1"},{"with":"1"},{"object":"1"},{"detection":"1"},{"in":"1"},{"a":"1"},{"different":"1"},{"way":"1"},{".":"1"}],"subSimilaritySentenceSection":"higher chance of containing an object .The yolo framework，on the other hand，deals with object detection in a different way. It takes the entire image in","duplicateSourceMD5":"BAE251915346CF40266C012D04197185"},{"score":0.58538014,"similaritySentence":"YOLO(You Only Look Once)is a model for object detection[14]used in this","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"YOLO":"1"},{"You":"1"},{"Only":"1"},{"Look":"1"},{"Once":"1"},{"，":"1"},{"is":"1"},{"a":"1"},{"framework":"0"},{"that":"0"},{"deals":"0"},{"with":"0"},{"object":"1"},{"detection":"1"},{"in":"1"},{"a":"1"},{"different":"0"},{"way":"0"},{".":"0"}],"similaritySegGreyList":[{"YOLO":"1"},{"(":"1"},{"You":"1"},{"Only":"1"},{"Look":"1"},{"Once":"1"},{")":"1"},{"is":"1"},{"a":"1"},{"model":"0"},{"for":"0"},{"object":"1"},{"detection":"1"},{"[":"0"},{"14":"0"},{"]":"0"},{"used":"0"},{"in":"1"},{"this":"0"}],"subSimilaritySentenceSection":"Only Look Once(YOLO) YOLO(You Only Look Once)is a model for object detection[14]used in this framework.The task is to","duplicateSourceMD5":"891B13D155478D8DD26042B90256E4DE"},{"score":0.5430824,"similaritySentence":"YOLO(You Only Look Once)，RetinaNet，and CenterNet for the object detection in videos captured","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"YOLO":"1"},{"You":"1"},{"Only":"1"},{"Look":"1"},{"Once":"1"},{"，":"0"},{"is":"0"},{"a":"0"},{"framework":"0"},{"that":"0"},{"deals":"0"},{"with":"0"},{"object":"1"},{"detection":"1"},{"in":"1"},{"a":"0"},{"different":"0"},{"way":"0"},{".":"0"}],"similaritySegGreyList":[{"YOLO":"1"},{"(":"1"},{"You":"1"},{"Only":"1"},{"Look":"1"},{"Once":"1"},{")":"0"},{"，":"0"},{"RetinaNet":"0"},{"，":"0"},{"and":"0"},{"CenterNet":"0"},{"for":"0"},{"the":"0"},{"object":"1"},{"detection":"1"},{"in":"1"},{"videos":"0"},{"captured":"0"}],"subSimilaritySentenceSection":"Shot Detector(SSD)， YOLO(You Only Look Once)，RetinaNet，and CenterNet for the object detection in videos captured by drones.We conduct experiments","duplicateSourceMD5":"736E74978E90BC0180F1A0F9AF898621"},{"score":0.54093564,"similaritySentence":"YOLO(You Only Look Once)，is a neural network for object detection which uses a single","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"YOLO":"1"},{"You":"1"},{"Only":"1"},{"Look":"1"},{"Once":"1"},{"，":"1"},{"is":"1"},{"a":"1"},{"framework":"0"},{"that":"0"},{"deals":"0"},{"with":"0"},{"object":"1"},{"detection":"1"},{"in":"0"},{"a":"1"},{"different":"0"},{"way":"0"},{".":"0"}],"similaritySegGreyList":[{"YOLO":"1"},{"(":"1"},{"You":"1"},{"Only":"1"},{"Look":"1"},{"Once":"1"},{")":"1"},{"，":"1"},{"is":"1"},{"a":"1"},{"neural":"0"},{"network":"0"},{"for":"0"},{"object":"1"},{"detection":"1"},{"which":"0"},{"uses":"0"},{"a":"1"},{"single":"0"}],"subSimilaritySentenceSection":".2.YOLO object detection YOLO(You Only Look Once)，is a neural network for object detection which uses a single neural network for the task of","duplicateSourceMD5":"DEAA97D06D97BED6C4B283E8B5012678"},{"score":0.5233918,"similaritySentence":"YOLO(you only look once)[12]ri a stateof-fteat reaLtime object detection system","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"YOLO":"1"},{"You":"1"},{"Only":"1"},{"Look":"1"},{"Once":"1"},{"，":"0"},{"is":"0"},{"a":"1"},{"framework":"0"},{"that":"0"},{"deals":"0"},{"with":"0"},{"object":"1"},{"detection":"1"},{"in":"0"},{"a":"1"},{"different":"0"},{"way":"0"},{".":"0"}],"similaritySegGreyList":[{"YOLO":"1"},{"(":"1"},{"you":"1"},{"only":"1"},{"look":"1"},{"once":"1"},{")":"0"},{"[":"0"},{"12":"0"},{"]":"0"},{"ri":"0"},{"a":"1"},{"stateof":"0"},{"-":"0"},{"fteat":"0"},{"reaLtime":"0"},{"object":"1"},{"detection":"1"},{"system":"0"}],"subSimilaritySentenceSection":"the scene.Among them， YOLO(you only look once)[12]ri a stateof-fteat reaLtime object detection system ，targeted for real-time","duplicateSourceMD5":"54D77B2A441329679370FE0099B744AB"},{"score":0.5233918,"similaritySentence":"YOLO(You Only Look Once:Unified，Real-Time Object Detection)and the Faster RCNN","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"YOLO":"1"},{"You":"1"},{"Only":"1"},{"Look":"1"},{"Once":"1"},{"，":"0"},{"is":"0"},{"a":"0"},{"framework":"0"},{"that":"0"},{"deals":"0"},{"with":"0"},{"object":"1"},{"detection":"1"},{"in":"0"},{"a":"0"},{"different":"0"},{"way":"0"},{".":"0"}],"similaritySegGreyList":[{"YOLO":"1"},{"(":"1"},{"You":"1"},{"Only":"1"},{"Look":"1"},{"Once":"1"},{":":"0"},{"Unified":"0"},{"，":"0"},{"Real":"0"},{"-":"0"},{"Time":"0"},{"Object":"1"},{"Detection":"1"},{")":"0"},{"and":"0"},{"the":"0"},{"Faster":"0"},{"RCNN":"0"}],"subSimilaritySentenceSection":"positioning.The typical algorithms are YOLO(You Only Look Once:Unified，Real-Time Object Detection)and the Faster RCNN (Regions with Convolutional Neural Network","duplicateSourceMD5":"23055D4F9C4EF1D0380D674B03AC5180"},{"score":0.5210526,"similaritySentence":"，A.You only look once:Unified，real-time object detection.In Proceedings of","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"YOLO":"0"},{"You":"1"},{"Only":"1"},{"Look":"1"},{"Once":"1"},{"，":"0"},{"is":"0"},{"a":"1"},{"framework":"0"},{"that":"0"},{"deals":"0"},{"with":"0"},{"object":"1"},{"detection":"1"},{"in":"1"},{"a":"1"},{"different":"0"},{"way":"0"},{".":"0"}],"similaritySegGreyList":[{"，":"1"},{"A":"1"},{".":"1"},{"You":"1"},{"only":"1"},{"look":"1"},{"once":"1"},{":":"0"},{"Unified":"0"},{"，":"0"},{"real":"0"},{"-":"0"},{"time":"0"},{"object":"1"},{"detection":"1"},{".":"1"},{"In":"1"},{"Proceedings":"0"},{"of":"0"}],"subSimilaritySentenceSection":"Girshick，R.;Farhadi ，A.You only look once:Unified，real-time object detection.In Proceedings of the IEEE Conference on Computer Vision","duplicateSourceMD5":"8B47F543B7A59C83018F4B1512FBAB68"},{"score":0.5187134,"similaritySentence":".You Only Look Once(YOLO)is a network for object detection in images.In this","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"YOLO":"1"},{"You":"1"},{"Only":"1"},{"Look":"1"},{"Once":"1"},{"，":"1"},{"is":"1"},{"a":"1"},{"framework":"0"},{"that":"0"},{"deals":"0"},{"with":"0"},{"object":"1"},{"detection":"1"},{"in":"1"},{"a":"1"},{"different":"0"},{"way":"0"},{".":"0"}],"similaritySegGreyList":[{".":"1"},{"You":"1"},{"Only":"1"},{"Look":"1"},{"Once":"1"},{"(":"1"},{"YOLO":"1"},{")":"1"},{"is":"1"},{"a":"1"},{"network":"0"},{"for":"0"},{"object":"1"},{"detection":"1"},{"in":"1"},{"images":"0"},{".":"0"},{"In":"1"},{"this":"0"}],"subSimilaritySentenceSection":"challenge of a high computational time .You Only Look Once(YOLO)is a network for object detection in images.In this paper，we propose a real","duplicateSourceMD5":"633324DEACCF426156D1E4594CD0FCFF"},{"score":0.5122807,"similaritySentence":"，\"You Only Look Once:Unified，Real-Time Object Detection\"，IEEE/CVF","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"YOLO":"0"},{"You":"1"},{"Only":"1"},{"Look":"1"},{"Once":"1"},{"，":"0"},{"is":"0"},{"a":"0"},{"framework":"0"},{"that":"0"},{"deals":"0"},{"with":"0"},{"object":"1"},{"detection":"1"},{"in":"0"},{"a":"0"},{"different":"0"},{"way":"0"},{".":"0"}],"similaritySegGreyList":[{"，":"1"},{"\"":"1"},{"You":"1"},{"Only":"1"},{"Look":"1"},{"Once":"1"},{":":"0"},{"Unified":"0"},{"，":"0"},{"Real":"0"},{"-":"0"},{"Time":"0"},{"Object":"1"},{"Detection":"1"},{"\"":"0"},{"，":"0"},{"IEEE":"0"},{"/":"0"},{"CVF":"0"}],"subSimilaritySentenceSection":".Girshick and A.Farhadi ，\"You Only Look Once:Unified，Real-Time Object Detection\"，IEEE/CVF International Conference on Computer Vision and","duplicateSourceMD5":"EB57F1FE6825228E6889B058C5E5C55B"},{"score":0.5099415,"similaritySentence":"，You only look once:Unified，real-time object detection，in Proc.of the","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"YOLO":"0"},{"You":"1"},{"Only":"1"},{"Look":"1"},{"Once":"1"},{"，":"0"},{"is":"0"},{"a":"0"},{"framework":"0"},{"that":"0"},{"deals":"0"},{"with":"0"},{"object":"1"},{"detection":"1"},{"in":"1"},{"a":"0"},{"different":"0"},{"way":"0"},{".":"0"}],"similaritySegGreyList":[{"，":"1"},{"You":"1"},{"only":"1"},{"look":"1"},{"once":"1"},{":":"0"},{"Unified":"0"},{"，":"0"},{"real":"0"},{"-":"0"},{"time":"0"},{"object":"1"},{"detection":"1"},{"，":"1"},{"in":"1"},{"Proc":"0"},{".":"0"},{"of":"0"},{"the":"0"}],"subSimilaritySentenceSection":"Girshick，and A.Farhadi ，You only look once:Unified，real-time object detection，in Proc.of the IEEE conference on computer vision and","duplicateSourceMD5":"761C955F7DA49B4AAB82ECD032B6FB47"},{"score":0.5099415,"similaritySentence":"A.:You only look once:unified，real-time object detection.In:Proceedings","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"YOLO":"0"},{"You":"1"},{"Only":"1"},{"Look":"1"},{"Once":"1"},{"，":"0"},{"is":"0"},{"a":"1"},{"framework":"0"},{"that":"0"},{"deals":"0"},{"with":"0"},{"object":"1"},{"detection":"1"},{"in":"1"},{"a":"1"},{"different":"0"},{"way":"0"},{".":"0"}],"similaritySegGreyList":[{"A":"1"},{".":"1"},{":":"1"},{"You":"1"},{"only":"1"},{"look":"1"},{"once":"1"},{":":"0"},{"unified":"0"},{"，":"0"},{"real":"0"},{"-":"0"},{"time":"0"},{"object":"1"},{"detection":"1"},{".":"1"},{"In":"1"},{":":"0"},{"Proceedings":"0"}],"subSimilaritySentenceSection":"，R.，Farhadi， A.:You only look once:unified，real-time object detection.In:Proceedings of the IEEE Conference on Computer","duplicateSourceMD5":"6A4B249AD797D0E63E8926369D096CE5"},{"score":0.5099415,"similaritySentence":"A.:You only look once:unified，real-time object detection.In:Proceedings","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"YOLO":"0"},{"You":"1"},{"Only":"1"},{"Look":"1"},{"Once":"1"},{"，":"0"},{"is":"0"},{"a":"1"},{"framework":"0"},{"that":"0"},{"deals":"0"},{"with":"0"},{"object":"1"},{"detection":"1"},{"in":"1"},{"a":"1"},{"different":"0"},{"way":"0"},{".":"0"}],"similaritySegGreyList":[{"A":"1"},{".":"1"},{":":"1"},{"You":"1"},{"only":"1"},{"look":"1"},{"once":"1"},{":":"0"},{"unified":"0"},{"，":"0"},{"real":"0"},{"-":"0"},{"time":"0"},{"object":"1"},{"detection":"1"},{".":"1"},{"In":"1"},{":":"0"},{"Proceedings":"0"}],"subSimilaritySentenceSection":"，R.，Farhadi， A.:You only look once:unified，real-time object detection.In:Proceedings of the IEEE Conference on Computer","duplicateSourceMD5":"2CD7404B018BA09CAD185D318E26701F"},{"score":0.50760233,"similaritySentence":"YOLO(You Only Look Once)[9]is a new object detection method，which is","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"YOLO":"1"},{"You":"1"},{"Only":"1"},{"Look":"1"},{"Once":"1"},{"，":"1"},{"is":"1"},{"a":"1"},{"framework":"0"},{"that":"0"},{"deals":"0"},{"with":"0"},{"object":"1"},{"detection":"1"},{"in":"0"},{"a":"1"},{"different":"0"},{"way":"0"},{".":"0"}],"similaritySegGreyList":[{"YOLO":"1"},{"(":"1"},{"You":"1"},{"Only":"1"},{"Look":"1"},{"Once":"1"},{")":"0"},{"[":"0"},{"9":"0"},{"]":"0"},{"is":"1"},{"a":"1"},{"new":"0"},{"object":"1"},{"detection":"1"},{"method":"0"},{"，":"0"},{"which":"0"},{"is":"1"}],"subSimilaritySentenceSection":"detection in the CNN network. YOLO(You Only Look Once)[9]is a new object detection method，which is characterized by fast detection and high","duplicateSourceMD5":"7FC372312A39E6865034FAB821832256"},{"score":0.5024741,"similaritySentence":"YOLO(You Only Look Once)model.YOLO model is a popular object detection algorithm with fast","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"YOLO":"1"},{"You":"1"},{"Only":"1"},{"Look":"1"},{"Once":"1"},{"，":"1"},{"is":"1"},{"a":"1"},{"framework":"0"},{"that":"0"},{"deals":"0"},{"with":"1"},{"object":"1"},{"detection":"1"},{"in":"0"},{"a":"1"},{"different":"0"},{"way":"0"},{".":"0"}],"similaritySegGreyList":[{"YOLO":"1"},{"(":"1"},{"You":"1"},{"Only":"1"},{"Look":"1"},{"Once":"1"},{")":"0"},{"model":"0"},{".":"0"},{"YOLO":"1"},{"model":"0"},{"is":"1"},{"a":"1"},{"popular":"0"},{"object":"1"},{"detection":"1"},{"algorithm":"0"},{"with":"1"},{"fast":"0"}],"subSimilaritySentenceSection":"al.[7]proposed YOLO(You Only Look Once)model.YOLO model is a popular object detection algorithm with fast speed and simple structure.YOLO","duplicateSourceMD5":"B12C73203C2BCF7161A4FE0E6EBAFE46"}]},{"content":"The R-CNN family of techniques primarily use regions to localize the objects within the image.","segRedList":["techniques","primarily","localize","regions","objects","family","within","image","the","The","use","to","of"],"synonymsContent":"","result":[{"score":0.7349537,"similaritySentence":"family of techniques we saw in Part 1 primarily use regions to localize the objects within the image","articleType":"net","classification":"net","originalSegGreyList":[{"The":"1"},{"R":"0"},{"-":"0"},{"CNN":"0"},{"family":"1"},{"of":"1"},{"techniques":"1"},{"primarily":"1"},{"use":"1"},{"regions":"1"},{"to":"1"},{"localize":"1"},{"the":"1"},{"objects":"1"},{"within":"1"},{"the":"1"},{"image":"1"},{".":"1"}],"similaritySegGreyList":[{"family":"1"},{"of":"1"},{"techniques":"1"},{"we":"0"},{"saw":"0"},{"in":"0"},{"Part":"0"},{"1":"0"},{"primarily":"1"},{"use":"1"},{"regions":"1"},{"to":"1"},{"localize":"1"},{"the":"1"},{"objects":"1"},{"within":"1"},{"the":"1"},{"image":"1"}],"subSimilaritySentenceSection":"Useful?The R-CNN family of techniques we saw in Part 1 primarily use regions to localize the objects within the image .The network does not look","duplicateSourceMD5":"A47D1A1C113E204138222B36D372E27F"},{"score":0.7063492,"similaritySentence":").The RCNN family of object detectors uses regions to localize the objects within an image.","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"The":"1"},{"R":"0"},{"-":"0"},{"CNN":"0"},{"family":"1"},{"of":"1"},{"techniques":"0"},{"primarily":"0"},{"use":"1"},{"regions":"1"},{"to":"1"},{"localize":"1"},{"the":"1"},{"objects":"1"},{"within":"1"},{"the":"1"},{"image":"1"},{".":"1"}],"similaritySegGreyList":[{")":"1"},{".":"1"},{"The":"1"},{"RCNN":"0"},{"family":"1"},{"of":"1"},{"object":"1"},{"detectors":"0"},{"uses":"1"},{"regions":"1"},{"to":"1"},{"localize":"1"},{"the":"1"},{"objects":"1"},{"within":"1"},{"an":"0"},{"image":"1"},{".":"1"}],"subSimilaritySentenceSection":"YOLO(You Only Look Once ).The RCNN family of object detectors uses regions to localize the objects within an image. Instead of the entire the network","duplicateSourceMD5":"D5E353C0D3D724A261A1CFFC6547BF30"},{"score":0.712482,"similaritySentence":"YOLO?The R CNN family of techniques used to localize the objects within the image.The","articleType":"net","classification":"net","originalSegGreyList":[{"The":"1"},{"R":"1"},{"-":"1"},{"CNN":"1"},{"family":"1"},{"of":"1"},{"techniques":"1"},{"primarily":"0"},{"use":"1"},{"regions":"0"},{"to":"1"},{"localize":"1"},{"the":"1"},{"objects":"1"},{"within":"1"},{"the":"1"},{"image":"1"},{".":"1"}],"similaritySegGreyList":[{"YOLO":"0"},{"?":"0"},{"The":"1"},{"R":"1"},{"CNN":"1"},{"family":"1"},{"of":"1"},{"techniques":"1"},{"used":"1"},{"to":"1"},{"localize":"1"},{"the":"1"},{"objects":"1"},{"within":"1"},{"the":"1"},{"image":"1"},{".":"1"},{"The":"1"}],"subSimilaritySentenceSection":"box for classified objects.Why YOLO?The R CNN family of techniques used to localize the objects within the image.The network does not look at the","duplicateSourceMD5":"BAE251915346CF40266C012D04197185"},{"score":0.6414141,"similaritySentence":"of-the-art object detection algorithms use regions to localize the object within the image.","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"The":"1"},{"R":"0"},{"-":"0"},{"CNN":"0"},{"family":"0"},{"of":"1"},{"techniques":"0"},{"primarily":"0"},{"use":"1"},{"regions":"1"},{"to":"1"},{"localize":"1"},{"the":"1"},{"objects":"1"},{"within":"1"},{"the":"1"},{"image":"1"},{".":"1"}],"similaritySegGreyList":[{"of":"1"},{"-":"1"},{"the":"1"},{"-":"0"},{"art":"0"},{"object":"1"},{"detection":"0"},{"algorithms":"0"},{"use":"1"},{"regions":"1"},{"to":"1"},{"localize":"1"},{"the":"1"},{"object":"1"},{"within":"1"},{"the":"1"},{"image":"1"},{".":"1"}],"subSimilaritySentenceSection":"All of the previous state- of-the-art object detection algorithms use regions to localize the object within the image. The network does not look at","duplicateSourceMD5":"FD50E04EE012BB763249F57F298292DC"},{"score":0.6382876,"similaritySentence":"]for tools detection.Faster R-CNN used regions to localize the object within the image","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"The":"1"},{"R":"1"},{"-":"1"},{"CNN":"1"},{"family":"0"},{"of":"0"},{"techniques":"0"},{"primarily":"0"},{"use":"1"},{"regions":"1"},{"to":"1"},{"localize":"1"},{"the":"1"},{"objects":"1"},{"within":"1"},{"the":"1"},{"image":"1"},{".":"1"}],"similaritySegGreyList":[{"]":"0"},{"for":"0"},{"tools":"0"},{"detection":"0"},{".":"0"},{"Faster":"0"},{"R":"1"},{"-":"1"},{"CNN":"1"},{"used":"1"},{"regions":"1"},{"to":"1"},{"localize":"1"},{"the":"1"},{"object":"1"},{"within":"1"},{"the":"1"},{"image":"1"}],"subSimilaritySentenceSection":"[33]and[34 ]for tools detection.Faster R-CNN used regions to localize the object within the image and the network does not look","duplicateSourceMD5":"5491B9C6E7B61F88A7352001B8E2E152"},{"score":0.5753367,"similaritySentence":"All of the previous object detection algorithms use regions to localize the object within the image.The","articleType":"net","classification":"net","originalSegGreyList":[{"The":"1"},{"R":"0"},{"-":"0"},{"CNN":"0"},{"family":"0"},{"of":"1"},{"techniques":"0"},{"primarily":"0"},{"use":"1"},{"regions":"1"},{"to":"1"},{"localize":"1"},{"the":"1"},{"objects":"1"},{"within":"1"},{"the":"1"},{"image":"1"},{".":"1"}],"similaritySegGreyList":[{"All":"0"},{"of":"1"},{"the":"1"},{"previous":"0"},{"object":"1"},{"detection":"0"},{"algorithms":"0"},{"use":"1"},{"regions":"1"},{"to":"1"},{"localize":"1"},{"the":"1"},{"object":"1"},{"within":"1"},{"the":"1"},{"image":"1"},{".":"1"},{"The":"1"}],"subSimilaritySentenceSection":"All of the previous object detection algorithms use regions to localize the object within the image.The network does not look at the","duplicateSourceMD5":"715324D948DD15E991DF83F2CCA4DC73"},{"score":0.51623374,"similaritySentence":"object.It maybe in the form of a bounding rectangle localizing the object within the image or","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"The":"1"},{"R":"0"},{"-":"0"},{"CNN":"0"},{"family":"0"},{"of":"1"},{"techniques":"0"},{"primarily":"0"},{"use":"0"},{"regions":"0"},{"to":"0"},{"localize":"1"},{"the":"1"},{"objects":"1"},{"within":"1"},{"the":"1"},{"image":"1"},{".":"1"}],"similaritySegGreyList":[{"object":"1"},{".":"0"},{"It":"0"},{"maybe":"0"},{"in":"0"},{"the":"1"},{"form":"0"},{"of":"1"},{"a":"0"},{"bounding":"0"},{"rectangle":"0"},{"localizing":"1"},{"the":"1"},{"object":"1"},{"within":"1"},{"the":"1"},{"image":"1"},{"or":"0"}],"subSimilaritySentenceSection":"the location of a particular target object.It maybe in the form of a bounding rectangle localizing the object within the image or a segmentation mask showing precisely where","duplicateSourceMD5":"206E5D20B6B24D3E6E2BEF55B4558DD8"}]},{"content":"The network does not look at the entire image，only at the parts of the images which have a higher chance of containing an object[2].","segRedList":["containing","network","higher","images","chance","object","entire","parts","image","which","only","does","look","have","not","the","The","of","an","at","a"],"synonymsContent":"","result":[{"score":0.9150943,"similaritySentence":".The network does not look at the entire image，only at the parts of the images which have a higher chance of containing an object.The yolo","articleType":"net","classification":"net","originalSegGreyList":[{"The":"1"},{"network":"1"},{"does":"1"},{"not":"1"},{"look":"1"},{"at":"1"},{"the":"1"},{"entire":"1"},{"image":"1"},{"，":"1"},{"only":"1"},{"at":"1"},{"the":"1"},{"parts":"1"},{"of":"1"},{"the":"1"},{"images":"1"},{"which":"1"},{"have":"1"},{"a":"1"},{"higher":"1"},{"chance":"1"},{"of":"1"},{"containing":"1"},{"an":"1"},{"object":"1"},{"[":"0"},{"2":"0"},{"]":"0"},{".":"0"}],"similaritySegGreyList":[{".":"1"},{"The":"1"},{"network":"1"},{"does":"1"},{"not":"1"},{"look":"1"},{"at":"1"},{"the":"1"},{"entire":"1"},{"image":"1"},{"，":"1"},{"only":"1"},{"at":"1"},{"the":"1"},{"parts":"1"},{"of":"1"},{"the":"1"},{"images":"1"},{"which":"1"},{"have":"1"},{"a":"1"},{"higher":"1"},{"chance":"1"},{"of":"1"},{"containing":"1"},{"an":"1"},{"object":"1"},{".":"1"},{"The":"1"},{"yolo":"0"}],"subSimilaritySentenceSection":"localize the objects within the image .The network does not look at the entire image，only at the parts of the images which have a higher chance of containing an object.The yolo framework，on the other hand","duplicateSourceMD5":"BAE251915346CF40266C012D04197185"},{"score":0.9035088,"similaritySentence":"dic 2018—The network does not look at the entire image，only at the parts of the images which have a higher chance of containing an object.","articleType":"net","classification":"net","originalSegGreyList":[{"The":"1"},{"network":"1"},{"does":"1"},{"not":"1"},{"look":"1"},{"at":"1"},{"the":"1"},{"entire":"1"},{"image":"1"},{"，":"1"},{"only":"1"},{"at":"1"},{"the":"1"},{"parts":"1"},{"of":"1"},{"the":"1"},{"images":"1"},{"which":"1"},{"have":"1"},{"a":"1"},{"higher":"1"},{"chance":"1"},{"of":"1"},{"containing":"1"},{"an":"1"},{"object":"1"},{"[":"0"},{"2":"0"},{"]":"0"},{".":"0"}],"similaritySegGreyList":[{"dic":"0"},{"2018":"0"},{"—":"0"},{"The":"1"},{"network":"1"},{"does":"1"},{"not":"1"},{"look":"1"},{"at":"1"},{"the":"1"},{"entire":"1"},{"image":"1"},{"，":"1"},{"only":"1"},{"at":"1"},{"the":"1"},{"parts":"1"},{"of":"1"},{"the":"1"},{"images":"1"},{"which":"1"},{"have":"1"},{"a":"1"},{"higher":"1"},{"chance":"1"},{"of":"1"},{"containing":"1"},{"an":"1"},{"object":"1"},{".":"1"}],"subSimilaritySentenceSection":"6 dic 2018—The network does not look at the entire image，only at the parts of the images which have a higher chance of containing an object.","duplicateSourceMD5":"A47D1A1C113E204138222B36D372E27F"}]},{"content":"Yolo on other hand takes the entire image in a single instance and predicts the bounding box coordinates and class probabilities for these boxes.","segRedList":["probabilities","coordinates","bounding","instance","predicts","single","entire","image","takes","boxes","these","class","box","for","the","and","in","a"],"synonymsContent":"","result":[{"score":0.8852459,"similaritySentence":"It takes the entire image in a single instance and predicts the bounding box coordinates and class probabilities for these boxes.","articleType":"net","classification":"net","originalSegGreyList":[{"Yolo":"0"},{"on":"0"},{"other":"0"},{"hand":"0"},{"takes":"1"},{"the":"1"},{"entire":"1"},{"image":"1"},{"in":"1"},{"a":"1"},{"single":"1"},{"instance":"1"},{"and":"1"},{"predicts":"1"},{"the":"1"},{"bounding":"1"},{"box":"1"},{"coordinates":"1"},{"and":"1"},{"class":"1"},{"probabilities":"1"},{"for":"1"},{"these":"1"},{"boxes":"1"},{".":"1"}],"similaritySegGreyList":[{"It":"0"},{"takes":"1"},{"the":"1"},{"entire":"1"},{"image":"1"},{"in":"1"},{"a":"1"},{"single":"1"},{"instance":"1"},{"and":"1"},{"predicts":"1"},{"the":"1"},{"bounding":"1"},{"box":"1"},{"coordinates":"1"},{"and":"1"},{"class":"1"},{"probabilities":"1"},{"for":"1"},{"these":"1"},{"boxes":"1"},{".":"1"}],"subSimilaritySentenceSection":"It takes the entire image in a single instance and predicts the bounding box coordinates and class probabilities for these boxes.","duplicateSourceMD5":"BAE251915346CF40266C012D04197185"},{"score":0.8770492,"similaritySentence":"2018/12/06 — It takes the entire image in a single instance and predicts the bounding box coordinates and class probabilities for these boxes.","articleType":"net","classification":"net","originalSegGreyList":[{"Yolo":"0"},{"on":"0"},{"other":"0"},{"hand":"0"},{"takes":"1"},{"the":"1"},{"entire":"1"},{"image":"1"},{"in":"1"},{"a":"1"},{"single":"1"},{"instance":"1"},{"and":"1"},{"predicts":"1"},{"the":"1"},{"bounding":"1"},{"box":"1"},{"coordinates":"1"},{"and":"1"},{"class":"1"},{"probabilities":"1"},{"for":"1"},{"these":"1"},{"boxes":"1"},{".":"1"}],"similaritySegGreyList":[{"2018":"0"},{"/":"0"},{"12":"0"},{"/":"0"},{"06":"0"},{"—":"0"},{"It":"0"},{"takes":"1"},{"the":"1"},{"entire":"1"},{"image":"1"},{"in":"1"},{"a":"1"},{"single":"1"},{"instance":"1"},{"and":"1"},{"predicts":"1"},{"the":"1"},{"bounding":"1"},{"box":"1"},{"coordinates":"1"},{"and":"1"},{"class":"1"},{"probabilities":"1"},{"for":"1"},{"these":"1"},{"boxes":"1"},{".":"1"}],"subSimilaritySentenceSection":"2018/12/06 — It takes the entire image in a single instance and predicts the bounding box coordinates and class probabilities for these boxes.","duplicateSourceMD5":"A47D1A1C113E204138222B36D372E27F"},{"score":0.7814815,"similaritySentence":".YOLO framework analyzes the entire image in a single instance and predicts the bounding boxes and the corresponding classes probabilities for these boxes.","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"Yolo":"1"},{"on":"0"},{"other":"0"},{"hand":"0"},{"takes":"0"},{"the":"1"},{"entire":"1"},{"image":"1"},{"in":"1"},{"a":"1"},{"single":"1"},{"instance":"1"},{"and":"1"},{"predicts":"1"},{"the":"1"},{"bounding":"1"},{"box":"1"},{"coordinates":"0"},{"and":"1"},{"class":"1"},{"probabilities":"1"},{"for":"1"},{"these":"1"},{"boxes":"1"},{".":"1"}],"similaritySegGreyList":[{".":"1"},{"YOLO":"1"},{"framework":"0"},{"analyzes":"0"},{"the":"1"},{"entire":"1"},{"image":"1"},{"in":"1"},{"a":"1"},{"single":"1"},{"instance":"1"},{"and":"1"},{"predicts":"1"},{"the":"1"},{"bounding":"1"},{"boxes":"1"},{"and":"1"},{"the":"1"},{"corresponding":"0"},{"classes":"1"},{"probabilities":"1"},{"for":"1"},{"these":"1"},{"boxes":"1"},{".":"1"}],"subSimilaritySentenceSection":"network in order to detect objects .YOLO framework analyzes the entire image in a single instance and predicts the bounding boxes and the corresponding classes probabilities for these boxes. The main advantage of YOLO over","duplicateSourceMD5":"D5E353C0D3D724A261A1CFFC6547BF30"},{"score":0.77622944,"similaritySentence":"It takes the entire image in a single instance and predicts the bounding box coordinates and class probabilities for these boxes.cfg model_","articleType":"net","classification":"net","originalSegGreyList":[{"Yolo":"0"},{"on":"0"},{"other":"0"},{"hand":"0"},{"takes":"1"},{"the":"1"},{"entire":"1"},{"image":"1"},{"in":"1"},{"a":"1"},{"single":"1"},{"instance":"1"},{"and":"1"},{"predicts":"1"},{"the":"1"},{"bounding":"1"},{"box":"1"},{"coordinates":"1"},{"and":"1"},{"class":"1"},{"probabilities":"1"},{"for":"1"},{"these":"1"},{"boxes":"1"},{".":"1"}],"similaritySegGreyList":[{"It":"0"},{"takes":"1"},{"the":"1"},{"entire":"1"},{"image":"1"},{"in":"1"},{"a":"1"},{"single":"1"},{"instance":"1"},{"and":"1"},{"predicts":"1"},{"the":"1"},{"bounding":"1"},{"box":"1"},{"coordinates":"1"},{"and":"1"},{"class":"1"},{"probabilities":"1"},{"for":"1"},{"these":"1"},{"boxes":"1"},{".":"0"},{"cfg":"0"},{"model":"0"},{"_":"0"}],"subSimilaritySentenceSection":"It takes the entire image in a single instance and predicts the bounding box coordinates and class probabilities for these boxes.cfg model_data/yolov3.","duplicateSourceMD5":"509CEA6B5A00EB3C1D27210304EA2045"},{"score":0.5061624,"similaritySentence":"box coordinates and class probabilities.In YOLO，a single CNN simultaneously predicts multiple bounding-boxes and class probabilities for those boxes.","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Yolo":"1"},{"on":"0"},{"other":"0"},{"hand":"0"},{"takes":"0"},{"the":"0"},{"entire":"0"},{"image":"0"},{"in":"1"},{"a":"1"},{"single":"1"},{"instance":"0"},{"and":"1"},{"predicts":"1"},{"the":"0"},{"bounding":"1"},{"box":"1"},{"coordinates":"1"},{"and":"1"},{"class":"1"},{"probabilities":"1"},{"for":"1"},{"these":"0"},{"boxes":"1"},{".":"1"}],"similaritySegGreyList":[{"box":"1"},{"coordinates":"1"},{"and":"1"},{"class":"1"},{"probabilities":"1"},{".":"1"},{"In":"1"},{"YOLO":"1"},{"，":"1"},{"a":"1"},{"single":"1"},{"CNN":"0"},{"simultaneously":"0"},{"predicts":"1"},{"multiple":"0"},{"bounding":"1"},{"-":"1"},{"boxes":"1"},{"and":"1"},{"class":"1"},{"probabilities":"1"},{"for":"1"},{"those":"0"},{"boxes":"1"},{".":"1"}],"subSimilaritySentenceSection":"from image pixels to bounding- box coordinates and class probabilities.In YOLO，a single CNN simultaneously predicts multiple bounding-boxes and class probabilities for those boxes. YOLO approach trains on full images","duplicateSourceMD5":"D09334F5D328270905422DC3CEB3AA8F"},{"score":0.50178576,"similaritySentence":"Look Once\u0027)method[20]employs a single convolutional network which predicts the bounding boxes and the class probabilities for these boxes","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Yolo":"0"},{"on":"0"},{"other":"0"},{"hand":"0"},{"takes":"0"},{"the":"1"},{"entire":"0"},{"image":"0"},{"in":"0"},{"a":"1"},{"single":"1"},{"instance":"0"},{"and":"1"},{"predicts":"1"},{"the":"1"},{"bounding":"1"},{"box":"1"},{"coordinates":"0"},{"and":"1"},{"class":"1"},{"probabilities":"1"},{"for":"1"},{"these":"1"},{"boxes":"1"},{".":"1"}],"similaritySegGreyList":[{"Look":"0"},{"Once":"0"},{"\u0027":"0"},{")":"0"},{"method":"0"},{"[":"0"},{"20":"0"},{"]":"0"},{"employs":"0"},{"a":"1"},{"single":"1"},{"convolutional":"0"},{"network":"0"},{"which":"0"},{"predicts":"1"},{"the":"1"},{"bounding":"1"},{"boxes":"1"},{"and":"1"},{"the":"1"},{"class":"1"},{"probabilities":"1"},{"for":"1"},{"these":"1"},{"boxes":"1"}],"subSimilaritySentenceSection":"the YOLO(‘ You Only Look Once\u0027)method[20]employs a single convolutional network which predicts the bounding boxes and the class probabilities for these boxes .YOLO has demonstrated state-","duplicateSourceMD5":"FD50E04EE012BB763249F57F298292DC"},{"score":0.50101185,"similaritySentence":"built on a dark flow framework and uses a single convolutional network to predict the bounding boxes and the class probabilities for these boxes.","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Yolo":"0"},{"on":"1"},{"other":"0"},{"hand":"0"},{"takes":"0"},{"the":"1"},{"entire":"0"},{"image":"0"},{"in":"0"},{"a":"1"},{"single":"1"},{"instance":"0"},{"and":"1"},{"predicts":"1"},{"the":"1"},{"bounding":"1"},{"box":"1"},{"coordinates":"0"},{"and":"1"},{"class":"1"},{"probabilities":"1"},{"for":"1"},{"these":"1"},{"boxes":"1"},{".":"1"}],"similaritySegGreyList":[{"built":"0"},{"on":"1"},{"a":"1"},{"dark":"0"},{"flow":"0"},{"framework":"0"},{"and":"1"},{"uses":"0"},{"a":"1"},{"single":"1"},{"convolutional":"0"},{"network":"0"},{"to":"0"},{"predict":"1"},{"the":"1"},{"bounding":"1"},{"boxes":"1"},{"and":"1"},{"the":"1"},{"class":"1"},{"probabilities":"1"},{"for":"1"},{"these":"1"},{"boxes":"1"},{".":"1"}],"subSimilaritySentenceSection":"than SSD Mobilenet，which is built on a dark flow framework and uses a single convolutional network to predict the bounding boxes and the class probabilities for these boxes. Hence，it required less computation","duplicateSourceMD5":"48A7B294968194D7D88D900AC3CE3977"}]},{"content":"These bounding boxes are weighted with the calculated probabilities[2].","segRedList":[],"synonymsContent":"","result":[]},{"content":"The biggest advantage of using YOLO is its speed it is incredibly fast and can process 45 frames per second.","segRedList":["incredibly","advantage","process","biggest","second","frames","using","speed","yolo","fast","YOLO","its","per","and","can","45","is","it"],"synonymsContent":"","result":[{"score":0.8253968,"similaritySentence":"biggest advantage using yolo is its superb speed – it\u0027s incredibly fast and can process 45 frames per second.","articleType":"net","classification":"net","originalSegGreyList":[{"The":"0"},{"biggest":"1"},{"advantage":"1"},{"of":"0"},{"using":"1"},{"YOLO":"1"},{"is":"1"},{"its":"1"},{"speed":"1"},{"it":"1"},{"is":"1"},{"incredibly":"1"},{"fast":"1"},{"and":"1"},{"can":"1"},{"process":"1"},{"45":"1"},{"frames":"1"},{"per":"1"},{"second":"1"},{".":"1"}],"similaritySegGreyList":[{"biggest":"1"},{"advantage":"1"},{"using":"1"},{"yolo":"1"},{"is":"1"},{"its":"1"},{"superb":"0"},{"speed":"1"},{"–":"0"},{"it":"1"},{"\u0027":"0"},{"s":"0"},{"incredibly":"1"},{"fast":"1"},{"and":"1"},{"can":"1"},{"process":"1"},{"45":"1"},{"frames":"1"},{"per":"1"},{"second":"1"},{".":"1"}],"subSimilaritySentenceSection":"The biggest advantage using yolo is its superb speed – it\u0027s incredibly fast and can process 45 frames per second.yolo also understands generalized object","duplicateSourceMD5":"BAE251915346CF40266C012D04197185"},{"score":0.77590513,"similaritySentence":"biggest advantage of using YOLO is its superb speed – it\u0027s incredibly fast and can process 45 frames per","articleType":"net","classification":"net","originalSegGreyList":[{"The":"0"},{"biggest":"1"},{"advantage":"1"},{"of":"1"},{"using":"1"},{"YOLO":"1"},{"is":"1"},{"its":"1"},{"speed":"1"},{"it":"1"},{"is":"1"},{"incredibly":"1"},{"fast":"1"},{"and":"1"},{"can":"1"},{"process":"1"},{"45":"1"},{"frames":"1"},{"per":"1"},{"second":"0"},{".":"0"}],"similaritySegGreyList":[{"biggest":"1"},{"advantage":"1"},{"of":"1"},{"using":"1"},{"YOLO":"1"},{"is":"1"},{"its":"1"},{"superb":"0"},{"speed":"1"},{"–":"0"},{"it":"1"},{"\u0027":"0"},{"s":"0"},{"incredibly":"1"},{"fast":"1"},{"and":"1"},{"can":"1"},{"process":"1"},{"45":"1"},{"frames":"1"},{"per":"1"}],"subSimilaritySentenceSection":"Dec 6，2018—The biggest advantage of using YOLO is its superb speed – it\u0027s incredibly fast and can process 45 frames per second.YOLO also understands","duplicateSourceMD5":"A47D1A1C113E204138222B36D372E27F"},{"score":0.65168536,"similaritySentence":"Its biggest advantage is its speed， and it can process 45 frames per second.","articleType":"net","classification":"net","originalSegGreyList":[{"The":"0"},{"biggest":"1"},{"advantage":"1"},{"of":"0"},{"using":"0"},{"YOLO":"0"},{"is":"1"},{"its":"1"},{"speed":"1"},{"it":"1"},{"is":"1"},{"incredibly":"0"},{"fast":"0"},{"and":"1"},{"can":"1"},{"process":"1"},{"45":"1"},{"frames":"1"},{"per":"1"},{"second":"1"},{".":"1"}],"similaritySegGreyList":[{"Its":"1"},{"biggest":"1"},{"advantage":"1"},{"is":"1"},{"its":"1"},{"speed":"1"},{"，":"1"},{"and":"1"},{"it":"1"},{"can":"1"},{"process":"1"},{"45":"1"},{"frames":"1"},{"per":"1"},{"second":"1"},{".":"1"}],"subSimilaritySentenceSection":"Its biggest advantage is its speed， and it can process 45 frames per second.","duplicateSourceMD5":"5A56DDA1F15C3B8C8DFB3C54C5630E49"},{"score":0.6000369,"similaritySentence":"of YOLO over other object detectors is its speed.It is fast and can process 45 frame per second.","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"The":"0"},{"biggest":"0"},{"advantage":"0"},{"of":"1"},{"using":"0"},{"YOLO":"1"},{"is":"1"},{"its":"1"},{"speed":"1"},{"it":"1"},{"is":"1"},{"incredibly":"0"},{"fast":"1"},{"and":"1"},{"can":"1"},{"process":"1"},{"45":"1"},{"frames":"1"},{"per":"1"},{"second":"1"},{".":"1"}],"similaritySegGreyList":[{"of":"1"},{"YOLO":"1"},{"over":"0"},{"other":"0"},{"object":"0"},{"detectors":"0"},{"is":"1"},{"its":"1"},{"speed":"1"},{".":"1"},{"It":"1"},{"is":"1"},{"fast":"1"},{"and":"1"},{"can":"1"},{"process":"1"},{"45":"1"},{"frame":"1"},{"per":"1"},{"second":"1"},{".":"1"}],"subSimilaritySentenceSection":"these boxes.The main advantage of YOLO over other object detectors is its speed.It is fast and can process 45 frame per second. YOLO predicts object-ness score","duplicateSourceMD5":"D5E353C0D3D724A261A1CFFC6547BF30"}]},{"content":"YOLO also understands generalized object representation[2].","segRedList":[],"synonymsContent":"","result":[]},{"content":"Being a part of the class of robust object detection algorithms，YOLO has been a significant leap in this field.","segRedList":[],"synonymsContent":"","result":[]},{"content":"Since it looks at the image a whole the predictions are informed by the global context of the image.","segRedList":["predictions","informed","context","global","looks","whole","image","are","the","by","it","It","at"],"synonymsContent":"","result":[{"score":0.6555555,"similaritySentence":"It looks at the whole image at test time so its predictions are informed by global context in the image","articleType":"net","classification":"net","originalSegGreyList":[{"Since":"0"},{"it":"1"},{"looks":"1"},{"at":"1"},{"the":"1"},{"image":"1"},{"a":"0"},{"whole":"1"},{"the":"1"},{"predictions":"1"},{"are":"1"},{"informed":"1"},{"by":"1"},{"the":"1"},{"global":"1"},{"context":"1"},{"of":"0"},{"the":"1"},{"image":"1"},{".":"1"}],"similaritySegGreyList":[{"It":"1"},{"looks":"1"},{"at":"1"},{"the":"1"},{"whole":"1"},{"image":"1"},{"at":"1"},{"test":"0"},{"time":"0"},{"so":"0"},{"its":"0"},{"predictions":"1"},{"are":"1"},{"informed":"1"},{"by":"1"},{"global":"1"},{"context":"1"},{"in":"0"},{"the":"1"},{"image":"1"}],"subSimilaritySentenceSection":"It looks at the whole image at test time so its predictions are informed by global context in the image .A sample image from the","duplicateSourceMD5":"950B7D6091A935F5E5BF8658E04A5AEB"},{"score":0.6555555,"similaritySentence":"It looks at the whole image at test time so its predictions are informed by global context in the image","articleType":"net","classification":"net","originalSegGreyList":[{"Since":"0"},{"it":"1"},{"looks":"1"},{"at":"1"},{"the":"1"},{"image":"1"},{"a":"0"},{"whole":"1"},{"the":"1"},{"predictions":"1"},{"are":"1"},{"informed":"1"},{"by":"1"},{"the":"1"},{"global":"1"},{"context":"1"},{"of":"0"},{"the":"1"},{"image":"1"},{".":"1"}],"similaritySegGreyList":[{"It":"1"},{"looks":"1"},{"at":"1"},{"the":"1"},{"whole":"1"},{"image":"1"},{"at":"1"},{"test":"0"},{"time":"0"},{"so":"0"},{"its":"0"},{"predictions":"1"},{"are":"1"},{"informed":"1"},{"by":"1"},{"global":"1"},{"context":"1"},{"in":"0"},{"the":"1"},{"image":"1"}],"subSimilaritySentenceSection":"It looks at the whole image at test time so its predictions are informed by global context in the image .It also makes predictions with","duplicateSourceMD5":"D3674FA0CFC34DED9A8E3071782F4866"},{"score":0.53248936,"similaritySentence":"person because it looks at the whole image at the test time，so the global context in the image","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"Since":"0"},{"it":"1"},{"looks":"1"},{"at":"1"},{"the":"1"},{"image":"1"},{"a":"0"},{"whole":"1"},{"the":"1"},{"predictions":"0"},{"are":"0"},{"informed":"0"},{"by":"0"},{"the":"1"},{"global":"1"},{"context":"1"},{"of":"0"},{"the":"1"},{"image":"1"},{".":"1"}],"similaritySegGreyList":[{"person":"0"},{"because":"0"},{"it":"1"},{"looks":"1"},{"at":"1"},{"the":"1"},{"whole":"1"},{"image":"1"},{"at":"1"},{"the":"1"},{"test":"0"},{"time":"0"},{"，":"0"},{"so":"0"},{"the":"1"},{"global":"1"},{"context":"1"},{"in":"0"},{"the":"1"},{"image":"1"}],"subSimilaritySentenceSection":"the YOLO algorithm to detect a person because it looks at the whole image at the test time，so the global context in the image informs its predictions.This algorithm","duplicateSourceMD5":"6A62C598504B7D27BB9E7D49E380F0DA"},{"score":0.5158227,"similaritySentence":"it is extremely fast and its predictions are informed by global context in the data.2.6.","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Since":"0"},{"it":"1"},{"looks":"0"},{"at":"0"},{"the":"1"},{"image":"0"},{"a":"0"},{"whole":"0"},{"the":"1"},{"predictions":"1"},{"are":"1"},{"informed":"1"},{"by":"1"},{"the":"1"},{"global":"1"},{"context":"1"},{"of":"0"},{"the":"1"},{"image":"0"},{".":"0"}],"similaritySegGreyList":[{"it":"1"},{"is":"1"},{"extremely":"0"},{"fast":"0"},{"and":"0"},{"its":"0"},{"predictions":"1"},{"are":"1"},{"informed":"1"},{"by":"1"},{"global":"1"},{"context":"1"},{"in":"0"},{"the":"1"},{"data":"0"},{".":"0"},{"2":"0"},{".":"0"},{"6":"0"},{".":"0"}],"subSimilaritySentenceSection":"not require a complex pipeline， it is extremely fast and its predictions are informed by global context in the data.2.6. YOLO Architecture YOLO is a unified","duplicateSourceMD5":"85091583DCE9AD8C33D21A6EEF57E782"}]},{"content":"Also，YOLO is one of the best algorithms for object detection and has shown a comparatively similar and sometimes better performance to the R-CNN and Fast R-CNN algorithms，","segRedList":["comparatively","performance","algorithms","detection","similar","object","shown","best","one","has","for","CNN","and","the","to","of","is","a","R"],"synonymsContent":"","result":[{"score":0.5913913,"similaritySentence":".This is one of the best algorithms for object detection and has shown a comparatively similar performance to the R-CNN algorithms.In the upcoming sections，we will learn about","articleType":"net","classification":"net","originalSegGreyList":[{"Also":"0"},{"，":"0"},{"YOLO":"0"},{"is":"1"},{"one":"1"},{"of":"1"},{"the":"1"},{"best":"1"},{"algorithms":"1"},{"for":"1"},{"object":"1"},{"detection":"1"},{"and":"1"},{"has":"1"},{"shown":"1"},{"a":"1"},{"comparatively":"1"},{"similar":"1"},{"and":"1"},{"sometimes":"0"},{"better":"0"},{"performance":"1"},{"to":"1"},{"the":"1"},{"R":"1"},{"-":"1"},{"CNN":"1"},{"and":"1"},{"Fast":"0"},{"R":"1"},{"-":"1"},{"CNN":"1"},{"algorithms":"1"},{"，":"1"}],"similaritySegGreyList":[{".":"0"},{"This":"0"},{"is":"1"},{"one":"1"},{"of":"1"},{"the":"1"},{"best":"1"},{"algorithms":"1"},{"for":"1"},{"object":"1"},{"detection":"1"},{"and":"1"},{"has":"1"},{"shown":"1"},{"a":"1"},{"comparatively":"1"},{"similar":"1"},{"performance":"1"},{"to":"1"},{"the":"1"},{"R":"1"},{"-":"1"},{"CNN":"1"},{"algorithms":"1"},{".":"0"},{"In":"0"},{"the":"1"},{"upcoming":"0"},{"sections":"0"},{"，":"0"},{"we":"0"},{"will":"0"},{"learn":"0"},{"about":"0"}],"subSimilaritySentenceSection":"YOLO also understands generalized object representation .This is one of the best algorithms for object detection and has shown a comparatively similar performance to the R-CNN algorithms.In the upcoming sections，we will learn about different techniques used in YOLO algorithm","duplicateSourceMD5":"A47D1A1C113E204138222B36D372E27F"},{"score":0.57372814,"similaritySentence":".This is one of the best algorithms for object detection and has shown a comparatively similar performance to the R CNN algorithms.So for better results we are using yolov4 for developing","articleType":"net","classification":"net","originalSegGreyList":[{"Also":"0"},{"，":"0"},{"YOLO":"0"},{"is":"1"},{"one":"1"},{"of":"1"},{"the":"1"},{"best":"1"},{"algorithms":"1"},{"for":"1"},{"object":"1"},{"detection":"1"},{"and":"1"},{"has":"1"},{"shown":"1"},{"a":"1"},{"comparatively":"1"},{"similar":"1"},{"and":"1"},{"sometimes":"0"},{"better":"1"},{"performance":"1"},{"to":"1"},{"the":"1"},{"R":"1"},{"-":"1"},{"CNN":"1"},{"and":"1"},{"Fast":"0"},{"R":"1"},{"-":"1"},{"CNN":"1"},{"algorithms":"1"},{"，":"1"}],"similaritySegGreyList":[{".":"0"},{"This":"0"},{"is":"1"},{"one":"1"},{"of":"1"},{"the":"1"},{"best":"1"},{"algorithms":"1"},{"for":"1"},{"object":"1"},{"detection":"1"},{"and":"1"},{"has":"1"},{"shown":"1"},{"a":"1"},{"comparatively":"1"},{"similar":"1"},{"performance":"1"},{"to":"1"},{"the":"1"},{"R":"1"},{"CNN":"1"},{"algorithms":"1"},{".":"0"},{"So":"0"},{"for":"1"},{"better":"1"},{"results":"0"},{"we":"0"},{"are":"1"},{"using":"0"},{"yolov4":"0"},{"for":"1"},{"developing":"0"}],"subSimilaritySentenceSection":"yolo also understands generalized object representation .This is one of the best algorithms for object detection and has shown a comparatively similar performance to the R CNN algorithms.So for better results we are using yolov4 for developing custom object detection.How YOLO","duplicateSourceMD5":"BAE251915346CF40266C012D04197185"}]},{"content":"since it uses single network for predictions unlike R-CNN that need plethora of images[2].","segRedList":[],"synonymsContent":"","result":[]},{"content":"3.2 CNN(Convolutional Neural Network)","segRedList":[],"synonymsContent":"","result":[]},{"content":"To detect if the trespassing is happening or not we feed the images to the CNN.","segRedList":[],"synonymsContent":"","result":[]},{"content":"","segRedList":[],"synonymsContent":"","result":[]},{"content":"Given an input surveillance video containing N frames，the model has to predict a binary time series of the same length N such that at each index i，","segRedList":[],"synonymsContent":"","result":[]},{"content":"we have the label yi of the corresponding frame fi[1].","segRedList":[],"synonymsContent":"","result":[]},{"content":"The human trespassing label is assigned to the positive class(1)while the“other activity”label is assigned to the negative class(0).","segRedList":["trespassing","activity","negative","positive","assigned","class","other","human","Human","label","is","to","1","0"],"synonymsContent":"","result":[{"score":0.69358975,"similaritySentence":"sponding frame fi.Human trespassing label is assigned to positive(1)class and.“other activity”label is assigned to negative class(0)","articleType":"net","classification":"net","originalSegGreyList":[{"The":"0"},{"human":"1"},{"trespassing":"1"},{"label":"1"},{"is":"1"},{"assigned":"1"},{"to":"1"},{"the":"0"},{"positive":"1"},{"class":"1"},{"(":"1"},{"1":"1"},{")":"0"},{"while":"0"},{"the":"0"},{"“":"0"},{"other":"1"},{"activity":"1"},{"”":"1"},{"label":"1"},{"is":"1"},{"assigned":"1"},{"to":"1"},{"the":"0"},{"negative":"1"},{"class":"1"},{"(":"1"},{"0":"1"},{")":"1"},{".":"1"}],"similaritySegGreyList":[{"sponding":"0"},{"frame":"0"},{"fi":"0"},{".":"0"},{"Human":"1"},{"trespassing":"1"},{"label":"1"},{"is":"1"},{"assigned":"1"},{"to":"1"},{"positive":"1"},{"(":"1"},{"1":"1"},{")":"1"},{"class":"1"},{"and":"0"},{".":"0"},{"“":"0"},{"other":"1"},{"activity":"1"},{"”":"1"},{"label":"1"},{"is":"1"},{"assigned":"1"},{"to":"1"},{"negative":"1"},{"class":"1"},{"(":"1"},{"0":"1"},{")":"1"}],"subSimilaritySentenceSection":"โดย M Bashir 2019 อ้างโดย5—sponding frame fi.Human trespassing label is assigned to positive(1)class and.“other activity”label is assigned to negative class(0).","duplicateSourceMD5":"81575F3F5C86038F313FD929210D1FC6"},{"score":0.6085586,"similaritySentence":"label set Y is assigned to the positive class(+1)if y 2 Y otherwise it is assigned to the negative class(—1).","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"The":"1"},{"human":"0"},{"trespassing":"0"},{"label":"1"},{"is":"1"},{"assigned":"1"},{"to":"1"},{"the":"1"},{"positive":"1"},{"class":"1"},{"(":"1"},{"1":"1"},{")":"0"},{"while":"0"},{"the":"1"},{"“":"0"},{"other":"0"},{"activity":"0"},{"”":"0"},{"label":"1"},{"is":"1"},{"assigned":"1"},{"to":"1"},{"the":"1"},{"negative":"1"},{"class":"1"},{"(":"0"},{"0":"0"},{")":"0"},{".":"0"}],"similaritySegGreyList":[{"label":"1"},{"set":"0"},{"Y":"0"},{"is":"1"},{"assigned":"1"},{"to":"1"},{"the":"1"},{"positive":"1"},{"class":"1"},{"(":"1"},{"+":"1"},{"1":"1"},{")":"0"},{"if":"0"},{"y":"0"},{"2":"0"},{"Y":"0"},{"otherwise":"0"},{"it":"0"},{"is":"1"},{"assigned":"1"},{"to":"1"},{"the":"1"},{"negative":"1"},{"class":"1"},{"(":"1"},{"—":"1"},{"1":"1"},{")":"1"},{".":"1"}],"subSimilaritySentenceSection":"instance x，associated with a label set Y is assigned to the positive class(+1)if y 2 Y otherwise it is assigned to the negative class(—1). Similar to our new proposed algorithm","duplicateSourceMD5":"30BD3245F699A76DE877690565C9B4F7"},{"score":0.52987987,"similaritySentence":"(x)\u003e0 then x is labeled as 1(the positive class)，otherwise it is labeled as-1(the negative class).","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"The":"1"},{"human":"0"},{"trespassing":"0"},{"label":"1"},{"is":"1"},{"assigned":"0"},{"to":"0"},{"the":"1"},{"positive":"1"},{"class":"1"},{"(":"1"},{"1":"1"},{")":"0"},{"while":"0"},{"the":"1"},{"“":"0"},{"other":"0"},{"activity":"0"},{"”":"0"},{"label":"1"},{"is":"1"},{"assigned":"0"},{"to":"0"},{"the":"1"},{"negative":"1"},{"class":"1"},{"(":"1"},{"0":"1"},{")":"1"},{".":"1"}],"similaritySegGreyList":[{"(":"0"},{"x":"0"},{")":"0"},{"\u003e":"0"},{"0":"1"},{"then":"0"},{"x":"0"},{"is":"1"},{"labeled":"1"},{"as":"0"},{"1":"1"},{"(":"1"},{"the":"1"},{"positive":"1"},{"class":"1"},{")":"0"},{"，":"0"},{"otherwise":"0"},{"it":"0"},{"is":"1"},{"labeled":"1"},{"as":"0"},{"-":"0"},{"1":"1"},{"(":"1"},{"the":"1"},{"negative":"1"},{"class":"1"},{")":"1"},{".":"1"}],"subSimilaritySentenceSection":"operator)such that if f (x)\u003e0 then x is labeled as 1(the positive class)，otherwise it is labeled as-1(the negative class). Let us further Feature 1 FIGURE","duplicateSourceMD5":"4473495DAACEC585D30FCC87E8AF8DFB"}]},{"content":"Since each prediction depends only on the corresponding frame fi，our problem corresponds to determining a function D with parameter θ such that:","segRedList":[],"synonymsContent":"","result":[]},{"content":"[1]","segRedList":[],"synonymsContent":"","result":[]},{"content":"The aim is to find a θ ∗ such that D(fi;","segRedList":["that","such","find","aim","is","fi","to","a"],"synonymsContent":"","result":[{"score":0.6626983,"similaritySentence":"Our aim is to find a p(fi)such that p fi","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"The":"0"},{"aim":"1"},{"is":"1"},{"to":"1"},{"find":"1"},{"a":"1"},{"θ":"0"},{"∗":"0"},{"such":"1"},{"that":"1"},{"D":"0"},{"(":"0"},{"fi":"1"},{";":"1"}],"similaritySegGreyList":[{"Our":"0"},{"aim":"1"},{"is":"1"},{"to":"1"},{"find":"1"},{"a":"1"},{"p":"0"},{"(":"0"},{"fi":"1"},{")":"1"},{"such":"1"},{"that":"1"},{"p":"0"},{"fi":"1"}],"subSimilaritySentenceSection":")/?\u003d pi. Our aim is to find a p(fi)such that p fi ~p(fi)is","duplicateSourceMD5":"5C7AD8DC2D3D1CBC32769FAB52BEEBDB"},{"score":0.6597542,"similaritySentence":"B，the aim is to find a T c S such that the","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"The":"1"},{"aim":"1"},{"is":"1"},{"to":"1"},{"find":"1"},{"a":"1"},{"θ":"0"},{"∗":"0"},{"such":"1"},{"that":"1"},{"D":"0"},{"(":"0"},{"fi":"0"},{";":"0"}],"similaritySegGreyList":[{"B":"0"},{"，":"0"},{"the":"1"},{"aim":"1"},{"is":"1"},{"to":"1"},{"find":"1"},{"a":"1"},{"T":"0"},{"c":"0"},{"S":"0"},{"such":"1"},{"that":"1"},{"the":"1"}],"subSimilaritySentenceSection":"Si).Given a budget B，the aim is to find a T c S such that the overall cost of sets in T","duplicateSourceMD5":"17388EC6C779BBBFF45023FBEF989FFF"},{"score":0.5973389,"similaritySentence":"t.Our aim is thus to find a，fl such that the","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"The":"1"},{"aim":"1"},{"is":"1"},{"to":"1"},{"find":"1"},{"a":"1"},{"θ":"0"},{"∗":"0"},{"such":"1"},{"that":"1"},{"D":"0"},{"(":"0"},{"fi":"0"},{";":"0"}],"similaritySegGreyList":[{"t":"0"},{".":"0"},{"Our":"0"},{"aim":"1"},{"is":"1"},{"thus":"0"},{"to":"1"},{"find":"1"},{"a":"1"},{"，":"0"},{"fl":"0"},{"such":"1"},{"that":"1"},{"the":"1"}],"subSimilaritySentenceSection":"fi)space for any time t.Our aim is thus to find a，fl such that the solution m(a，fl","duplicateSourceMD5":"F7ACCEC143A7500D936B409F059EFD9A"},{"score":0.5522273,"similaritySentence":".our aim is to find a solution y e L2(fi)","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"The":"0"},{"aim":"1"},{"is":"1"},{"to":"1"},{"find":"1"},{"a":"1"},{"θ":"0"},{"∗":"0"},{"such":"0"},{"that":"0"},{"D":"0"},{"(":"0"},{"fi":"1"},{";":"1"}],"similaritySegGreyList":[{".":"0"},{"our":"0"},{"aim":"1"},{"is":"1"},{"to":"1"},{"find":"1"},{"a":"1"},{"solution":"0"},{"y":"0"},{"e":"0"},{"L2":"0"},{"(":"0"},{"fi":"1"},{")":"1"}],"subSimilaritySentenceSection":"\u003d 0，i.e .our aim is to find a solution y e L2(fi) of the linear integral equation P","duplicateSourceMD5":"64AF19B3357AF7FE97D2C32607B25A08"},{"score":0.5,"similaritySentence":"the aim is to find vertical force u e Uad \u003d L2(fi","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"The":"1"},{"aim":"1"},{"is":"1"},{"to":"1"},{"find":"1"},{"a":"0"},{"θ":"0"},{"∗":"0"},{"such":"0"},{"that":"0"},{"D":"0"},{"(":"0"},{"fi":"1"},{";":"1"}],"similaritySegGreyList":[{"the":"1"},{"aim":"1"},{"is":"1"},{"to":"1"},{"find":"1"},{"vertical":"0"},{"force":"0"},{"u":"0"},{"e":"0"},{"Uad":"0"},{"\u003d":"0"},{"L2":"0"},{"(":"0"},{"fi":"1"}],"subSimilaritySentenceSection":"In the OC-OP， the aim is to find vertical force u e Uad \u003d L2(fi )as the control function and","duplicateSourceMD5":"DEFC40C472F4F52D0EAA4A645ECF4DA8"}]},{"content":"θ ∗)yi where yi is the ground truth label corresponding to fi.","segRedList":["ground","truth","label","where","the","yi","is"],"synonymsContent":"","result":[{"score":0.6444444,"similaritySentence":"i \u003d j\u003d 1 where yi is the ground truth label(one-hot","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"θ":"0"},{"∗":"0"},{")":"0"},{"yi":"1"},{"where":"1"},{"yi":"1"},{"is":"1"},{"the":"1"},{"ground":"1"},{"truth":"1"},{"label":"1"},{"corresponding":"0"},{"to":"0"},{"fi":"0"},{".":"0"}],"similaritySegGreyList":[{"i":"0"},{"\u003d":"0"},{"j\u003d":"0"},{"1":"0"},{"where":"1"},{"yi":"1"},{"is":"1"},{"the":"1"},{"ground":"1"},{"truth":"1"},{"label":"1"},{"(":"0"},{"one":"0"},{"-":"0"},{"hot":"0"}],"subSimilaritySentenceSection":"Pij)，(1) i \u003d j\u003d 1 where yi is the ground truth label(one-hot vector)of representation ri，","duplicateSourceMD5":"80563C4168C32E94D2FFA4E220BC6CE9"},{"score":0.62083334,"similaritySentence":"i\u003cm}，where Yi is the ground truth labels and Yi is","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"θ":"0"},{"∗":"0"},{")":"0"},{"yi":"1"},{"where":"1"},{"yi":"1"},{"is":"1"},{"the":"1"},{"ground":"1"},{"truth":"1"},{"label":"1"},{"corresponding":"0"},{"to":"0"},{"fi":"0"},{".":"0"}],"similaritySegGreyList":[{"i":"0"},{"\u003c":"0"},{"m":"0"},{"}":"0"},{"，":"0"},{"where":"1"},{"Yi":"1"},{"is":"1"},{"the":"1"},{"ground":"1"},{"truth":"1"},{"labels":"1"},{"and":"0"},{"Yi":"1"},{"is":"1"}],"subSimilaritySentenceSection":"xi，Yi)1\u003c i\u003cm}，where Yi is the ground truth labels and Yi is its predicted labels.These performance","duplicateSourceMD5":"BB4CB240152AC37AF7259A7FAD1CCACA"},{"score":0.5576389,"similaritySentence":"number，yi is the ground truth label(i.e.0 or","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"θ":"0"},{"∗":"0"},{")":"0"},{"yi":"1"},{"where":"0"},{"yi":"1"},{"is":"1"},{"the":"1"},{"ground":"1"},{"truth":"1"},{"label":"1"},{"corresponding":"0"},{"to":"0"},{"fi":"0"},{".":"0"}],"similaritySegGreyList":[{"number":"0"},{"，":"0"},{"yi":"1"},{"is":"1"},{"the":"1"},{"ground":"1"},{"truth":"1"},{"label":"1"},{"(":"0"},{"i":"0"},{".":"0"},{"e":"0"},{".":"0"},{"0":"0"},{"or":"0"}],"subSimilaritySentenceSection":"Pi where N is the pixel number，yi is the ground truth label(i.e.0 or 1)andp¡ is the predicted","duplicateSourceMD5":"C5889DF9862840791D7B80BD4658DD5F"},{"score":0.5465278,"similaritySentence":"..，yn be the ground truth labels(scores)and f1，","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"θ":"0"},{"∗":"0"},{")":"0"},{"yi":"0"},{"where":"0"},{"yi":"0"},{"is":"1"},{"the":"1"},{"ground":"1"},{"truth":"1"},{"label":"1"},{"corresponding":"0"},{"to":"0"},{"fi":"0"},{".":"0"}],"similaritySegGreyList":[{".":"0"},{".":"0"},{"，":"0"},{"yn":"0"},{"be":"1"},{"the":"1"},{"ground":"1"},{"truth":"1"},{"labels":"1"},{"(":"0"},{"scores":"0"},{")":"0"},{"and":"0"},{"f1":"0"},{"，":"0"}],"subSimilaritySentenceSection":"Let y1，y2，. ..，yn be the ground truth labels(scores)and f1， f2，...，","duplicateSourceMD5":"EADFA54C5751AB379DC851BB1766AD94"},{"score":0.5402777,"similaritySentence":"yi，yi(w))where yi is the ground truth output of","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"θ":"0"},{"∗":"0"},{")":"0"},{"yi":"1"},{"where":"1"},{"yi":"1"},{"is":"1"},{"the":"1"},{"ground":"1"},{"truth":"1"},{"label":"0"},{"corresponding":"0"},{"to":"0"},{"fi":"0"},{".":"0"}],"similaritySegGreyList":[{"yi":"1"},{"，":"1"},{"yi":"1"},{"(":"0"},{"w":"0"},{")":"0"},{")":"0"},{"where":"1"},{"yi":"1"},{"is":"1"},{"the":"1"},{"ground":"1"},{"truth":"1"},{"output":"0"},{"of":"0"}],"subSimilaritySentenceSection":"bound on the loss A( yi，yi(w))where yi is the ground truth output of training example i and yi(","duplicateSourceMD5":"730B192D39127B9D7F11DFA42DDFA065"},{"score":0.5402777,"similaritySentence":"Z 2 IR(Qx-is the ground truth labels.The ground truth","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"θ":"0"},{"∗":"0"},{")":"0"},{"yi":"0"},{"where":"0"},{"yi":"0"},{"is":"1"},{"the":"1"},{"ground":"1"},{"truth":"1"},{"label":"1"},{"corresponding":"0"},{"to":"0"},{"fi":"0"},{".":"0"}],"similaritySegGreyList":[{"Z":"0"},{"2":"0"},{"IR":"0"},{"(":"0"},{"Qx":"0"},{"-":"0"},{"is":"1"},{"the":"1"},{"ground":"1"},{"truth":"1"},{"labels":"1"},{".":"1"},{"The":"1"},{"ground":"1"},{"truth":"1"}],"subSimilaritySentenceSection":"[40])，where Z 2 IR(Qx-is the ground truth labels.The ground truth labels are represented as(1","duplicateSourceMD5":"FF019B5C890A2F60C33059710E1E97A7"},{"score":0.52916664,"similaritySentence":"c \u003d 1 where yi is the ground-truth class of pixel i，","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"θ":"0"},{"∗":"0"},{")":"0"},{"yi":"1"},{"where":"1"},{"yi":"1"},{"is":"1"},{"the":"1"},{"ground":"1"},{"truth":"1"},{"label":"0"},{"corresponding":"0"},{"to":"0"},{"fi":"0"},{".":"0"}],"similaritySegGreyList":[{"c":"0"},{"\u003d":"0"},{"1":"0"},{"where":"1"},{"yi":"1"},{"is":"1"},{"the":"1"},{"ground":"1"},{"-":"1"},{"truth":"1"},{"class":"0"},{"of":"0"},{"pixel":"0"},{"i":"0"},{"，":"0"}],"subSimilaritySentenceSection":"0 X w(2) c \u003d 1 where yi is the ground-truth class of pixel i， wc is the weight for class","duplicateSourceMD5":"5B5BBD59300E6A366CF83A5913B8050D"},{"score":0.52777773,"similaritySentence":"Nt7 \u003d i where yi is the ground truth weight for the test image i","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"θ":"0"},{"∗":"0"},{")":"0"},{"yi":"1"},{"where":"1"},{"yi":"1"},{"is":"1"},{"the":"1"},{"ground":"1"},{"truth":"1"},{"label":"0"},{"corresponding":"0"},{"to":"0"},{"fi":"0"},{".":"0"}],"similaritySegGreyList":[{"Nt7":"0"},{"\u003d":"0"},{"i":"0"},{"where":"1"},{"yi":"1"},{"is":"1"},{"the":"1"},{"ground":"1"},{"truth":"1"},{"weight":"0"},{"for":"0"},{"the":"1"},{"test":"0"},{"image":"0"},{"i":"0"}],"subSimilaritySentenceSection":"yi-y^!， Nt7 \u003d i where yi is the ground truth weight for the test image i ，yi is the corresponding estimated","duplicateSourceMD5":"5E148AEE239986143487F8C7E8DDFE54"},{"score":0.5128655,"similaritySentence":"where as the last ground-truth(right)has labels corresponding to the","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"θ":"0"},{"∗":"0"},{")":"0"},{"yi":"0"},{"where":"1"},{"yi":"0"},{"is":"0"},{"the":"1"},{"ground":"1"},{"truth":"1"},{"label":"1"},{"corresponding":"1"},{"to":"1"},{"fi":"0"},{".":"0"}],"similaritySegGreyList":[{"where":"1"},{"as":"0"},{"the":"1"},{"last":"0"},{"ground":"1"},{"-":"1"},{"truth":"1"},{"(":"0"},{"right":"0"},{")":"0"},{"has":"0"},{"labels":"1"},{"corresponding":"1"},{"to":"1"},{"the":"1"}],"subSimilaritySentenceSection":"corresponding to the smallest heads， where as the last ground-truth(right)has labels corresponding to the largest heads.These maps(","duplicateSourceMD5":"BEDACA567D988042C753398974B4FD8F"},{"score":0.5118056,"similaritySentence":"yi e{0，1}9 is the ground truth label of the","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"θ":"0"},{"∗":"0"},{")":"0"},{"yi":"1"},{"where":"0"},{"yi":"1"},{"is":"1"},{"the":"1"},{"ground":"1"},{"truth":"1"},{"label":"1"},{"corresponding":"0"},{"to":"0"},{"fi":"0"},{".":"0"}],"similaritySegGreyList":[{"yi":"1"},{"e":"0"},{"{":"0"},{"0":"0"},{"，":"0"},{"1":"0"},{"}":"0"},{"9":"0"},{"is":"1"},{"the":"1"},{"ground":"1"},{"truth":"1"},{"label":"1"},{"of":"0"},{"the":"1"}],"subSimilaritySentenceSection":"i\u003cm}，where yi e{0，1}9 is the ground truth label of the ith test example.Let yi","duplicateSourceMD5":"F40E325829C905255391F4ABB0771F68"},{"score":0.5118056,"similaritySentence":";-).lt is the ground truth label of pixel i.\u003c","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"θ":"0"},{"∗":"0"},{")":"0"},{"yi":"0"},{"where":"0"},{"yi":"0"},{"is":"1"},{"the":"1"},{"ground":"1"},{"truth":"1"},{"label":"1"},{"corresponding":"0"},{"to":"0"},{"fi":"0"},{".":"0"}],"similaritySegGreyList":[{";":"0"},{"-":"0"},{")":"0"},{".":"0"},{"lt":"0"},{"is":"1"},{"the":"1"},{"ground":"1"},{"truth":"1"},{"label":"1"},{"of":"0"},{"pixel":"0"},{"i":"0"},{".":"0"},{"\u003c":"0"}],"subSimilaritySentenceSection":"logp(y;-x ;-).lt is the ground truth label of pixel i.\u003c Pij(Xi，xj)","duplicateSourceMD5":"9E0BBBF6E6D3F4CB2DD4EFFC7807CFB6"},{"score":0.50294787,"similaritySentence":")is the mean，c are the ground-truth labels across all persons","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"θ":"0"},{"∗":"0"},{")":"0"},{"yi":"0"},{"where":"0"},{"yi":"0"},{"is":"1"},{"the":"1"},{"ground":"1"},{"truth":"1"},{"label":"1"},{"corresponding":"0"},{"to":"0"},{"fi":"0"},{".":"0"}],"similaritySegGreyList":[{")":"1"},{"is":"1"},{"the":"1"},{"mean":"0"},{"，":"0"},{"c":"0"},{"are":"1"},{"the":"1"},{"ground":"1"},{"-":"1"},{"truth":"1"},{"labels":"1"},{"across":"0"},{"all":"0"},{"persons":"0"}],"subSimilaritySentenceSection":"))where E(. )is the mean，c are the ground-truth labels across all persons and videos concatenated into a single","duplicateSourceMD5":"50EE27310879A5FDA786409A72864E21"}]},{"content":"The ground truth label has the following definition[1]:","segRedList":[],"synonymsContent":"","result":[]},{"content":"[1]","segRedList":[],"synonymsContent":"","result":[]},{"content":"5.","segRedList":[],"synonymsContent":"","result":[]},{"content":"Advantages:","segRedList":[],"synonymsContent":"","result":[]},{"content":"-","segRedList":[],"synonymsContent":"","result":[]},{"content":"A real-time ability can be gained to help avoid risks related to railway trespassing.","segRedList":[],"synonymsContent":"","result":[]},{"content":"Many such automated systems in the field can be integrated，including maintenance，security，traffic and passenger models，to form actions that consider multiple aspects.","segRedList":[],"synonymsContent":"","result":[]},{"content":"Improvements and experience can be integrated into the future learning process and automated effectively via machine learning，which is critical for safety systems.","segRedList":[],"synonymsContent":"","result":[]},{"content":"","segRedList":[],"synonymsContent":"","result":[]},{"content":"The effectiveness of safety operations in stations and other areas linked to railway activities can be improved.","segRedList":[],"synonymsContent":"","result":[]},{"content":"","segRedList":[],"synonymsContent":"","result":[]},{"content":"Time and costs can be saved while improving accuracy to enable long-term quality improvements.","segRedList":[],"synonymsContent":"","result":[]},{"content":"Both passenger and workforce experiences can be improved，which reflect on the overall market image.","segRedList":[],"synonymsContent":"","result":[]},{"content":"","segRedList":[],"synonymsContent":"","result":[]},{"content":"Data gathering can be enhanced to more fully utilise effective connections between assets and people.","segRedList":[],"synonymsContent":"","result":[]},{"content":"With the help of this system the delay in railway can be avoided caused by deaths on tracks.","segRedList":[],"synonymsContent":"","result":[]},{"content":"It helps in establishing a better crowd management trespassing detection system.","segRedList":[],"synonymsContent":"","result":[]},{"content":"","segRedList":[],"synonymsContent":"","result":[]},{"content":"6.","segRedList":[],"synonymsContent":"","result":[]},{"content":"Disadvantages:","segRedList":[],"synonymsContent":"","result":[]},{"content":"-","segRedList":[],"synonymsContent":"","result":[]},{"content":"Some issues might occur in the future due to the large volume of data which affects the flow and speed of detecting desired outcomes.","segRedList":[],"synonymsContent":"","result":[]},{"content":"Installation and maintenance of this system at a large scale is expensive.","segRedList":["Installation","installation","maintenance","This","this","and","is","an","a"],"synonymsContent":"","result":[{"score":0.5923891,"similaritySentence":"installation and maintenance.This is more than an event:it is","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Installation":"1"},{"and":"1"},{"maintenance":"1"},{"of":"0"},{"this":"1"},{"system":"0"},{"at":"0"},{"a":"1"},{"large":"0"},{"scale":"0"},{"is":"1"},{"expensive":"0"},{".":"0"}],"similaritySegGreyList":[{"installation":"1"},{"and":"1"},{"maintenance":"1"},{".":"1"},{"This":"1"},{"is":"1"},{"more":"0"},{"than":"0"},{"an":"1"},{"event":"0"},{":":"0"},{"it":"0"},{"is":"1"}],"subSimilaritySentenceSection":"The ®nal element is achieving quality installation and maintenance.This is more than an event:it is a process requiring development of training","duplicateSourceMD5":"68550DEF7B906DF2C754BA022F5EC04E"},{"score":0.5923891,"similaritySentence":"installation and maintenance.This is more than an event:it is","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"Installation":"1"},{"and":"1"},{"maintenance":"1"},{"of":"0"},{"this":"1"},{"system":"0"},{"at":"0"},{"a":"1"},{"large":"0"},{"scale":"0"},{"is":"1"},{"expensive":"0"},{".":"0"}],"similaritySegGreyList":[{"installation":"1"},{"and":"1"},{"maintenance":"1"},{".":"1"},{"This":"1"},{"is":"1"},{"more":"0"},{"than":"0"},{"an":"1"},{"event":"0"},{":":"0"},{"it":"0"},{"is":"1"}],"subSimilaritySentenceSection":"The ®nal element is achieving quality installation and maintenance.This is more than an event:it is a process requiring development of training","duplicateSourceMD5":"20CA9A411DD0F27E0318CF2D76FCEE98"},{"score":0.56988084,"similaritySentence":"installation and maintenance requirements，the method is the least expensive of all","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Installation":"1"},{"and":"1"},{"maintenance":"1"},{"of":"1"},{"this":"0"},{"system":"0"},{"at":"0"},{"a":"0"},{"large":"0"},{"scale":"0"},{"is":"1"},{"expensive":"1"},{".":"1"}],"similaritySegGreyList":[{"installation":"1"},{"and":"1"},{"maintenance":"1"},{"requirements":"0"},{"，":"0"},{"the":"0"},{"method":"0"},{"is":"1"},{"the":"0"},{"least":"0"},{"expensive":"1"},{"of":"1"},{"all":"0"}],"subSimilaritySentenceSection":"sludge.Due to the low installation and maintenance requirements，the method is the least expensive of all .Solar drying is affected by","duplicateSourceMD5":"1595FC2CF620F57934C0083DB6EF81A1"},{"score":0.5531135,"similaritySentence":"installation and maintenance.This enclosure is 3.0m long，1","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"Installation":"1"},{"and":"1"},{"maintenance":"1"},{"of":"0"},{"this":"1"},{"system":"0"},{"at":"0"},{"a":"0"},{"large":"0"},{"scale":"0"},{"is":"1"},{"expensive":"0"},{".":"0"}],"similaritySegGreyList":[{"installation":"1"},{"and":"1"},{"maintenance":"1"},{".":"1"},{"This":"1"},{"enclosure":"0"},{"is":"1"},{"3":"0"},{".":"0"},{"0m":"0"},{"long":"0"},{"，":"0"},{"1":"0"}],"subSimilaritySentenceSection":"solar hot water，to simplify installation and maintenance.This enclosure is 3.0m long，1 .2m wide，and 0","duplicateSourceMD5":"B45D0806BB621A0518579A08E060A1AD"},{"score":0.5314857,"similaritySentence":"installation and maintenance is now the most expensive part of a submarine cable","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"Installation":"1"},{"and":"1"},{"maintenance":"1"},{"of":"1"},{"this":"0"},{"system":"0"},{"at":"0"},{"a":"1"},{"large":"0"},{"scale":"0"},{"is":"1"},{"expensive":"1"},{".":"1"}],"similaritySegGreyList":[{"installation":"1"},{"and":"1"},{"maintenance":"1"},{"is":"1"},{"now":"0"},{"the":"0"},{"most":"0"},{"expensive":"1"},{"part":"0"},{"of":"1"},{"a":"1"},{"submarine":"0"},{"cable":"0"}],"subSimilaritySentenceSection":"-term submarine observatories.Cable installation and maintenance is now the most expensive part of a submarine cable system.To reduce costs，","duplicateSourceMD5":"9281DE49CA8B22C1D6B37AFB72C832D6"},{"score":0.51611227,"similaritySentence":"，installation and subsequent maintenance of these low head systems is labour intensive","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"Installation":"1"},{"and":"1"},{"maintenance":"1"},{"of":"1"},{"this":"0"},{"system":"1"},{"at":"0"},{"a":"0"},{"large":"0"},{"scale":"0"},{"is":"1"},{"expensive":"0"},{".":"0"}],"similaritySegGreyList":[{"，":"1"},{"installation":"1"},{"and":"1"},{"subsequent":"0"},{"maintenance":"1"},{"of":"1"},{"these":"0"},{"low":"0"},{"head":"0"},{"systems":"1"},{"is":"1"},{"labour":"0"},{"intensive":"0"}],"subSimilaritySentenceSection":").In practice the design ，installation and subsequent maintenance of these low head systems is labour intensive and requires a thorough understanding of","duplicateSourceMD5":"85A5BF096543A5D8BE839CC499B5DC02"},{"score":0.506216,"similaritySentence":"proper operation and maintenance of the systems.Its current major business is","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Installation":"0"},{"and":"1"},{"maintenance":"1"},{"of":"1"},{"this":"0"},{"system":"1"},{"at":"0"},{"a":"0"},{"large":"0"},{"scale":"0"},{"is":"1"},{"expensive":"0"},{".":"0"}],"similaritySegGreyList":[{"proper":"0"},{"operation":"0"},{"and":"1"},{"maintenance":"1"},{"of":"1"},{"the":"0"},{"systems":"1"},{".":"0"},{"Its":"0"},{"current":"0"},{"major":"0"},{"business":"0"},{"is":"1"}],"subSimilaritySentenceSection":"，end users etc for the proper operation and maintenance of the systems.Its current major business is described below:Figure 3.","duplicateSourceMD5":"55931C6BF62977F187E361A51D6C9D99"},{"score":0.5002035,"similaritySentence":"installation and maintenance are important.That said，the main issues for","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Installation":"1"},{"and":"1"},{"maintenance":"1"},{"of":"0"},{"this":"0"},{"system":"0"},{"at":"0"},{"a":"0"},{"large":"0"},{"scale":"0"},{"is":"1"},{"expensive":"0"},{".":"0"}],"similaritySegGreyList":[{"installation":"1"},{"and":"1"},{"maintenance":"1"},{"are":"1"},{"important":"0"},{".":"0"},{"That":"0"},{"said":"0"},{"，":"0"},{"the":"0"},{"main":"0"},{"issues":"0"},{"for":"0"}],"subSimilaritySentenceSection":"grade specification)and simplicity of installation and maintenance are important.That said，the main issues for the sustainability of these systems are","duplicateSourceMD5":"97B102B7FE6830CFCEABD874732B7D70"}]},{"content":"","segRedList":[],"synonymsContent":"","result":[]},{"content":"Effectiveness of a Public Security Camera Is Doubted sometimes","segRedList":[],"synonymsContent":"","result":[]},{"content":"7.","segRedList":[],"synonymsContent":"","result":[]},{"content":"Conclusion:","segRedList":[],"synonymsContent":"","result":[]},{"content":"-","segRedList":[],"synonymsContent":"","result":[]},{"content":"This report explains the approach made into railway trespassing detection with a system that is capable of of executing security check with deep learning models to verify","segRedList":[],"synonymsContent":"","result":[]},{"content":"any trespassing breach and as an end result the model will generate alert signals to classify the trespassers in order to avoid any deaths.","segRedList":[],"synonymsContent":"","result":[]},{"content":"The accuracy of this model stands at 90-95%in detecting trespassing activities.","segRedList":[],"synonymsContent":"","result":[]},{"content":"This detection is totally based on frames that is created by the algorithm to detect human where the frames themselves are independent.","segRedList":[],"synonymsContent":"","result":[]},{"content":"With this approach we can gather information on trespassing activities and find and build effective solutions to avoid future trespassing activities.","segRedList":[],"synonymsContent":"","result":[]},{"content":"8.","segRedList":[],"synonymsContent":"","result":[]},{"content":"ACKNOWLEDGEMENT:","segRedList":[],"synonymsContent":"","result":[]},{"content":"-","segRedList":[],"synonymsContent":"","result":[]},{"content":"This paper along with the proposed solution would not have been possible with just a few people.","segRedList":[],"synonymsContent":"","result":[]},{"content":"To publish a research paper and to express your way understanding a problem and trying to come with a solution was inspired and was supported by a lot of people.","segRedList":[],"synonymsContent":"","result":[]},{"content":"We would like to express our sincere gratitude，deepest vote of thanks to our Project Guide Professor Mangesh Manake，","segRedList":["gratitude","sincere","express","would","like","our","We","to"],"synonymsContent":"","result":[{"score":0.53338236,"similaritySentence":"We would like to express our sincere gratitude to our Russian colleagues，foremost the palaeontologists Mrs.M.S","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"We":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"express":"1"},{"our":"1"},{"sincere":"1"},{"gratitude":"1"},{"，":"0"},{"deepest":"0"},{"vote":"0"},{"of":"0"},{"thanks":"0"},{"to":"1"},{"our":"1"},{"Project":"0"},{"Guide":"0"},{"Professor":"0"},{"Mangesh":"0"},{"Manake":"0"},{"，":"0"}],"similaritySegGreyList":[{"We":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"express":"1"},{"our":"1"},{"sincere":"1"},{"gratitude":"1"},{"to":"1"},{"our":"1"},{"Russian":"0"},{"colleagues":"0"},{"，":"0"},{"foremost":"0"},{"the":"0"},{"palaeontologists":"0"},{"Mrs":"0"},{".":"0"},{"M":"0"},{".":"0"},{"S":"0"}],"subSimilaritySentenceSection":"Naterstad were of invaluable assistance. We would like to express our sincere gratitude to our Russian colleagues，foremost the palaeontologists Mrs.M.S .Raaben，Drs.LN","duplicateSourceMD5":"D92EAED0BF9F6489ADAE1C68252A6BFD"},{"score":0.51620024,"similaritySentence":"we would like to express my sincere gratitude towards them.First and foremost，we owe our heartfelt thanks to","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"We":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"express":"1"},{"our":"1"},{"sincere":"1"},{"gratitude":"1"},{"，":"0"},{"deepest":"0"},{"vote":"0"},{"of":"0"},{"thanks":"1"},{"to":"1"},{"our":"1"},{"Project":"0"},{"Guide":"0"},{"Professor":"0"},{"Mangesh":"0"},{"Manake":"0"},{"，":"0"}],"similaritySegGreyList":[{"we":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"express":"1"},{"my":"0"},{"sincere":"1"},{"gratitude":"1"},{"towards":"0"},{"them":"0"},{".":"0"},{"First":"0"},{"and":"0"},{"foremost":"0"},{"，":"0"},{"we":"1"},{"owe":"0"},{"our":"1"},{"heartfelt":"0"},{"thanks":"1"},{"to":"1"}],"subSimilaritySentenceSection":"never be completed，thus here we would like to express my sincere gratitude towards them.First and foremost，we owe our heartfelt thanks to my distinguished and cordial supervisor，","duplicateSourceMD5":"BEB4E8F7F065E1F1FFEFB0D1BECACB22"},{"score":0.50891834,"similaritySentence":".We would like to express our sincere gratitude to all the TPC members and organizers for their hard work，","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"We":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"express":"1"},{"our":"1"},{"sincere":"1"},{"gratitude":"1"},{"，":"0"},{"deepest":"0"},{"vote":"0"},{"of":"0"},{"thanks":"0"},{"to":"1"},{"our":"1"},{"Project":"0"},{"Guide":"0"},{"Professor":"0"},{"Mangesh":"0"},{"Manake":"0"},{"，":"0"}],"similaritySegGreyList":[{".":"1"},{"We":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"express":"1"},{"our":"1"},{"sincere":"1"},{"gratitude":"1"},{"to":"1"},{"all":"0"},{"the":"0"},{"TPC":"0"},{"members":"0"},{"and":"0"},{"organizers":"0"},{"for":"0"},{"their":"0"},{"hard":"0"},{"work":"0"},{"，":"0"}],"subSimilaritySentenceSection":"platform for academic connection and exchange .We would like to express our sincere gratitude to all the TPC members and organizers for their hard work， precious time and endeavor preparing for","duplicateSourceMD5":"4A5C3A7834272880C5F0DC4294BD25E7"},{"score":0.50891834,"similaritySentence":".We would like to express our sincere gratitude to all the TPC members and organizers for their hard work，","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"We":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"express":"1"},{"our":"1"},{"sincere":"1"},{"gratitude":"1"},{"，":"0"},{"deepest":"0"},{"vote":"0"},{"of":"0"},{"thanks":"0"},{"to":"1"},{"our":"1"},{"Project":"0"},{"Guide":"0"},{"Professor":"0"},{"Mangesh":"0"},{"Manake":"0"},{"，":"0"}],"similaritySegGreyList":[{".":"1"},{"We":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"express":"1"},{"our":"1"},{"sincere":"1"},{"gratitude":"1"},{"to":"1"},{"all":"0"},{"the":"0"},{"TPC":"0"},{"members":"0"},{"and":"0"},{"organizers":"0"},{"for":"0"},{"their":"0"},{"hard":"0"},{"work":"0"},{"，":"0"}],"subSimilaritySentenceSection":"platform for academic connection and exchange .We would like to express our sincere gratitude to all the TPC members and organizers for their hard work， precious time and endeavor preparing for","duplicateSourceMD5":"B7D4D1742E0163A2A6DB244DBA56AE81"},{"score":0.50891834,"similaritySentence":".We would like to express our sincere gratitude to all the TPC members and organizers for their hard work，","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"We":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"express":"1"},{"our":"1"},{"sincere":"1"},{"gratitude":"1"},{"，":"0"},{"deepest":"0"},{"vote":"0"},{"of":"0"},{"thanks":"0"},{"to":"1"},{"our":"1"},{"Project":"0"},{"Guide":"0"},{"Professor":"0"},{"Mangesh":"0"},{"Manake":"0"},{"，":"0"}],"similaritySegGreyList":[{".":"1"},{"We":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"express":"1"},{"our":"1"},{"sincere":"1"},{"gratitude":"1"},{"to":"1"},{"all":"0"},{"the":"0"},{"TPC":"0"},{"members":"0"},{"and":"0"},{"organizers":"0"},{"for":"0"},{"their":"0"},{"hard":"0"},{"work":"0"},{"，":"0"}],"subSimilaritySentenceSection":"Journal of Physics:Conference Series .We would like to express our sincere gratitude to all the TPC members and organizers for their hard work， precious time and endeavor preparing for","duplicateSourceMD5":"391CBD3326DB72B92BE226C66D1B1E7F"},{"score":0.50891834,"similaritySentence":".We would like to express our sincere gratitude to all the TPC members and organizers for their hard work，","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"We":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"express":"1"},{"our":"1"},{"sincere":"1"},{"gratitude":"1"},{"，":"0"},{"deepest":"0"},{"vote":"0"},{"of":"0"},{"thanks":"0"},{"to":"1"},{"our":"1"},{"Project":"0"},{"Guide":"0"},{"Professor":"0"},{"Mangesh":"0"},{"Manake":"0"},{"，":"0"}],"similaritySegGreyList":[{".":"1"},{"We":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"express":"1"},{"our":"1"},{"sincere":"1"},{"gratitude":"1"},{"to":"1"},{"all":"0"},{"the":"0"},{"TPC":"0"},{"members":"0"},{"and":"0"},{"organizers":"0"},{"for":"0"},{"their":"0"},{"hard":"0"},{"work":"0"},{"，":"0"}],"subSimilaritySentenceSection":"in a volume by EDP Sciences .We would like to express our sincere gratitude to all the TPC members and organizers for their hard work， precious time and endeavor preparing for","duplicateSourceMD5":"115DD2DD284A8CC90C6A4FED01E751E1"},{"score":0.50891834,"similaritySentence":".We would like to express our sincere gratitude to all supporters starting with our honorable keynote speakers:Prof.","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"We":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"express":"1"},{"our":"1"},{"sincere":"1"},{"gratitude":"1"},{"，":"0"},{"deepest":"0"},{"vote":"0"},{"of":"0"},{"thanks":"0"},{"to":"1"},{"our":"1"},{"Project":"0"},{"Guide":"0"},{"Professor":"0"},{"Mangesh":"0"},{"Manake":"0"},{"，":"0"}],"similaritySegGreyList":[{".":"1"},{"We":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"express":"1"},{"our":"1"},{"sincere":"1"},{"gratitude":"1"},{"to":"1"},{"all":"0"},{"supporters":"0"},{"starting":"0"},{"with":"0"},{"our":"1"},{"honorable":"0"},{"keynote":"0"},{"speakers":"0"},{":":"0"},{"Prof":"0"},{".":"0"}],"subSimilaritySentenceSection":"research works both international and interdisciplinary .We would like to express our sincere gratitude to all supporters starting with our honorable keynote speakers:Prof. Hiroshi Matsuno，Prof.Ramjee","duplicateSourceMD5":"11FEA0A4E8BAA10E639A39312DED5E0F"},{"score":0.50826377,"similaritySentence":".We would like to express our sincere gratitude to the members of the SETH scientific committee for their work to","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"We":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"express":"1"},{"our":"1"},{"sincere":"1"},{"gratitude":"1"},{"，":"0"},{"deepest":"0"},{"vote":"0"},{"of":"1"},{"thanks":"0"},{"to":"1"},{"our":"1"},{"Project":"0"},{"Guide":"0"},{"Professor":"0"},{"Mangesh":"0"},{"Manake":"0"},{"，":"0"}],"similaritySegGreyList":[{".":"1"},{"We":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"express":"1"},{"our":"1"},{"sincere":"1"},{"gratitude":"1"},{"to":"1"},{"the":"0"},{"members":"0"},{"of":"1"},{"the":"0"},{"SETH":"0"},{"scientific":"0"},{"committee":"0"},{"for":"0"},{"their":"0"},{"work":"0"},{"to":"1"}],"subSimilaritySentenceSection":"as many of the oral presentations .We would like to express our sincere gratitude to the members of the SETH scientific committee for their work to plan the sessions.We would","duplicateSourceMD5":"58C2C8D76CF156216B5AEDA8349E13D8"}]},{"content":"without his constant support and guidance throughout the semester this project could not have been possible.","segRedList":["possible","project","support","have","This","this","been","and","not","the"],"synonymsContent":"","result":[{"score":0.5795207,"similaritySentence":"encouragement and support，during the duration of this project.This project would not have been possible","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"without":"0"},{"his":"0"},{"constant":"0"},{"support":"1"},{"and":"1"},{"guidance":"0"},{"throughout":"0"},{"the":"1"},{"semester":"0"},{"this":"1"},{"project":"1"},{"could":"0"},{"not":"1"},{"have":"1"},{"been":"1"},{"possible":"1"},{".":"1"}],"similaritySegGreyList":[{"encouragement":"0"},{"and":"1"},{"support":"1"},{"，":"0"},{"during":"0"},{"the":"1"},{"duration":"0"},{"of":"0"},{"this":"1"},{"project":"1"},{".":"1"},{"This":"1"},{"project":"1"},{"would":"0"},{"not":"1"},{"have":"1"},{"been":"1"},{"possible":"1"}],"subSimilaritySentenceSection":"WSU-L21，for their encouragement and support，during the duration of this project.This project would not have been possible without the special attention provided by","duplicateSourceMD5":"E34D968639D7F477C9C6C607C031CEEE"}]},{"content":"Finally，we would like to extend our heartfelt gratitude to our friends and family members.","segRedList":["gratitude","heartfelt","extend","would","like","our","and","we","to"],"synonymsContent":"","result":[{"score":0.6329573,"similaritySentence":"note，we would like to extend our heartfelt gratitude to all the hard-working and","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"Finally":"0"},{"，":"0"},{"we":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"extend":"1"},{"our":"1"},{"heartfelt":"1"},{"gratitude":"1"},{"to":"1"},{"our":"1"},{"friends":"0"},{"and":"1"},{"family":"0"},{"members":"0"},{".":"0"}],"similaritySegGreyList":[{"note":"0"},{"，":"0"},{"we":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"extend":"1"},{"our":"1"},{"heartfelt":"1"},{"gratitude":"1"},{"to":"1"},{"all":"0"},{"the":"0"},{"hard":"0"},{"-":"0"},{"working":"0"},{"and":"1"}],"subSimilaritySentenceSection":"the Organizing Committee.On this note，we would like to extend our heartfelt gratitude to all the hard-working and dedicated members of the NICS19 Organising","duplicateSourceMD5":"9DC83BEFC5933DBE5DB781DA3DC99291"},{"score":0.6201276,"similaritySentence":".Finally，we would like to express our heartfelt gratitude for the generous financial support from","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Finally":"1"},{"，":"1"},{"we":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"extend":"0"},{"our":"1"},{"heartfelt":"1"},{"gratitude":"1"},{"to":"1"},{"our":"1"},{"friends":"0"},{"and":"0"},{"family":"0"},{"members":"0"},{".":"0"}],"similaritySegGreyList":[{".":"1"},{"Finally":"1"},{"，":"1"},{"we":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"express":"0"},{"our":"1"},{"heartfelt":"1"},{"gratitude":"1"},{"for":"0"},{"the":"0"},{"generous":"0"},{"financial":"0"},{"support":"0"},{"from":"0"}],"subSimilaritySentenceSection":"information is urgently needed by clinicians .Finally，we would like to express our heartfelt gratitude for the generous financial support from the organizations listed on the previous","duplicateSourceMD5":"E7192958BC1BB43D9C01AE0D1E93F9C4"},{"score":0.61012626,"similaritySentence":"article，we would like to express our heartfelt gratitude to Prof.Dr.Anne Buvé","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Finally":"0"},{"，":"0"},{"we":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"extend":"0"},{"our":"1"},{"heartfelt":"1"},{"gratitude":"1"},{"to":"1"},{"our":"1"},{"friends":"0"},{"and":"0"},{"family":"0"},{"members":"0"},{".":"0"}],"similaritySegGreyList":[{"article":"0"},{"，":"0"},{"we":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"express":"0"},{"our":"1"},{"heartfelt":"1"},{"gratitude":"1"},{"to":"1"},{"Prof":"0"},{".":"0"},{"Dr":"0"},{".":"0"},{"Anne":"0"},{"Buvé":"0"}],"subSimilaritySentenceSection":"comments on the ideas for this article，we would like to express our heartfelt gratitude to Prof.Dr.Anne Buvé and Prof.Dr.Herman","duplicateSourceMD5":"3CCABD11D4F1550656175AB89D621F5F"},{"score":0.5905184,"similaritySentence":"Finally，I ’ d like to extend my heartfelt gratitude to Dr.Furuse ’ s","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Finally":"1"},{"，":"0"},{"we":"0"},{"would":"0"},{"like":"1"},{"to":"1"},{"extend":"1"},{"our":"0"},{"heartfelt":"1"},{"gratitude":"1"},{"to":"1"},{"our":"0"},{"friends":"0"},{"and":"0"},{"family":"0"},{"members":"0"},{".":"0"}],"similaritySegGreyList":[{"Finally":"1"},{"，":"0"},{"I":"0"},{"’":"0"},{"d":"0"},{"like":"1"},{"to":"1"},{"extend":"1"},{"my":"0"},{"heartfelt":"1"},{"gratitude":"1"},{"to":"1"},{"Dr":"0"},{".":"0"},{"Furuse":"0"},{"’":"0"},{"s":"0"}],"subSimilaritySentenceSection":"wonderful achievements in your life. Finally，I ’ d like to extend my heartfelt gratitude to Dr.Furuse ’ s wife，as she supported us","duplicateSourceMD5":"F58F858AE24E2BA6959695E4B256DB30"},{"score":0.57910293,"similaritySentence":".Finally，I would like to express my heartfelt gratitude to the authorities of the Korea","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"Finally":"1"},{"，":"0"},{"we":"0"},{"would":"1"},{"like":"1"},{"to":"1"},{"extend":"0"},{"our":"0"},{"heartfelt":"1"},{"gratitude":"1"},{"to":"1"},{"our":"0"},{"friends":"0"},{"and":"0"},{"family":"0"},{"members":"0"},{".":"0"}],"similaritySegGreyList":[{".":"1"},{"Finally":"1"},{"，":"0"},{"I":"0"},{"would":"1"},{"like":"1"},{"to":"1"},{"express":"0"},{"my":"0"},{"heartfelt":"1"},{"gratitude":"1"},{"to":"1"},{"the":"0"},{"authorities":"0"},{"of":"0"},{"the":"0"},{"Korea":"0"}],"subSimilaritySentenceSection":"has supported me along the way .Finally，I would like to express my heartfelt gratitude to the authorities of the Korea Foundation for their generous support of","duplicateSourceMD5":"456B6EF4124A28942C08ACC55953ABA2"},{"score":0.5729594,"similaritySentence":".Acknowledgment We would like to extend our heartfelt gratitude to the study participants，the welfare","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Finally":"0"},{"，":"0"},{"we":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"extend":"1"},{"our":"1"},{"heartfelt":"1"},{"gratitude":"1"},{"to":"1"},{"our":"1"},{"friends":"0"},{"and":"0"},{"family":"0"},{"members":"0"},{".":"0"}],"similaritySegGreyList":[{".":"0"},{"Acknowledgment":"0"},{"We":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"extend":"1"},{"our":"1"},{"heartfelt":"1"},{"gratitude":"1"},{"to":"1"},{"the":"0"},{"study":"0"},{"participants":"0"},{"，":"0"},{"the":"0"},{"welfare":"0"}],"subSimilaritySentenceSection":"-effect relationships behind social isolation .Acknowledgment We would like to extend our heartfelt gratitude to the study participants，the welfare division of K Ward，and","duplicateSourceMD5":"ED5584410CDA35F58BF28953D55BFEFF"},{"score":0.53945774,"similaritySentence":"initiative.We also extend our heartfelt gratitude to our families，friends，and colleaguesfor their","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Finally":"0"},{"，":"0"},{"we":"1"},{"would":"0"},{"like":"0"},{"to":"1"},{"extend":"1"},{"our":"1"},{"heartfelt":"1"},{"gratitude":"1"},{"to":"1"},{"our":"1"},{"friends":"1"},{"and":"1"},{"family":"1"},{"members":"0"},{".":"0"}],"similaritySegGreyList":[{"initiative":"0"},{".":"0"},{"We":"1"},{"also":"0"},{"extend":"1"},{"our":"1"},{"heartfelt":"1"},{"gratitude":"1"},{"to":"1"},{"our":"1"},{"families":"1"},{"，":"1"},{"friends":"1"},{"，":"1"},{"and":"1"},{"colleaguesfor":"0"},{"their":"0"}],"subSimilaritySentenceSection":"hours and patience devoted to this initiative.We also extend our heartfelt gratitude to our families，friends，and colleaguesfor their support.In particular，we","duplicateSourceMD5":"965AAE85449244A970D151F582BC1FFD"},{"score":0.53431374,"similaritySentence":"this undertaking.We also extend our heartfelt gratitude to our families，friends，and colleagues","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Finally":"0"},{"，":"0"},{"we":"1"},{"would":"0"},{"like":"0"},{"to":"1"},{"extend":"1"},{"our":"1"},{"heartfelt":"1"},{"gratitude":"1"},{"to":"1"},{"our":"1"},{"friends":"1"},{"and":"1"},{"family":"1"},{"members":"0"},{".":"0"}],"similaritySegGreyList":[{"this":"0"},{"undertaking":"0"},{".":"0"},{"We":"1"},{"also":"0"},{"extend":"1"},{"our":"1"},{"heartfelt":"1"},{"gratitude":"1"},{"to":"1"},{"our":"1"},{"families":"1"},{"，":"1"},{"friends":"1"},{"，":"1"},{"and":"1"},{"colleagues":"0"}],"subSimilaritySentenceSection":"countless hours and patience devoted to this undertaking.We also extend our heartfelt gratitude to our families，friends，and colleagues for their support and valued feedback","duplicateSourceMD5":"2CC76749760CC1E1B0B22ADCC071F468"},{"score":0.5234205,"similaritySentence":"Acknowledgements We would like to express our heartfelt gratitude to countless individuals who helped us in the","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"Finally":"0"},{"，":"0"},{"we":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"extend":"0"},{"our":"1"},{"heartfelt":"1"},{"gratitude":"1"},{"to":"1"},{"our":"1"},{"friends":"0"},{"and":"0"},{"family":"0"},{"members":"0"},{".":"0"}],"similaritySegGreyList":[{"Acknowledgements":"0"},{"We":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"express":"0"},{"our":"1"},{"heartfelt":"1"},{"gratitude":"1"},{"to":"1"},{"countless":"0"},{"individuals":"0"},{"who":"0"},{"helped":"0"},{"us":"1"},{"in":"0"},{"the":"0"}],"subSimilaritySentenceSection":".This page intentionally left blank Acknowledgements We would like to express our heartfelt gratitude to countless individuals who helped us in the successful completion of this book.","duplicateSourceMD5":"848673EBB2E77B15C481740D4A6A0E7E"},{"score":0.50899816,"similaritySentence":"Niwa.Also we would like to express our special gratitude to Mr.Rokuro Miyake for","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Finally":"0"},{"，":"0"},{"we":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"extend":"0"},{"our":"1"},{"heartfelt":"0"},{"gratitude":"1"},{"to":"1"},{"our":"1"},{"friends":"0"},{"and":"0"},{"family":"0"},{"members":"0"},{".":"0"}],"similaritySegGreyList":[{"Niwa":"0"},{".":"0"},{"Also":"0"},{"we":"1"},{"would":"1"},{"like":"1"},{"to":"1"},{"express":"0"},{"our":"1"},{"special":"0"},{"gratitude":"1"},{"to":"1"},{"Mr":"0"},{".":"0"},{"Rokuro":"0"},{"Miyake":"0"},{"for":"0"}],"subSimilaritySentenceSection":"Komatsuda，and Dr.Takeshi Niwa.Also we would like to express our special gratitude to Mr.Rokuro Miyake for his extraordinary effort put into the","duplicateSourceMD5":"65244192988E46B80353BD8E38C14A31"},{"score":0.50578177,"similaritySentence":"This author would like to express her heartfelt gratitude to Dr.Kevin Chan for his insightful","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Finally":"0"},{"，":"0"},{"we":"0"},{"would":"1"},{"like":"1"},{"to":"1"},{"extend":"0"},{"our":"0"},{"heartfelt":"1"},{"gratitude":"1"},{"to":"1"},{"our":"0"},{"friends":"0"},{"and":"0"},{"family":"0"},{"members":"0"},{".":"0"}],"similaritySegGreyList":[{"This":"0"},{"author":"0"},{"would":"1"},{"like":"1"},{"to":"1"},{"express":"0"},{"her":"0"},{"heartfelt":"1"},{"gratitude":"1"},{"to":"1"},{"Dr":"0"},{".":"0"},{"Kevin":"0"},{"Chan":"0"},{"for":"0"},{"his":"0"},{"insightful":"0"}],"subSimilaritySentenceSection":"study.(CS MSc) This author would like to express her heartfelt gratitude to Dr.Kevin Chan for his insightful advice and towering support throughout the","duplicateSourceMD5":"9232763E80ADB26262160D7F7342C56E"},{"score":0.5024174,"similaritySentence":"later in 2008.I would like to express my heartfelt gratitude to the renal care providers","articleType":"学术期刊","classification":"local","originalSegGreyList":[{"Finally":"0"},{"，":"0"},{"we":"0"},{"would":"1"},{"like":"1"},{"to":"1"},{"extend":"0"},{"our":"0"},{"heartfelt":"1"},{"gratitude":"1"},{"to":"1"},{"our":"0"},{"friends":"0"},{"and":"0"},{"family":"0"},{"members":"0"},{".":"0"}],"similaritySegGreyList":[{"later":"0"},{"in":"0"},{"2008":"0"},{".":"0"},{"I":"0"},{"would":"1"},{"like":"1"},{"to":"1"},{"express":"0"},{"my":"0"},{"heartfelt":"1"},{"gratitude":"1"},{"to":"1"},{"the":"0"},{"renal":"0"},{"care":"0"},{"providers":"0"}],"subSimilaritySentenceSection":"in Europe and more travels abroad later in 2008.I would like to express my heartfelt gratitude to the renal care providers involved in my treatment for their","duplicateSourceMD5":"A17AE429E7713ACC375764720F3EF5C8"},{"score":0.5008058,"similaritySentence":"At last，I would like to express my heartfelt gratitude to the conference team and staff","articleType":"foreignOthers","classification":"local","originalSegGreyList":[{"Finally":"0"},{"，":"0"},{"we":"0"},{"would":"1"},{"like":"1"},{"to":"1"},{"extend":"0"},{"our":"0"},{"heartfelt":"1"},{"gratitude":"1"},{"to":"1"},{"our":"0"},{"friends":"0"},{"and":"1"},{"family":"0"},{"members":"0"},{".":"0"}],"similaritySegGreyList":[{"At":"0"},{"last":"0"},{"，":"0"},{"I":"0"},{"would":"1"},{"like":"1"},{"to":"1"},{"express":"0"},{"my":"0"},{"heartfelt":"1"},{"gratitude":"1"},{"to":"1"},{"the":"0"},{"conference":"0"},{"team":"0"},{"and":"1"},{"staff":"0"}],"subSimilaritySentenceSection":"reward all your hard work. At last，I would like to express my heartfelt gratitude to the conference team and staff and all those who have given","duplicateSourceMD5":"B81FB9BE1D8BD15165C0749292E93235"}]},{"content":"","segRedList":[],"synonymsContent":"","result":[]},{"content":"9.","segRedList":[],"synonymsContent":"","result":[]},{"content":"REFERENCES:","segRedList":[],"synonymsContent":"","result":[]},{"content":"-","segRedList":[],"synonymsContent":"","result":[]},{"content":"Muzammil Bashir，Elke A.Rundensteiner，Ramoza Ahsan，“A deep learning approach to trespassing detection using video surveillance data，”2019 IEEE International Conference on Big Data(Big Data)","segRedList":[],"synonymsContent":"","result":[]},{"content":"Analytics Vidya，“A Practical Guide to Object Detection using the Popular YOLO Framework – Part III(with Python codes)”，Available:https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/.","segRedList":[],"synonymsContent":"","result":[]},{"content":"Hamad Alawad，Sakdirat Kaewunruen，Min An，“A deep learning approach towards railway safety risk assessment”，2020 IEEE Access.","segRedList":[],"synonymsContent":"","result":[]}];